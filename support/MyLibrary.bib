@misc{centerforhistoryandnewmedia,
  title = {Zotero {{Quick Start Guide}}},
  timestamp = {2017-01-18T19:28:39Z},
  url = {http://zotero.org/support/quick_start_guide},
  author = {{Center for History and New Media}}
}

@article{grabe2001b,
  title = {Estimation of Measurement Uncertainties - an Alternative to the {{ISO Guide}}},
  volume = {38},
  abstract = {The procedure for uncertainty assessment set out in the Guide to the Expression of Uncertainty in Measurement, published by the International Organization for Standardization (ISO), is compared with an alternative system. The ISO Guide links the overall uncertainty to a so-called kP factor, implicitly expressing the degree of confidence that the measured result should cover the true value of the physical quantity in question. Here it is argued that this standard procedure, in which the magnitude of a given kp factor is tied to the degree of confidence, presents the experimenter with difficulties. These arise from the need to associate a probability density with so-called unknown systematic errors. Consequently, the recommended procedures may not be taken up by experimenters who carry out laboratory measurements on a daily basis. A simplification of the ISO procedure suggested by the Guide involves the assignment of the values 1 or 2 (or even 3) to kP. The alternative approach presented here argues that this simplification does not reflect the prevailing physical situation, and that the degree of confidence obtained lacks physical objectivity. Stationary measurement processes, strictly separating random and unknown systematic errors, are considered. Random errors are assumed to be normally distributed, and no probability distribution density is attributed to the unknown systematic errors. As random and systematic errors are kept separate, it is possible to express the influence of random errors by generalized confidence intervals (from Student's t-distribution) and the influence of systematic errors by worst-case estimates. No confidence statement is associated with the overall uncertainties, and the intrinsic problem of the Guide is circumvented. The formalisms are robust, transparent, and locate measured quantities with reasonable reliability.},
  timestamp = {2017-01-18T20:01:48Z},
  number = {2},
  journal = {Metrologia},
  author = {Grabe, Michael},
  year = {2001},
  pages = {97--106}
}

@inproceedings{jalkio2011,
  title = {Measurement {{Uncertainty}} in {{Undergraduate Physics}} - {{Student Misconceptions}} and {{Points}} of {{Difficulty}}},
  timestamp = {2017-01-18T20:01:49Z},
  booktitle = {Proceedings of the {{ASEE Annual Conference}}},
  author = {Jalkio, J.},
  editor = {Vancouver, British Columbia},
  month = jun,
  year = {2011}
}

@inproceedings{jalkio2011a,
  title = {Uncertainty about {{Uncertainty}}: What Constitutes `knowledge of Probability and Statistics Appropriate to the Program Name and Objectives},
  timestamp = {2017-01-18T20:01:49Z},
  booktitle = {Proceedings of the {{ASEE Annual Conference}}},
  author = {Jalkio, J.},
  editor = {Vancouver, British Columbia},
  month = jun,
  year = {2011}
}

@article{pedregosa2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  volume = {12},
  issn = {1532-4435},
  timestamp = {2017-01-18T20:01:49Z},
  url = {http://dl.acm.org/citation.cfm?id=1953048.2078195},
  journal = {J. Mach. Learn. Res.},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  month = nov,
  year = {2011},
  pages = {2825--2830},
  groups = {machine learning}
}

@book{rabinovich2005,
  address = {New York, New York},
  title = {Measurement {{Errors}} and {{Uncertainties}}},
  isbn = {0-387-25358-0},
  timestamp = {2017-01-18T20:01:49Z},
  publisher = {{Springer}},
  author = {Rabinovich, Semyon G.},
  year = {2005}
}

@book{russell2003,
  edition = {2},
  title = {Artificial {{Intelligence}}: {{A Modern Approach}}},
  isbn = {0-13-790395-2},
  timestamp = {2017-01-18T20:01:49Z},
  publisher = {{Pearson Education}},
  author = {Russell, Stuart J. and Norvig, Peter},
  year = {2003},
  groups = {machine learning}
}

@book{theodoridis2015,
  address = {Oxford},
  title = {Machine {{Learning}}},
  isbn = {978-0-12-801522-3},
  timestamp = {2017-01-18T20:01:49Z},
  url = {http://www.sciencedirect.com/science/article/pii/B978012801522300001X},
  publisher = {{Academic Press}},
  author = {Theodoridis, Sergios},
  editor = {Theodoridis, Sergios},
  year = {2015},
  keywords = {Machine learning},
  groups = {machine learning}
}

@article{arsenault2014,
  title = {Machine Learning for Many-Body Physics: {{The}} Case of the {{Anderson}} Impurity Model},
  volume = {90},
  doi = {10.1103/PhysRevB.90.155136},
  timestamp = {2017-01-18T20:03:07Z},
  number = {15},
  url = {http://link.aps.org/doi/10.1103/PhysRevB.90.155136},
  journal = {Phys. Rev. B},
  author = {{\c c}$\backslash$fiois Arsenault, Louis-Fran{\i}fmmode $\backslash$mbox{\c c}$\backslash$else and Lopez-Bezanilla, Alejandro and {von Lilienfeld}, O. Anatole and Millis, Andrew J.},
  month = oct,
  year = {2014},
  pages = {155136},
  groups = {machine learning}
}

@article{arsenault2015,
  title = {Machine Learning for Many-Body Physics: Efficient Solution of Dynamical Mean-Field Theory},
  timestamp = {2017-01-18T20:03:08Z},
  url = {https://arxiv.org/abs/1506.08858},
  journal = {ArXiv e-prints},
  author = {Arsenault, L.-F. and {Anatole von Lilienfeld}, O. and Millis, A. J.},
  month = jun,
  year = {2015},
  keywords = {Condensed Matter - Strongly Correlated Electrons,Statistics - Machine Learning},
  groups = {machine learning}
}

@article{ball2010,
  title = {Data {{Mining And Machine Learning In Astronomy}}},
  volume = {19},
  doi = {10.1142/S0218271810017160},
  timestamp = {2017-01-18T20:03:08Z},
  number = {07},
  url = {http://www.worldscientific.com/doi/abs/10.1142/S0218271810017160},
  journal = {International Journal of Modern Physics D},
  author = {Ball, Nicholas M. and Brunner, Robert J.},
  year = {2010},
  pages = {1049--1106},
  groups = {machine learning}
}

@article{banks2001,
  title = {Modelling and Estimating Uncertainty in Parameter Estimation},
  volume = {17},
  abstract = {In this paper we discuss questions related to reliability or variability of estimated parameters in deterministic least-squares problems. By viewing the parameters for the inverse problem as realizations for a random variable we are able to use standard results from probability theory to formulate a tractable probabilistic framework to treat this uncertainty. We discuss method stability and approximate problems and are able to show convergence of solutions of the approximate problems to those of the original problem. The efficacy of our approach is demonstrated in numerical examples involving estimation of constant parameters in differential equations.},
  timestamp = {2017-01-18T20:03:08Z},
  number = {1},
  url = {http://stacks.iop.org/0266-5611/17/i=1/a=308},
  journal = {Inverse Problems},
  author = {Banks, H. T. and Bihari, Kathleen L.},
  year = {2001},
  pages = {95},
  groups = {machine learning}
}

@book{barber2012,
  title = {Bayesian {{Reasoning}} and {{Machine Learning}}},
  timestamp = {2017-01-18T20:03:08Z},
  url = {http://www.cs.ucl.ac.uk/staff/d.barber/brml},
  publisher = {{Cambridge University Press}},
  author = {Barber, D.},
  year = {2012},
  groups = {machine learning}
}

@book{bevington2003,
  address = {New York, NY},
  title = {Data Reduction and Error Analysis for the Physical Sciences; 3rd Ed.},
  timestamp = {2017-01-18T20:03:08Z},
  url = {https://cds.cern.ch/record/1305448},
  publisher = {{McGraw-Hill}},
  author = {Bevington, Philip R and Robinson, D Keith},
  year = {2003},
  groups = {machine learning}
}

@techreport{bipm2008d,
  title = {Evaluation of {{Measurement}} Data - {{Guide}} to the {{Expression}} of {{Uncertainty}} in {{Measurement}}},
  timestamp = {2017-01-18T20:03:08Z},
  number = {JCGM 100:2008},
  institution = {Joint Committee for Guides in Metrology},
  author = {{BIPM} and {IEC} and {IFCC} and {ILAC} and {ISO} and {IUPAC} and {IUPAP} and {OIML}},
  year = {2008},
  groups = {machine learning}
}

@techreport{bipm2008a,
  type = {JCGM},
  title = {Evaluation of {{Measurement Data}}-{{Supplement}} 1 to the {{Guide}} to the {{Expression}} of {{Uncertainty}} in {{Measurement}}-{{Propagation}} of {{Distributions Using}} a {{Monte Carlo Method}}},
  timestamp = {2017-01-18T20:03:08Z},
  number = {101:2008},
  institution = {Joint Committee for Guides in Metrology},
  author = {{BIPM} and {IEC} and {IFCC} and {ILAC} and {ISO} and {IUPAC} and {IUPAP} and {OIML}},
  year = {2008},
  groups = {machine learning}
}

@techreport{bipm2008,
  title = {International {{Vocabulary}} of {{Metrology}}\textemdash{}{{Basic}} and General Concepts and Associated Terms ({{VIM}})},
  timestamp = {2017-01-18T20:03:08Z},
  number = {JCGM 200:2008},
  institution = {Joint Committee for Guides in Metrology},
  author = {{BIPM} and {IEC} and {IFCC} and {ILAC} and {ISO} and {IUPAC} and {IUPAP} and {OIML}},
  year = {2008},
  groups = {machine learning}
}

@inproceedings{claireadam-bourdarios2015,
  title = {The {{Higgs}} Boson Machine Learning Challenge},
  volume = {42},
  timestamp = {2017-01-18T20:03:08Z},
  url = {http://www.jmlr.org/proceedings/papers/v42/cowa14.pdf},
  booktitle = {{{JMLR}}: {{Workshop}} and {{Conference Proceedings}}},
  author = {Claire Adam-Bourdarios, Glen Cowan, C\textasciiacute{}ecile Germain, Isabelle Guyon, Bal\textasciiacute{}azs K\textasciiacute{}egl, David Rousseau},
  year = {2015},
  pages = {19--55},
  groups = {machine learning}
}

@article{cox2006a,
  title = {The Use of a {{Monte Carlo}} Method for Evaluating Uncertainty and Expanded Uncertainty},
  volume = {43},
  abstract = {The Guide to the Expression of Uncertainty in Measurement (GUM) is the internationally accepted master document for the evaluation of uncertainty. It contains a procedure that is suitable for many, but not all, uncertainty evaluation problems met in practice. This procedure constitutes an approximation to the general solution of the Markov formula, which infers the probability density function (PDF) for the output quantities (measurands) from the model of the measurement and the PDFs for the input quantities. This paper shows that a Monte Carlo method is an effective and versatile tool for determining the PDF for the measurands. This method provides a consistent Bayesian approach to the evaluation of uncertainty. Although in principle straightforward, some care is required in representing and validating the results obtained using the method. The paper provides guidance on optimizing the approach, identifies some pitfalls and indicates means for validating the results.},
  timestamp = {2017-01-18T20:03:08Z},
  number = {4},
  journal = {Metrologia},
  author = {Cox, Maurice G. and Siebert, Bernd R. L.},
  year = {2006},
  pages = {S178},
  groups = {machine learning}
}

@article{fotowicz2006a,
  title = {An Analytical Method for Calculating a Coverage Interval},
  volume = {43},
  abstract = {The paper presents an analytical method for calculating a coverage interval. The method consists in approximation of the convolution of standard distributions attributed to input quantity values, such as Student's, normal, rectangular, triangular or trapezoidal. It may be applied when the model of a measurand is linear or close to linear.},
  timestamp = {2017-01-18T20:03:08Z},
  number = {1},
  journal = {Metrologia},
  author = {Fotowicz, Pawe{\l}},
  year = {2006},
  pages = {42},
  groups = {machine learning}
}

@book{jalkio2001a,
  title = {Technical {{Report}} on the {{Lateral Resolution Test}} for Optical Non-Contact Probes Submitted to {{ASME}} Standards Committee {{B89}}.4.14},
  timestamp = {2017-01-18T20:03:08Z},
  author = {Jalkio, J. A.},
  year = {2001},
  groups = {machine learning,self}
}

@techreport{jalkio1999a,
  type = {SME Technical Paper},
  title = {The {{Use}} of {{Optical Probes}} on {{Coordinate Measuring Machines}} \textendash{} {{Strengths}} and {{Weaknesses}}},
  timestamp = {2017-01-18T20:03:08Z},
  number = {IQ99-206},
  institution = {Society of Manufacturing Engineers},
  author = {Jalkio, J. A.},
  year = {1999},
  groups = {machine learning,self}
}

@techreport{jalkio1987a,
  type = {SME Technical Paper},
  title = {A {{Survey}} of {{Non Contact Range Measurement Techniques}}},
  timestamp = {2017-01-18T20:03:08Z},
  number = {IQ87-858},
  institution = {Society of Manufacturing Engineers},
  author = {Jalkio, J. A.},
  year = {1987},
  groups = {machine learning,self}
}

@article{jalkio1985a,
  title = {Three {{Dimensional Inspection}} Using {{Multi}}-Stripe {{Structured Light}}},
  volume = {24},
  timestamp = {2017-01-18T20:03:08Z},
  number = {6},
  journal = {Optical Engineering},
  author = {Jalkio, J. A. and Kim, R. C. and Case, S. K.},
  year = {1985},
  pages = {966--974},
  groups = {machine learning}
}

@article{malzahn2005,
  title = {A Statistical Physics Approach for the Analysis of Machine Learning Algorithms on Real Data},
  volume = {2005},
  abstract = {We combine the replica approach of statistical physics with a variational technique to make it applicable for the analysis of machine learning algorithms on real data. The method is applied to Gaussian process models and their relative, the support vector machine. We discuss the quality of our theoretical results in comparison to experiments. As a key result, we apply our theory on real world benchmark data and show its potential for practical applications by deriving approximate expressions for data averaged performance measures which hold for general data distributions and allow us to optimize the performance of the learning algorithm.},
  timestamp = {2017-01-18T20:03:08Z},
  number = {11},
  url = {http://stacks.iop.org/1742-5468/2005/i=11/a=P11001},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  author = {Malzahn, D{\~A}\textparagraph{}rthe and Opper, Manfred},
  year = {2005},
  pages = {P11001},
  groups = {machine learning}
}

@article{mccullock1943,
  title = {A {{Logical Calculus}} of the Ideas Inherent on {{Nervous}} Activity},
  volume = {5},
  timestamp = {2017-01-18T20:04:47Z},
  number = {4},
  url = {http://www.cse.chalmers.se/~coquand/AUTOMATA/mcp.pdf},
  journal = {Bulletin of Mathematical biophysics},
  author = {McCullock, Warren and Pitts, Walter},
  year = {1943},
  pages = {115--133},
  groups = {machine learning}
}

@article{mjolsness2001,
  title = {Machine {{Learning}} for {{Science}}: {{State}} of the {{Art}} and {{Future Prospects}}},
  volume = {293},
  issn = {0036-8075},
  doi = {10.1126/science.293.5537.2051},
  abstract = {Recent advances in machine learning methods, along with successful applications across a wide variety of fields such as planetary science and bioinformatics, promise powerful new tools for practicing scientists. This viewpoint highlights some useful characteristics of modern machine learning methods and their relevance to scientific applications. We conclude with some speculations on near-term progress and promising directions.},
  timestamp = {2017-01-18T20:03:08Z},
  number = {5537},
  url = {http://science.sciencemag.org/content/293/5537/2051},
  journal = {Science},
  author = {Mjolsness, Eric and DeCoste, Dennis},
  year = {2001},
  pages = {2051--2055},
  groups = {machine learning}
}

@book{murphy2012,
  title = {Machine {{Learning}}: {{A Probabilistic Perspective}}},
  isbn = {0-262-01802-0 978-0-262-01802-9},
  timestamp = {2017-01-18T20:03:08Z},
  publisher = {{The MIT Press}},
  author = {Murphy, Kevin P.},
  year = {2012},
  groups = {machine learning}
}

@article{mysore1995,
  title = {Evaluation of {{High Precision Triangulation Sensors}} for {{Coordinate}} Measurement},
  timestamp = {2017-01-18T20:03:08Z},
  journal = {Proceedings of the 1995 ASPE annual meeting},
  author = {Mysore, A. and Jalkio, J. A. and Konicek, L.},
  year = {1995},
  groups = {machine learning}
}

@article{peretto1994,
  title = {An {{Introduction}} to the {{Modeling}} of {{Neural Networks}}},
  volume = {62},
  doi = {http://dx.doi.org/10.1119/1.17754},
  timestamp = {2017-01-18T20:03:08Z},
  number = {7},
  url = {http://scitation.aip.org/content/aapt/journal/ajp/62/7/10.1119/1.17754},
  journal = {American Journal of Physics},
  author = {Peretto, Pierre and Jensen, Roderick V.},
  year = {1994},
  pages = {668--669},
  groups = {machine learning}
}

@article{pertile2010a,
  title = {Comparison between Two Modern Uncertainty Expression and Propagation Approaches},
  volume = {238},
  timestamp = {2017-01-18T20:03:08Z},
  journal = {Journal of Physics: Conference Series},
  author = {Pertile, M. and Debei, S.},
  year = {2010},
  pages = {012033},
  groups = {machine learning}
}

@incollection{raol2004,
  series = {Control, Robotics \&amp; Sensors},
  title = {Parameter Estimation Using Artificial Neural Networks and Genetic},
  timestamp = {2017-01-18T20:03:08Z},
  url = {http://digital-library.theiet.org/content/books/10.1049/pbce065e_ch10},
  booktitle = {Modelling and {{Parameter Estimation}} of {{Dynamic Systems}}},
  publisher = {{Institution of Engineering and Technology}},
  author = {Raol, J. R. and Girija, G. and Singh, J.},
  year = {2004},
  note = {Research in the area of artificial neural networks has advanced at a rapid pace in recent times. The artificial neural network possesses a good ability to learn adaptively. The decision process in an artificial neural network is based on certain nonlinear operations. Such nonlinearities are useful: i) in improving the convergence speed (of the algorithm); ii) to provide more general nonlinear mapping between input-output signals; and iii) to reduce the effect of outliers in the measurements. One of the most successful artificial neural networks is the so-called feed forward neural network. In this chapter first the description of the feed forward neural network and its training algorithms is given. Next, parameter estimation using this approach is discussed. The presentation of training algorithms is such that it facilitates MATLAB implementation. Subsequently, recurrent neural networks are described. Several schemes based on recurrent neural networks are presented for parameter estimation of dynamical systems. Subsequently, the genetic algorithm is described and its application to parameter estimation considered.},
  keywords = {artificial neural networks,convergence speed,decision process,dynamical systems,feed forward neural network,genetic algorithms,input-output signals,MATLAB implementation,nonlinearities,nonlinear mapping,nonlinear operations,parameter estimation,recurrent neural networks,training algorithms},
  pages = {233--282},
  groups = {machine learning}
}

@book{willink2005b,
  title = {A Procedure for the Evaluation of Measurement Uncertainty Based on Moments},
  volume = {42},
  isbn = {0026-1394},
  abstract = {A practical, transparent and accurate procedure is proposed for the evaluation of measurement uncertainty. The procedure is based on established statistical theory, allows the treatment of correlated input quantities estimated from a finite number of observations and permits the assignment of asymmetric distributions to input quantities. The procedure is applicable wherever the basic method of clause 8 of the Guide to the Expression of Uncertainty in Measurement (the Guide ) is applicable. As in the method of the Guide , distributions are associated with input quantities for use with a linearized formulation of the measurement function. These distributions are combined using a property of their statistical cumulants, which are simple functions of their moments. The distribution ultimately obtained for the measurand is a member of the Pearson family, which has the set of scaled t -distributions as a subset. So the procedure allows greater flexibility than the method of the Guide in the representation of the distribution associated with the measurand. The procedure has the qualities of internal consistency and transferability as outlined in clause 0.4 of the Guide and is consistent with principles of the 'propagation of distributions' that underlie a proposed supplement to the Guide . In many cases it will permit the quotation of uncertainty intervals that are narrower than those of the Guide yet with the same justifiable probability of containing the measurand, so providing a more informative statement to the user of the interval. The procedure is presented for the probabilities of 0.95 and 0.99 but can be extended to accommodate other probabilities.},
  timestamp = {2017-01-18T20:03:08Z},
  number = {5},
  author = {Willink, R.},
  year = {2005},
  groups = {machine learning}
}

@article{tran2013,
  title = {Predicting the {{Behavior}} of a {{Chaotic Pendulum}} with a {{Variable Interaction Potential}}},
  volume = {23},
  timestamp = {2017-01-18T20:03:08Z},
  number = {3},
  journal = {Chaos},
  author = {Tran, V. and Brost, E. and Johnston, M. and Jalkio, J.},
  year = {2013},
  groups = {machine learning}
}

@article{grabe2001,
  title = {Estimation of Measurement Uncertainties - an Alternative to the {{ISO Guide}}},
  volume = {38},
  abstract = {The procedure for uncertainty assessment set out in the Guide to the Expression of Uncertainty in Measurement, published by the International Organization for Standardization (ISO), is compared with an alternative system. The ISO Guide links the overall uncertainty to a so-called kP factor, implicitly expressing the degree of confidence that the measured result should cover the true value of the physical quantity in question. Here it is argued that this standard procedure, in which the magnitude of a given kp factor is tied to the degree of confidence, presents the experimenter with difficulties. These arise from the need to associate a probability density with so-called unknown systematic errors. Consequently, the recommended procedures may not be taken up by experimenters who carry out laboratory measurements on a daily basis. A simplification of the ISO procedure suggested by the Guide involves the assignment of the values 1 or 2 (or even 3) to kP. The alternative approach presented here argues that this simplification does not reflect the prevailing physical situation, and that the degree of confidence obtained lacks physical objectivity. Stationary measurement processes, strictly separating random and unknown systematic errors, are considered. Random errors are assumed to be normally distributed, and no probability distribution density is attributed to the unknown systematic errors. As random and systematic errors are kept separate, it is possible to express the influence of random errors by generalized confidence intervals (from Student's t-distribution) and the influence of systematic errors by worst-case estimates. No confidence statement is associated with the overall uncertainties, and the intrinsic problem of the Guide is circumvented. The formalisms are robust, transparent, and locate measured quantities with reasonable reliability.},
  timestamp = {2017-01-18T20:03:08Z},
  number = {2},
  journal = {Metrologia},
  author = {Grabe, Michael},
  year = {2001},
  pages = {97--106},
  groups = {machine learning}
}

@book{rabinovich2005a,
  address = {New York, New York},
  title = {Measurement {{Errors}} and {{Uncertainties}}},
  isbn = {0-387-25358-0},
  timestamp = {2017-01-18T20:03:08Z},
  publisher = {{Springer}},
  author = {Rabinovich, Semyon G.},
  year = {2005},
  groups = {machine learning}
}

@inproceedings{jalkio2011b,
  title = {Measurement {{Uncertainty}} in {{Undergraduate Physics}} - {{Student Misconceptions}} and {{Points}} of {{Difficulty}}},
  timestamp = {2017-01-18T20:03:08Z},
  booktitle = {Proceedings of the {{ASEE Annual Conference}}},
  author = {Jalkio, J.},
  editor = {Vancouver, British Columbia},
  month = jun,
  year = {2011},
  groups = {machine learning,self}
}

@inproceedings{jalkio2011c,
  title = {Uncertainty about {{Uncertainty}}: What Constitutes `knowledge of Probability and Statistics Appropriate to the Program Name and Objectives},
  timestamp = {2017-01-18T20:03:08Z},
  booktitle = {Proceedings of the {{ASEE Annual Conference}}},
  author = {Jalkio, J.},
  editor = {Vancouver, British Columbia},
  month = jun,
  year = {2011},
  groups = {machine learning,self}
}

@article{allie2003,
  title = {Teaching {{Measurement}} in the {{Introductory Physics Laboratory}}},
  volume = {41},
  timestamp = {2017-01-18T20:05:43Z},
  number = {7},
  journal = {The Physics Teacher},
  author = {Allie, Saalih and Buffler, Andy and Campbell, Bob and Lubben, Fred and Evangelinos, Dimitris and Psillos, Dimitris and Valassiades, Odysseas},
  month = oct,
  year = {2003},
  keywords = {measurement,measurement errors,physics education,student experiments,teaching,units (measurement)},
  pages = {394--401},
  groups = {refworks}
}

@article{baratto2008,
  title = {Measurand: A Cornerstone Concept in Metrology},
  volume = {45},
  abstract = {This paper proposes to discuss some concepts whose importance is fundamental to any theory of measurement. From the epistemological point of view there is a very significant inconsistency in the concept of measurand contained in the GUM. According to the model adopted by the GUM, a particular measurand remains the same measurand after a change in its definition, notwithstanding the fact that new conditions had been included or other pre-existing conditions had been altered in the definition. And also notwithstanding the very probable fact that, in consequence, the uncertainty component associated with its definition had been changed. The problem that arises from this theoretical model is that a particular measurand is not completely characterized by a precise and unique definition. Instead, it is considered an 'ideal' utopian particular concept to which would converge a specific series of 'measurands' (more properly their definitions), continually redefined in an infinite succession. Such a theoretical basis seems very inappropriate: the model intends to determine the uncertainty of uncertainly defined particular measurands. Of great relevance is the proposal of a new definition for the concept of measurand, a concept that interfaces the realms of Metrology and experimentation. Other concepts have been created or redefined with sufficient precision and selectivity to consider some aspects that remain somewhat confused in the model of measurement currently applied.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {3},
  journal = {Metrologia},
  author = {Baratto, A. C.},
  year = {2008},
  note = {Discussion of the definition of measurand and definitional uncertainty},
  pages = {299--307},
  groups = {refworks}
}

@book{behnke1974,
  address = {Cambridge, Mass.},
  title = {Fundamentals of Mathematics- {{Foundations}} of Mathematics. {{The}} Real Number System and Algebra.},
  volume = {1},
  isbn = {0-262-52093-1},
  language = {English},
  timestamp = {2017-01-18T20:05:44Z},
  publisher = {{MIT Press}},
  author = {Behnke, Heinrich and Bachmann, F. and Fladt, K. and Suss, W.},
  year = {1974},
  note = {Grundz{\"u}ge der Mathematik.; edited by H. Behnke ... [et al.] ; with the assistance of H. Gerike ... [et al.] ; translated by S.H. Gould.; v. : ill. ; 23 cm; Translation of Grundz{\"u}ge der Mathematik.; Includes bibliographies and index.; v. 1. Foundations of mathematics. The real number system and algebra.},
  keywords = {Mathematics},
  groups = {refworks}
}

@techreport{bell2001,
  type = {NPL Measurement Good Practice Guide},
  title = {A Beginner's Guide to Uncertainty in Measurement.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {11},
  author = {Bell, S A},
  year = {2001},
  groups = {refworks}
}

@book{bevington2002,
  address = {New York, N.Y.},
  title = {Data {{Reduction}} and {{Error Analysis}} for the {{Physical Sciences}}},
  isbn = {0-07-119926-8},
  timestamp = {2017-01-18T20:05:44Z},
  publisher = {{McGraw Hill}},
  author = {Bevington, Philip R. and Robinson, D. Keith},
  year = {2002},
  groups = {refworks}
}

@article{bich2006,
  title = {Non-Linear Models and Best Estimates in the {{GUM}}},
  volume = {43},
  abstract = {A non-linear function of a sample average is different from the average of that function evaluated for each element of the sample. However, in the Guide to the Expression of Uncertainty in Measurement (GUM) the second approach to calculating an average is considered potentially preferable to the first in order to obtain a measurand estimate. In this paper the issue is discussed, and it is shown that the second approach is inconsistent with the GUM framework. However, it is indeed preferable for input quantities having an intrinsically random behaviour. We comment on the approach adopted in this respect in Supplement 1 of the GUM and show that it is consistent with the first method.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {4},
  journal = {Metrologia},
  author = {Bich, Walter and Callegaro, Luca and Pennecchi, Francesca},
  year = {2006},
  pages = {S196},
  groups = {refworks}
}

@article{bich2006a,
  title = {Evolution of the '{{Guide}} to the {{Expression}} of {{Uncertainty}} in {{Measurement}}'},
  volume = {43},
  abstract = {A number of Joint Committees of the Bureau International des Poids et Mesures and other international organizations carry out particular tasks of common interest. The Joint Committee for Guides in Metrology (JCGM) has amongst its tasks the promotion of the ' Guide to the Expression of Uncertainty in Measurement ' (GUM), the preparation of further documents for its broad application, and revision and promotion of the use of the ' International Vocabulary of Basic and General Terms in Metrology '. This paper summarizes the documents relating to the GUM planned by JCGM.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {4},
  journal = {Metrologia},
  author = {Bich, Walter and Cox, Maurice G. and Harris, Peter M.},
  year = {2006},
  note = {Description of plan to overhaul GUM via supplements},
  pages = {S161--S166},
  groups = {refworks}
}

@article{bievre2009,
  title = {The 2007 {{International Vocabulary}} of {{Metrology}} ({{VIM}}), {{JCGM}} 200:2008 [{{ISO}}/{{IEC Guide}} 99]: {{Meeting}} the Need for Intercontinentally Understood Concepts and Their Associated Intercontinentally Agreed Terms.},
  volume = {42},
  timestamp = {2017-01-18T20:05:44Z},
  number = {4-5},
  journal = {Clin Biochem},
  author = {Bi{\`e}vre, Paul De},
  year = {2009},
  note = {19863914},
  pages = {246--248},
  groups = {refworks}
}

@techreport{bipm2008c,
  title = {Evaluation of {{Measurement}} Data - {{Guide}} to the {{Expression}} of {{Uncertainty}} in {{Measurement}}},
  timestamp = {2017-01-18T20:05:44Z},
  number = {JCGM 100:2008},
  institution = {Joint Committee for Guides in Metrology},
  author = {{BIPM} and {IEC} and {IFCC} and {ILAC} and {ISO} and {IUPAC} and {IUPAP} and {OIML}},
  year = {2008},
  groups = {refworks}
}

@techreport{bipm2008b,
  type = {JCGM},
  title = {Evaluation of {{Measurement Data}}-{{Supplement}} 1 to the {{Guide}} to the {{Expression}} of {{Uncertainty}} in {{Measurement}}-{{Propagation}} of {{Distributions Using}} a {{Monte Carlo Method}}},
  timestamp = {2017-01-18T20:05:44Z},
  number = {101:2008},
  institution = {Joint Committee for Guides in Metrology},
  author = {{BIPM} and {IEC} and {IFCC} and {ILAC} and {ISO} and {IUPAC} and {IUPAP} and {OIML}},
  year = {2008},
  groups = {refworks}
}

@techreport{bipm2008e,
  title = {International {{Vocabulary}} of {{Metrology}}\textemdash{}{{Basic}} and General Concepts and Associated Terms ({{VIM}})},
  timestamp = {2017-01-18T20:05:44Z},
  number = {JCGM 200:2008},
  institution = {Joint Committee for Guides in Metrology},
  author = {{BIPM} and {IEC} and {IFCC} and {ILAC} and {ISO} and {IUPAC} and {IUPAP} and {OIML}},
  year = {2008},
  groups = {refworks}
}

@article{birge1939,
  title = {The {{Propagation}} of {{Errors}}},
  volume = {7},
  timestamp = {2017-01-18T20:05:44Z},
  number = {6},
  journal = {The American Physics Teacher},
  author = {Birge, Raymond T.},
  month = dec,
  year = {1939},
  note = {Early article describing derivation of error progagation equation from taylor series and resulting expressions for sums, products, exponents},
  pages = {351--357},
  groups = {refworks}
}

@article{blazquez2008,
  title = {The Coverage Factor in a {{Flatten}}\textendash{}{{Gaussian}} Distribution},
  volume = {45},
  abstract = {According to the Guide to the Expression of Uncertainty in Measurement , a coverage factor that produces an expanded uncertainty having an approximate level of confidence is recommended in the final expression of the measurement result. The numerical value for the coverage factor depends on the probability density function. When a Type A evaluation following a Gaussian (normal) distribution is combined with a Type B one following a rectangular distribution, the resulting probability density function is neither Gaussian nor rectangular, but similar to both, which we term a Flatten\textendash{}Gaussian probability density function. The variance of such a function is the combined variance in the usual way, but the coverage factor must be calculated.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {5},
  journal = {Metrologia},
  author = {Bl{\'a}zquez, J. and Garc{\'\i}a-Berrocal, A. and Montalvo, C. and Balb{\'a}s, M.},
  year = {2008},
  pages = {503},
  groups = {refworks}
}

@book{bolstad2007,
  address = {Hoboken, N.J.},
  title = {Introduction to {{Bayesian Statistics}}},
  isbn = {0-470-14115-8},
  timestamp = {2017-01-18T20:05:44Z},
  publisher = {{John Wiley and Sons}},
  author = {Bolstad, William M.},
  year = {2007},
  groups = {refworks}
}

@article{box1980,
  title = {Sampling and {{Bayes}}' {{Inference}} in {{Scientific Modelling}} and {{Robustness}}},
  volume = {143},
  abstract = {Scientific learning is an iterative process employing Criticism and Estimation. Correspondingly the formulated model factors into two complementary parts\textendash{}a predictive part allowing model criticism, and a Bayes posterior part allowing estimation. Implications for significance tests, the theory of precise measurement and for ridge estimates are considered. Predictive checking functions for transformation, serial correlation, bad values, and their relation with Bayesian options are considered. Robustness is seen from a Bayesian viewpoint and examples are given. For the bad value problem a comparison with M estimators is made.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {4},
  journal = {Journal of the Royal Statistical Society.Series A (General)},
  author = {Box, George E. P.},
  year = {1980},
  pages = {383--430},
  groups = {refworks}
}

@book{box2005,
  address = {Hoboken, NJ},
  title = {Statistics for {{Experimenters}}: {{Design}}, {{Innovation}}, and {{Discovery}}},
  isbn = {0-471-71813-0},
  timestamp = {2017-01-18T20:05:44Z},
  publisher = {{Wiley}},
  author = {Box, George E. P. and Hunter, J. Stuart and Hund, Edelgard},
  year = {2005},
  groups = {refworks}
}

@article{braginsky2000,
  title = {Thermo-Refractive Noise in Gravitational Wave Antennae},
  volume = {271},
  abstract = {Thermodynamical fluctuations of temperature in mirrors of gravitational wave antennae may be transformed into additional noise not only through thermal expansion coefficient [V.B. Braginsky, M.L. Gorodetsky, S.P. Vyatchanin, Phys. Lett. A 264 (1999) 1; cond-mat/9912139] but also through temperature dependence of refraction index. The intensity of this noise is comparable with other known noises at frequencies $\sim$1 kHz.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {5-6},
  journal = {Physics Letters A},
  author = {Braginsky, V. B. and Gorodetsky, M. L. and Vyatchanin, S. P.},
  year = {7/10 2000},
  pages = {303--307},
  groups = {refworks}
}

@article{buffler2008,
  title = {Teaching {{Measurement}} and {{Uncertainty}} the {{GUM Way}}},
  volume = {46},
  timestamp = {2017-01-18T20:05:44Z},
  number = {9},
  journal = {The Physics Teacher},
  author = {Buffler, Andy and Allie, Saalih and Lubben, Fred},
  month = dec,
  year = {2008},
  keywords = {data analysis,educational courses,measurement uncertainty,physics education,probability,teaching},
  pages = {539--543},
  groups = {refworks}
}

@article{buffler2001,
  title = {The Development of First Year Physics Students' Ideas about Measurement in Terms of Point and Set Paradigms},
  volume = {23},
  abstract = {The ideas of first year university students about measurement in the physics laboratory are explored. Student responses to written probes administered at the beginning of the year are compared to those written after a 12 week laboratory course. The 'point' and 'set' paradigms are used as a model to analyse the responses to the probes. At the heart of the point paradigm is that both action and reasoning are based solely on individual measurements in a data set. On the other hand, subscribing to the set paradigm implies an understanding that a series of measurements are to be viewed as a collective that can be modelled by theoretical constructs, such as the mean and standard deviation. The degree of consistent use of these paradigms by individual students across the sets of probes is investigated. Implications for effective teaching interventions in the physics laboratory are discussed.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {11},
  journal = {International Journal of Science Education},
  author = {Buffler, Andy and Allie, Saalih and Lubben, Fred},
  year = {2001},
  pages = {1137},
  groups = {refworks}
}

@article{callen1951,
  title = {Irreversibility and {{Generalized Noise}}},
  volume = {83},
  timestamp = {2017-01-18T20:05:44Z},
  number = {1},
  journal = {Phys.Rev.},
  author = {Callen, Herbert B. and Welton, Theodore A.},
  month = jul,
  year = {1951},
  pages = {34--40},
  groups = {refworks}
}

@article{chen2006,
  title = {Extending the Discussion on Coverage Intervals and Statistical Coverage Intervals},
  volume = {43},
  abstract = {Willink ( 2004 Metrologia [/0026-1394/41/3/l01] 41 L5{\^a}\texteuro``6 ) is concerned that, in the society of metrology, there is potential for confusion between coverage interval and statistical coverage interval and he makes a precise interpretaion of these two terms. We further clarify that the confidence of a coverage interval is actually a statistical coverage interval.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {6},
  url = {http://stacks.iop.org/0026-1394/43/i=6/a=N03},
  journal = {Metrologia},
  author = {Chen, Lin-An and Hung, Hui-Nien},
  year = {2006},
  pages = {43},
  groups = {refworks}
}

@article{colyvan2008,
  title = {Is {{Probability}} the {{Only Coherent Approach}} to {{Uncertainty}}?},
  volume = {28},
  abstract = {In this article, I discuss an argument that purports to prove that probability theory is the only sensible means of dealing with uncertainty. I show that this argument can succeed only if some rather controversial assumptions about the nature of uncertainty are accepted. I discuss these assumptions and provide reasons for rejecting them. I also present examples of what I take to be non-probabilistic uncertainty.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {3},
  journal = {Risk Analysis},
  author = {Colyvan, Mark},
  year = {2008},
  pages = {645--652},
  groups = {refworks}
}

@article{colyvan2004,
  title = {The Philosophical Significance of {{Cox}}'s Theorem},
  volume = {37},
  timestamp = {2017-01-18T20:05:44Z},
  number = {1},
  journal = {International Journal of Approximate Reasoning},
  author = {Colyvan, Mark},
  year = {2004},
  keywords = {Belief,Cox,Excluded middle,Non-classical logic,probability,Uncertainty},
  pages = {71},
  groups = {refworks}
}

@article{cordero2007,
  title = {Evaluating the Uncertainties of Data Rendered by Computational Models},
  volume = {44},
  abstract = {Computational models allow calculation of the value of an output quantity from a set of linked input quantities . The value of the output quantity yielded by a model is evidently influenced by errors in the determination of the input quantities. Therefore, the uncertainties of the output data can be expressed in terms of the uncertainties of the input quantities by using a Monte Carlo-based uncertainty propagation technique. As an example, we evaluated the uncertainty of the spectral UV irradiance rendered by a radiative transfer model under cloudless sky conditions. This model allows calculation of the spectrally resolved solar UV irradiance from some set of measured input quantities linked with the concentration of atmospheric constituents, the surface reflectivity as well as the spectral characteristics of the aerosol modulation. Although only a single model was used in this work, the methodology applied to evaluate the uncertainty is general and can be applied to any other computational model.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {3},
  journal = {Metrologia},
  author = {Cordero, Raul R. and Seckmeyer, Gunther and Labbe, Fernando},
  year = {2007},
  note = {monte carlo folder, good description of monte carlo methods},
  pages = {23},
  groups = {refworks}
}

@article{cousins1995,
  title = {Why Isn't Every Physicist a {{Bayesian}}?},
  volume = {63},
  timestamp = {2017-01-18T20:05:44Z},
  number = {5},
  journal = {American Journal of Physics},
  author = {Cousins, Robert D.},
  month = may,
  year = {1995},
  keywords = {data analysis,DECISION MAKING,DISTRIBUTION FUNCTIONS,ERRORS,GAUSSIAN PROCESSES,MEASURING METHODS,probability,STATISTICS},
  pages = {398--410},
  groups = {refworks}
}

@article{cox2008,
  title = {A Probabilistic Approach to the Analysis of Measurement Processes},
  volume = {45},
  abstract = {We consider a probabilistic model of the measurement process, based on identifying two main sub-processes, named observation and restitution. Observation constitutes the transformations involved in producing the observable output. Restitution constitutes the determination of the measurand (the quantity measured) from the observable output, and includes data processing. After providing a probabilistic representation of the observation sub-process, we derive appropriate formulae for addressing restitution and describing the overall measurement process. The model allows the treatment in probabilistic terms of both the random and systematic effects that influence the measurement process, and may prove particularly useful in the formulation phase of uncertainty evaluation. We also discuss the different ways in which the measurand can be characterized by a probability distribution, and demonstrate the application of the approach to the analysis of risk in conformance testing.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {5},
  journal = {Metrologia},
  author = {Cox, Maurice G. and Rossi, Giovanni B. and Harris, Peter M. and Forbes, Alistair},
  year = {2008},
  pages = {493},
  groups = {refworks}
}

@book{cox2006,
  title = {The Use of a {{Monte Carlo}} Method for Evaluating Uncertainty and Expanded Uncertainty},
  volume = {43},
  isbn = {0026-1394},
  abstract = {The Guide to the Expression of Uncertainty in Measurement (GUM) is the internationally accepted master document for the evaluation of uncertainty. It contains a procedure that is suitable for many, but not all, uncertainty evaluation problems met in practice. This procedure constitutes an approximation to the general solution of the Markov formula, which infers the probability density function (PDF) for the output quantities (measurands) from the model of the measurement and the PDFs for the input quantities. This paper shows that a Monte Carlo method is an effective and versatile tool for determining the PDF for the measurands. This method provides a consistent Bayesian approach to the evaluation of uncertainty. Although in principle straightforward, some care is required in representing and validating the results obtained using the method. The paper provides guidance on optimizing the approach, identifies some pitfalls and indicates means for validating the results.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {4},
  author = {Cox, Maurice G. and Siebert, Bernd R. L.},
  year = {2006},
  groups = {refworks}
}

@article{cox1946,
  title = {Probability, {{Frequency}} and {{Reasonable Expectation}}},
  volume = {14},
  timestamp = {2017-01-18T20:05:44Z},
  number = {1},
  journal = {American Journal of Physics},
  author = {Cox, R. T.},
  month = jan,
  year = {1946},
  pages = {1--13},
  groups = {refworks}
}

@article{crawford2000,
  title = {Bias and Variance Reduction in Computer Simulation Studies},
  volume = {124},
  timestamp = {2017-01-18T20:05:44Z},
  number = {3},
  journal = {European Journal of Operational Research},
  author = {Crawford, J. W. and Gallwey, T. J.},
  year = {8/1 2000},
  pages = {571--590},
  groups = {refworks}
}

@book{dagostini2003,
  title = {Bayesian Reasoning in Data Analysis {{A}} Critical Introduction},
  timestamp = {2017-01-18T20:05:44Z},
  publisher = {{World Scientific Publishing}},
  author = {D'Agostini, Giulio},
  year = {2003},
  groups = {refworks}
}

@article{dagostini1999,
  title = {Teaching Statistics in the Physics Curriculum: {{Unifying}} and Clarifying Role of Subjective Probability},
  volume = {67},
  timestamp = {2017-01-18T20:05:44Z},
  number = {12},
  journal = {American Journal of Physics},
  author = {D'Agostini, Giulio},
  month = dec,
  year = {1999},
  keywords = {Bayes methods,inference mechanisms,probability,STATISTICS,teaching},
  pages = {1260--1268},
  groups = {refworks}
}

@book{deardorff2001,
  title = {Introductory {{Physics Students}}' {{Treatment}} of {{Measurement Uncertainty}}},
  timestamp = {2017-01-18T20:05:44Z},
  author = {Deardorff, Duane Lee},
  year = {2001},
  groups = {refworks}
}

@article{decker2008,
  title = {Analysing Redundancy in a Line Scale Comparison Using {{Monte Carlo}} Methods},
  volume = {19},
  abstract = {Supplement 1 to the ISO Guide to the Expression of Uncertainty in Measurement (GUM) outlines the application of Monte Carlo (MC) simulation to uncertainty propagation through the mathematical model of the measurement. The MC approach uses the information contained in the probability density functions associated with each of the influence parameters of the measurement to determine the probability density function of the output quantity. This paper discusses an approach to uncertainty evaluation for calibration of line scales amenable for application of MC methods. The model for the measurement uncertainty in line scale calibration is interpreted from the perspective of the line-scale user, namely a distance measurement between any two intervals of the line scale. For this purpose, the introduction of a consensus invariant assigned in an alternative interpretation of the data can also expedite international comparison analysis. The proposed method is illustrated using a practical real-world example from an international comparison of calibration of line scales.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {6},
  journal = {Measurement Science and Technology},
  author = {Decker, J. E. and Steele, A. G. and Bosse, H. and Douglas, R. J.},
  year = {2008},
  pages = {064005 (6pp)},
  groups = {refworks}
}

@article{dempster1968,
  title = {A {{Generalization}} of {{Bayesian Inference}}},
  volume = {30},
  abstract = {Procedures of statistical inference are described which generalize Bayesian inference in specific ways. Probability is used in such a way that in general only bounds may be placed on the probabilities of given events, and probability systems of this kind are suggested both for sample information and for prior information. These systems are then combined using a specified rule. Illustrations are given for inferences about trinomial probabilities, and for inferences about a monotone sequence of binomial pi. Finally, some comments are made on the general class of models which produce upper and lower probabilities, and on the specific models which underlie the suggested inference procedures.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {2},
  journal = {Journal of the Royal Statistical Society.Series B (Methodological)},
  author = {Dempster, A. P.},
  year = {1968},
  pages = {205--247},
  groups = {refworks}
}

@article{dieck1997,
  title = {Measurement Uncertainty Models},
  volume = {36},
  abstract = {For decades, experimenters, engineers and statisticians have developed several types of measurement uncertainty models. This paper explores the origins of some of the most common models, their strengths, their weaknesses and the reasons for using them in particular situations. Models considered include the U.S. Air Force standard model, the American Society of Mechanical Engineers (ASME) standard models (c. 1985), the International Standards Organization (ISO) model and the proposed simplified ASME uncertainty model which is in harmony with the ISO model."},
  timestamp = {2017-01-18T20:05:44Z},
  number = {1},
  journal = {ISA transactions},
  author = {Dieck, Ronald H.},
  year = {1997},
  pages = {29},
  groups = {refworks}
}

@article{dubois2004,
  title = {Probability-{{Possibility Transformations}}, {{Triangular Fuzzy Sets}}, and {{Probabilistic Inequalities}}},
  volume = {10},
  timestamp = {2017-01-18T20:05:44Z},
  number = {4},
  journal = {Reliable Computing},
  author = {Dubois, Didier and Foulloy, Laurent and Mauris, Gilles and Prade, Henri},
  year = {2004},
  note = {id: 1; issn: print 1385-3139},
  pages = {273--297},
  groups = {refworks}
}

@article{dubois1997,
  title = {The Three Semantics of Fuzzy Sets},
  volume = {90},
  abstract = {Three main semantics for membership functions seem to exist in the literature: similarity, preference and uncertainty. Each semantics underlies a particular class of applications. Similarity notions are exploited in clustering analysis and fuzzy controllers. Uncertainty is captured by fuzzy sets in the framework of possibility theory. The membership function of a fuzzy set is also sometimes a kind of utility function that represents flexible constraints in decision problems. This paper advocates the claim that progress in operational semantics of membership functions presupposes that these distinct semantics be acknowledged and related to more basic measurement issues in terms of distance, cost and frequency, on which scientific traditions exist.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {2},
  journal = {Fuzzy Sets and Systems},
  author = {Dubois, Didier and Prade, Henri},
  year = {9/1 1997},
  keywords = {Preference,Similarity,Uncertainty},
  pages = {141--150},
  groups = {refworks}
}

@article{dubois1989,
  title = {Fuzzy Sets, Probability and Measurement},
  volume = {40},
  abstract = {In recent years, the problem of uncertainty modeling has received much attention from scholars in artificial intelligence and decision theory. Various formal settings, including but not restricted to fuzzy sets and possibility measures, have been proposed, based on different intuitions, and dealing with various kinds of uncertain data. The two main research directions are upper and lower probabilities which convey the idea of imprecisely estimated probability measures, and distorted probabilities for the descriptive assessment of partial belief. Possibility measures, and thereby fuzzy sets, stand at the crossroads of these new approaches. Traditional views, interpretive settings and canonical experiments for the measurement of probability such as frequentist approaches, betting theories, comparative uncertainty relations are currently extended to the generalized uncertainty measures. These works shed new light on various interpretations of fuzzy sets and clarify their links with probability theory; conversely Zadeh's logical point of view on fuzzy sets suggests a set-theoretic perspective on uncertainty measures, that brings together numerical quantification and logic."},
  timestamp = {2017-01-18T20:05:44Z},
  number = {2},
  journal = {European Journal of Operational Research},
  author = {Dubois, Didier and Prade, Henri},
  year = {1989},
  keywords = {Fuzzy sets,measurement,possibility theory,probability theory,random set,STATISTICS},
  pages = {135},
  groups = {refworks}
}

@inproceedings{dubois1993,
  title = {On {{Possibility}}/{{Probability Transformations}}},
  timestamp = {2017-01-18T20:05:44Z},
  booktitle = {Proceedings of {{Fourth IFSA Conference}}},
  publisher = {{Kluwer Academic Publ}},
  author = {Dubois, Didier and Prade, Henri and Sandri, Sandra},
  year = {1993},
  note = {pdf in dropbox},
  pages = {103--112},
  groups = {refworks}
}

@article{duck1998,
  title = {Toward an Understanding of the Spin-Statistics Theorem},
  volume = {66},
  doi = {10.1119/1.18860},
  timestamp = {2017-01-18T20:05:44Z},
  number = {4},
  url = {http://link.aip.org/link/?AJP/66/284/1},
  journal = {American Journal of Physics},
  author = {Duck, Ian and Sudarshan, E. C. G.},
  year = {1998},
  keywords = {axiomatic field theory,harmonic oscillators,nonlinear field theory,quantum statistical mechanics,relativistic quantum field theory,teaching},
  pages = {284--303},
  groups = {refworks}
}

@article{derrico2009,
  title = {Paradigms for Uncertainty Treatments: {{A}} Comparative Analysis with Application to Measurement},
  volume = {42},
  abstract = {This paper shows that, among diverse axiomatic systems (many-valued logics, fuzzy sets, calculus of probability, and theory of evidence), the calculus of probability uniquely provides a paradigm able to process uncertainty without violating any classical logic's law (excluded middle, non-contradiction, and so on). A characterization of this paradigm is outlined in mathematical logic terms, focusing on quantitative treatment of measurement uncertainty.},
  timestamp = {2017-01-18T20:05:44Z},
  number = {4},
  journal = {Measurement},
  author = {D'Errico, Giampaolo E.},
  month = may,
  year = {2009},
  keywords = {Estimation,Inference systems,measurement uncertainty},
  pages = {494--500},
  groups = {refworks}
}

@article{efron1986,
  title = {Why {{Isn}}'t {{Everyone}} a {{Bayesian}}?},
  volume = {40},
  abstract = {Originally a talk delivered at a conference on Bayesian statistics, this article attempts to answer the following question: why is most scientific data analysis carried out in a non-Bayesian framework? The argument consists mainly of some practical examples of data analysis, in which the Bayesian approach is difficult but Fisherian/frequentist solutions are relatively easy. There is a brief discussion of objectivity in statistical analyses and of the difficulties of achieving objectivity within a Bayesian framework. The article ends with a list of practical advantages of Fisherian/frequentist methods, which so far seem to have outweighed the philosophical superiority of Bayesianism.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {1},
  journal = {The American Statistician},
  author = {Efron, B.},
  month = feb,
  year = {1986},
  pages = {1--5},
  groups = {refworks}
}

@article{eisenhart1968,
  title = {Expression of the {{Uncertainties}} of {{Final Results}}},
  volume = {160},
  timestamp = {2017-01-18T20:05:45Z},
  number = {3833},
  journal = {Science},
  author = {Eisenhart, Churchill},
  month = jun,
  year = {1968},
  note = {Milestone article laying foundation of modern error analysis},
  pages = {1201--1204},
  groups = {refworks}
}

@article{elster2007a,
  title = {Calculation of Uncertainty in the Presence of Prior Knowledge},
  volume = {44},
  abstract = {The Guide to the Expression of Uncertainty in Measurement (GUM) applies to the generic situation that a model is available which relates the quantity of interest to input quantities and that information on the input quantities is given in terms of estimates and uncertainties. Within this framework, the GUM specifies how to calculate an estimate of the quantity of interest and the uncertainty associated with this estimate as well as a coverage interval for the quantity according to a stipulated coverage probability. In this paper the case is considered that additional prior knowledge about the quantity of interest is available. For instance, the quantity of interest could be known to be positive or some prior estimate and its associated uncertainty may be given. This case is treated using probability density functions which encode the information on the input quantities as well as the prior knowledge about the quantity of interest. In general, numerical techniques have to be employed and an easy-to-apply Monte Carlo method is proposed. For the particular case of a linear (or linearized) model relation and Gaussian probability density functions an analytic solution is derived and discussed. Finally, the proposed treatment, its numerical implementation and the possible benefit of taking into account prior knowledge are illustrated by an example.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {2},
  journal = {Metrologia},
  author = {Elster, Clemens},
  year = {2007},
  pages = {111},
  groups = {refworks}
}

@article{elster2008,
  title = {Uncertainty Evaluation for Dynamic Measurements Modelled by a Linear Time-Invariant System},
  volume = {45},
  abstract = {Evaluation of measurement uncertainty is considered when the value of the measurand depends on the continuous variable time. A concept of dynamic measurement uncertainty is introduced by generalizing the GUM approach. The concept is applied to linear and time-invariant systems which are often appropriate to model dynamic measurements. Digital filtering is proposed for estimating the time-dependent value of the measurand and the design of an appropriate FIR filter is described. Dynamic uncertainty evaluation is then carried out for this analysis and conditions are specified for its proper use. The approach is illustrated for the particular example of a second-order model. It is shown in terms of simulations that the proposed analysis yields significantly improved results when compared with the sometimes applied quasi-static treatment.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {4},
  journal = {Metrologia},
  author = {Elster, Clemens and Link, Alfred},
  year = {2008},
  pages = {464--473},
  groups = {refworks}
}

@article{elster2009,
  title = {Bayesian Uncertainty Analysis under Prior Ignorance of the Measurand versus Analysis Using the {{Supplement}} 1 to the {{Guide}} : A Comparison},
  volume = {46},
  abstract = {A recent supplement to the GUM (GUM S1) is compared with a Bayesian analysis in terms of a particular task of data analysis, one where no prior knowledge of the measurand is presumed. For the Bayesian analysis, an improper prior density on the measurand is employed. It is shown that both approaches yield the same results when the measurand depends linearly on the input quantities, but generally different results otherwise. This difference is shown to be not a conceptual one, but due to the fact that the two methods correspond to Bayesian analysis under different parametrizations, with ignorance of the measurand expressed by a non-informative prior on a different parameter. The use of the improper prior for the measurand itself may result in an improper posterior probability density function (PDF) when the measurand depends non-linearly on the input quantities. On the other hand, the PDF of the measurand derived by the GUM supplement method is always proper but may sometimes have undesirable properties such as non-existence of moments. It is concluded that for a linear model both analyses can safely be applied. For a non-linear model, the GUM supplement approach may be preferred over a Bayesian analysis using a constant prior on the measurand. But since in this case the GUM S1 PDF may also have undesirable properties, and as often some prior knowledge about the measurand may be established, metrologists are strongly encouraged to express this prior knowledge in terms of a proper PDF which can then be included in a Bayesian analysis. The results of this paper are illustrated by an example of a simple non-linear model.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {3},
  journal = {Metrologia},
  author = {Elster, Clemens and Toman, Blaza},
  year = {2009},
  pages = {261},
  groups = {refworks}
}

@book{elster2007,
  title = {Draft {{GUM Supplement}} 1 and {{Bayesian}} Analysis},
  volume = {44},
  isbn = {0026-1394},
  abstract = {The relation between uncertainty evaluation according to the recent draft of GUM Supplement 1 and the application of Bayesian statistics including Bayes' theorem is considered. In the case of a Type A evaluation of uncertainty, repeated measurement indications are regarded as independently drawn from normal frequency distributions and, according to its suggestion, the numerical evaluation method of Supplement 1 is applied after having assigned scaled and shifted t -distributions to the corresponding input quantities. It will be shown that this approach is equivalent to a Bayesian analysis using Bayes' theorem with commonly used prior distributions.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {3},
  author = {Elster, Clemens and W{\"o}ger, Wolfgang and Cox, Maurice G.},
  year = {2007},
  groups = {refworks}
}

@article{estler1999,
  title = {Measurement as {{Inference}}: {{Fundamental Ideas}}},
  volume = {48},
  timestamp = {2018-03-28T19:41:05Z},
  number = {2},
  journal = {Annals of the CIRP},
  author = {Estler, W. Tyler},
  year = {1999},
  pages = {611--632},
  file = {MeasurementasInference.pdf:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\XWJA5AXQ\\MeasurementasInference.pdf:application/pdf},
  groups = {refworks,uncertainty}
}

@article{ferrero2008,
  title = {The Construction of Random-Fuzzy Variables from the Available Relevant Metrological Information},
  volume = {57},
  timestamp = {2017-01-18T20:05:45Z},
  number = {12},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  author = {Ferrero, Alessandro and Salicone, Simona},
  year = {2008},
  pages = {365--374},
  groups = {refworks}
}

@inproceedings{ferrero2006a,
  address = {Stockholm, Sweden},
  title = {Decision {{Making}} in the {{Presence}} of {{Measurement Uncertainty}}: An {{Approach}} in {{Terms}} of the {{Theory}} of {{Evidence}}},
  abstract = {In almost every scientific and technical activity, decisions are taken on the basis of data coming from the field, either in a direct way (measurement results) or in an indirect way (as the result of some processing of the experimental data). Generally, decisions are taken by comparing data coming from the field with each other or with a given threshold. The problem is that experimental data, collected as the results of a measurement activity, are affected by uncertainty, and therefore the comparison cannot be performed as a simple comparison between two scalar values, but should also take into account the measurement uncertainty. In fact, neglecting measurement uncertainty could lead to decisions that might be dramatically wrong. The problem of decision making in the presence of measurement uncertainty is very often neglected, or approached in a probabilistic way. This paper proposes a new, more general approach, based on the theory of evidence},
  timestamp = {2017-01-18T20:05:45Z},
  booktitle = {9th {{International Conference}} on {{Probabilistic Methods Applied}} to {{Power Systems}}},
  publisher = {{IEEE}},
  author = {Ferrero, A. and Salicone, S.},
  month = jun,
  year = {2006},
  keywords = {DECISION MAKING,measurement uncertainty,theory-of-evidence},
  pages = {1},
  groups = {refworks}
}

@article{ferrero2006,
  title = {Fully {{Comprehensive Mathematical Approach}} to the {{Expression}} of {{Uncertainty}} in {{Measurement}}},
  volume = {55},
  timestamp = {2017-01-18T20:05:45Z},
  number = {3},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  author = {Ferrero, A. and Salicone, Simona},
  year = {2006},
  pages = {706--712},
  groups = {refworks}
}

@article{ferrero2005,
  title = {A Comparative Analysis of the Statistical and Random-Fuzzy Approaches in the Expression of Uncertainty in Measurement},
  volume = {54},
  abstract = {The present practice for uncertainty expression and estimation in measurement, endorsed in the IEC-ISO Guide to the Expression of Uncertainty in Measurement, is based on a statistical approach, which is also the basis for the Monte Carlo method generally employed to overcome the problems met in the strict application of the guide. More recently, methods based on the fuzzy theory have been proposed too, with encouraging results. This paper compares the results obtained, in the expression of uncertainty, by the use of the Monte Carlo method and the random-fuzzy variable method. Both methods are applied to a real, digital signal processing-based instrument for electric power quality measurement, and the obtained results are compared and discussed.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {4},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  author = {Ferrero, A. and Salicone, S.},
  month = aug,
  year = {2005},
  keywords = {digital signal processing instrument,electric power quality measurement,fuzzy set theory,fuzzy theory,IEC-ISO standard,IEC standards,ISO standards,measurement estimation,measurement uncertainty,Monte Carlo method,Monte Carlo methods,power supply quality,power system measurement,random-fuzzy approach,random-fuzzy variable method,statistical analysis,statistical approach,uncertainty expression},
  pages = {1475},
  groups = {refworks}
}

@article{ferrero2004,
  title = {The Random-Fuzzy Variables: A New Approach to the Expression of Uncertainty in Measurement},
  volume = {53},
  abstract = {The good measurement practice requires that the measurement uncertainty is estimated and provided together with the measurement result. The practice today, which is reflected in the reference standard provided by the IEC-ISO "Guide to the expression of uncertainty in measurement," adopts a statistical approach for the expression and estimation of the uncertainty, since the probability theory is the most known and used mathematical tool to deal with distributions of values. However, the probability theory is not the only tool to deal with distributions of values and is not the most suitable one when the values do not distribute in a totally random way. In this case, a more general theory, the theory of the evidence, should be considered. This paper recalls the fundamentals of the theory of the evidence and frames the random-fuzzy variables within this theory, showing how they can usefully be employed to represent the result of a measurement together with its associated uncertainty. The mathematics is defined on the random-fuzzy variables, so that the uncertainty can be processed, and simple examples are given.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {5},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  author = {Ferrero, A. and Salicone, S.},
  month = oct,
  year = {2004},
  keywords = {evidence theory,IEC-ISO,IEC standards,measurement theory,measurement uncertainty,metrology,probability,probability theory,random-fuzzy variables,statistical approach,uncertainty expression,uncertainty measurement},
  pages = {1370},
  groups = {refworks}
}

@article{fotowicz2006,
  title = {An Analytical Method for Calculating a Coverage Interval},
  volume = {43},
  abstract = {The paper presents an analytical method for calculating a coverage interval. The method consists in approximation of the convolution of standard distributions attributed to input quantity values, such as Student's, normal, rectangular, triangular or trapezoidal. It may be applied when the model of a measurand is linear or close to linear.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {1},
  journal = {Metrologia},
  author = {Fotowicz, Pawe{\l}},
  year = {2006},
  pages = {42},
  groups = {refworks}
}

@article{fotowicz2004,
  title = {A Method of Approximation of the Coverage Factor in Calibration},
  volume = {35},
  timestamp = {2017-01-18T20:05:45Z},
  number = {3},
  journal = {Measurement},
  author = {Fotowicz, Pawel},
  month = apr,
  year = {2004},
  keywords = {Coverage factor,Probability distribution,Uncertainty},
  pages = {251--256},
  groups = {refworks}
}

@article{fox2009,
  title = {Bayesian Credible Intervals for Response Surface Optima},
  volume = {139},
  timestamp = {2017-01-18T20:05:45Z},
  number = {7},
  journal = {Journal of Statistical Planning and Inference},
  author = {Fox, Richard J. and Elgart, David and Davis, S. Christopher},
  year = {7/1 2009},
  keywords = {Bayesian,Credible intervals,Design of experiment,Response surface},
  pages = {2498--2501},
  groups = {refworks}
}

@article{fraser1961,
  title = {On {{Fiducial Inference}}},
  volume = {32},
  timestamp = {2017-01-18T20:05:45Z},
  number = {3},
  journal = {The Annals of Mathematical Statistics},
  author = {Fraser, D. A. S.},
  month = sep,
  year = {1961},
  pages = {661--676},
  groups = {refworks}
}

@article{frenkel2005,
  title = {Monte {{Carlo}}-Based Estimation of Uncertainty Owing to Limited Resolution of Digital Instruments},
  volume = {42},
  abstract = {We have used a Monte Carlo simulation for investigating the output of a digital instrument, of resolution half-width a , when its analogue input is a Gaussian signal. The digitizing process converts an actual mean to an observed mean and an actual variance to an observed variance. The resulting relationships are plotted as two sets of graphs, in one of which the observed mean is the parameter, while in the other the observed variance is the parameter. Given an observed mean and an observed variance, it is then easy to infer from the graphs the corresponding actual mean and actual variance. The graphs illustrate, for example, that unless the observed variance is very low, the actual variance can be recovered from the observed variance by subtracting a 2 /3 from the observed variance.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {5},
  journal = {Metrologia},
  author = {Frenkel, R. B. and Kirkup, L.},
  year = {2005},
  pages = {L27},
  groups = {refworks}
}

@article{geer1992,
  title = {A {{Mathematical Analysis}} of {{Information}}-{{Preserving Transformations}} between {{Probabalistic}} and {{Possibilistic Formulations}} of {{Uncertainty}}},
  volume = {20},
  abstract = {It is now generally recognized that uncertainty can be formalized in different mathematical theories. Two of these theories, on which we focus in this paper, are probability theory and possibility theory. The paper deals with transformations from probabilistic formalizations of uncertainty into their possibilistic counterparts that contain the same amount of uncertainty and, consequently, the same amount of information (expressed as a reduction of uncertainty) as well; it also deals with the inverse uncertainty and information preserving transformations. Since well-justified and unique measures of uncertainty (and information) are now well established in both probability theory and possibility theory, the transformations are well defined. Mathematical properties of the transformations are analyzed in the paper under the assumption that probabilities and possibilities are connected via interval or log-interval scales. The primary results are: (i) the interval scale transformation that preserves information exists and is unique only from probability theory to possibility theory, but the inverse transformation does not always exist; (ii) the log-interval scale transformation exists and is unique in both directions; and (iii) the log-interval scale transformation satisfies the probability-possibility consistency requirement.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {2},
  journal = {International Journal of General Systems},
  author = {Geer, James F. and Klir, George J.},
  year = {1992},
  pages = {143},
  groups = {refworks}
}

@incollection{goldblatt2006,
  series = {Handbook of the History of Logic},
  title = {Mathematical {{Modal Logic}}: A {{View}} of Its {{Evolution}}},
  volume = {Vol 7},
  timestamp = {2017-01-18T20:05:45Z},
  publisher = {{Elsevier}},
  author = {Goldblatt, Robert},
  editor = {Gabbay, Dov M. and Woods, John},
  year = {2006},
  pages = {1--98},
  groups = {refworks}
}

@book{grabe2010,
  address = {Heidelberg},
  title = {Generalized {{Gaussian Error Calculus}}},
  isbn = {978-3-642-03304-9},
  timestamp = {2017-01-18T20:05:45Z},
  publisher = {{Springer-Verlag}},
  author = {Grabe, Michael},
  year = {2010},
  groups = {refworks}
}

@article{grabe2001a,
  title = {Estimation of Measurement Uncertainties - an Alternative to the {{ISO Guide}}},
  volume = {38},
  abstract = {The procedure for uncertainty assessment set out in the Guide to the Expression of Uncertainty in Measurement, published by the International Organization for Standardization (ISO), is compared with an alternative system. The ISO Guide links the overall uncertainty to a so-called kP factor, implicitly expressing the degree of confidence that the measured result should cover the true value of the physical quantity in question. Here it is argued that this standard procedure, in which the magnitude of a given kp factor is tied to the degree of confidence, presents the experimenter with difficulties. These arise from the need to associate a probability density with so-called unknown systematic errors. Consequently, the recommended procedures may not be taken up by experimenters who carry out laboratory measurements on a daily basis. A simplification of the ISO procedure suggested by the Guide involves the assignment of the values 1 or 2 (or even 3) to kP. The alternative approach presented here argues that this simplification does not reflect the prevailing physical situation, and that the degree of confidence obtained lacks physical objectivity. Stationary measurement processes, strictly separating random and unknown systematic errors, are considered. Random errors are assumed to be normally distributed, and no probability distribution density is attributed to the unknown systematic errors. As random and systematic errors are kept separate, it is possible to express the influence of random errors by generalized confidence intervals (from Student's t-distribution) and the influence of systematic errors by worst-case estimates. No confidence statement is associated with the overall uncertainties, and the intrinsic problem of the Guide is circumvented. The formalisms are robust, transparent, and locate measured quantities with reasonable reliability.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {2},
  journal = {Metrologia},
  author = {Grabe, Michael},
  year = {2001},
  pages = {97--106},
  groups = {refworks}
}

@article{grabe1987,
  title = {Principles of {{Metrological Statistics}}},
  volume = {23},
  abstract = {That portion of applied statistics which is used for the treatment of experimental data is essentially founded on unbiased estimators - notwithstanding the fact that any real measurement process implies so-called unknown systematic errors. Until now, those perturbations giving rise to unknown deviations from the true values have been formally randomized, i.e. they are treated as if they were of random origin. Biased estimators thus again become fictitiously unbiased. However, the associated procedures suffer from two disadvantages: they are too complicated and, in the author's opinion, they generate uncertainties which are not sufficiently reliable. The submitted paper attempts to establish "Metrological Statistics" which, being based on biased estimators, is much simpler to handle and which, in particular, yields safe estimates for measurement uncertainties.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {4},
  journal = {Metrologia},
  author = {Grabe, Michael},
  year = {1987},
  pages = {213--219},
  groups = {refworks}
}

@article{grabe1978,
  title = {Note on the {{Application}} of the {{Method}} of {{Least Squares}}},
  volume = {14},
  abstract = {The efficiency of the method of least squares depends substantially upon the choice of the design matrix, since the latter is responsible for the transfer of the uncertainties, inherent in the initial observations, to the unknown parameters. The present paper suggests that the transfer is best if, given the diagonal elements AiAi of the product matrix A A, the columns of the design matrix A are mutually orthogonal.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {4},
  journal = {Metrologia},
  author = {Grabe, Michael},
  year = {1978},
  pages = {143--146},
  groups = {refworks}
}

@article{gregory2005,
  title = {A Standard Approach to Measurement Uncertainties for Scientists and Engineers in Medicine},
  volume = {28},
  abstract = {The critical nature of health care demands high performance levels from medical equipment. To ensure these performance levels are maintained, medical physicists and biomedical engineers conduct a range of measurements on equipment during acceptance testing and on-going quality assurance programs. Wherever there are measurements, there are measurement uncertainties with potential conflicts between the measurements made by installers, owners and occasionally regulators. Prior to 1993, various methods were used to calculate and report measurement uncertainties. In 1993, the International Organization for Standardization published the Guide to the Expression of Uncertainty in Measurement (GUM). The document was jointly published with six international organizations principally involved in measurements and standards. The GUM is regarded as an international benchmark on how measurement uncertainty should be calculated and reported. Despite the critical nature of these measurements, there has not been widespread use of the GUM by medical physicists and biomedical engineers. This may be due to the complexity of the GUM. Some organisations have published guidance on the GUM tailored to specific measurement disciplines. This paper presents the philosophy behind the GUM, and demonstrates, with a medical physics measurement example, how the GUM recommends uncertainties be calculated and reported.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {2},
  journal = {Australasian Physical \& Engineering Science in Medicine},
  author = {Gregory, K. and Bibbo, G. and Pattison, J.},
  year = {2005},
  keywords = {Biomedicine},
  pages = {131--139},
  groups = {refworks}
}

@book{hajek2009,
  title = {Stanford {{Encyclopedia}} of {{Philosophy}} - {{Interpretations}} of {{Probability}}},
  volume = {2011},
  timestamp = {2017-01-18T20:05:45Z},
  number = {1/11},
  author = {H{\'a}jek, Alan},
  month = dec,
  year = {2009},
  groups = {refworks}
}

@article{hall2008,
  title = {Evaluating Methods of Calculating Measurement Uncertainty},
  volume = {45},
  abstract = {This communication demonstrates the need for independent validation when an uncertainty calculation procedure is applied to a particular type of measurement problem. A simple measurement scenario is used to highlight differences in the performance of two general methods of uncertainty calculation, one from the Guide to the Expression of Uncertainty in Measurement (GUM) and one from Supplement 1 to the 'Guide to the Expression of Uncertainty in Measurement'\textendash{}Propagation of Distributions using a Monte Carlo method. The performance of these methods is investigated in terms of the long-run success rate when applied to many simulated measurements in the scenario. An individual application of the method is deemed successful if an uncertainty interval containing the measurand is obtained. The alternative approach to validation taken in the Supplement, that an uncertainty interval calculated by a Monte Carlo method can be used to validate the GUM method, is not consistent with the results of this study.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {2},
  journal = {Metrologia},
  author = {Hall, B. D.},
  year = {2008},
  pages = {5--8},
  groups = {refworks}
}

@article{halpern1999,
  title = {A {{Counter Example}} to {{Theorems}} of {{Cox}} and {{Fine}}},
  volume = {10},
  timestamp = {2017-01-18T20:05:45Z},
  journal = {Journal of Artificail Intelligence Research},
  author = {Halpern, J. Y.},
  year = {1999},
  pages = {67--85},
  groups = {refworks}
}

@article{hannig2006,
  title = {Fiducial {{Generalized Confidence Intervals}}},
  volume = {101},
  timestamp = {2017-01-18T20:05:45Z},
  journal = {Journal of the American Statistical Association},
  author = {Hannig, Jan and Iyer, Hari and Patterson, Paul},
  year = {2006},
  pages = {254--269},
  groups = {refworks}
}

@article{hannig2007,
  title = {Fiducial Approach to Uncertainty Assessment Accounting for Error Due to Instrument Resolution},
  volume = {44},
  abstract = {This paper presents an approach for making inference on the parameters {\^I}\textonequarter{} and {\"I}$f$ of a Gaussian distribution in the presence of resolution errors. The approach is based on the principle of fiducial inference and requires a Monte Carlo method for computing uncertainty intervals. A small simulation study is carried out to evaluate the performance of the proposed procedure and compare it with some of the existing procedures. The results indicate that the fiducial procedure is comparable to the best of the competing procedures for inference on {\^I}\textonequarter. However, unlike some of the competing procedures, the same Monte Carlo calculations also provide inference for {\"I}$f$ and many other related quantities of interest.},
  timestamp = {2017-01-18T20:05:45Z},
  number = {6},
  journal = {Metrologia},
  author = {Hannig, Jan and Iyer, Hari K. and Wang, C. M.},
  year = {2007},
  pages = {476},
  groups = {refworks}
}

@book{harriman2010,
  title = {The Logical Leap : Induction in Physics},
  isbn = {978-0-451-23005-8 0-451-23005-1},
  language = {English},
  timestamp = {2017-01-18T20:05:45Z},
  publisher = {{New American Library, New York}},
  author = {Harriman, David},
  year = {2010},
  groups = {refworks}
}

@article{hartley1928,
  title = {Transmission of {{Information}}},
  volume = {7},
  timestamp = {2017-01-18T20:05:45Z},
  number = {3},
  journal = {Bell System Technical Journal},
  author = {Hartley, R. V. L.},
  month = jul,
  year = {1928},
  pages = {535},
  groups = {refworks}
}

@article{herrey1965,
  title = {Confidence {{Intervals Based}} on the {{Mean Absolute Deviation}} of a {{Normal Sample}}},
  volume = {60},
  abstract = {Confidence intervals for the population mean of a normal distribution can be determined from the distribution of the variate \$H = $\backslash$sqrt n($\backslash$bar x - $\backslash$mu)/d\$, analogue to Student's t-distribution but based on the mean absolute deviation d instead of the standard deviation. The H-distribution is derived: the frequency function is symmetric about zero, with central ordinate \$(1/$\backslash$pi)$\backslash$sqrt(1 - n\^-1)\$; asymptotically it is normal, \$N(0, $\backslash$sqrt($\backslash$pi/2))\$. An approximate formula for the calculation of the percent values is developed and numerical factors tabulated by which the mean absolute deviation of a normal sample of size n is to be multiplied in order to obtain 95 percent and 50 percent confidence limits of the mean, for n = 2(1)15(5)30, 40, 60, 120. It is shown that the increase in length against confidence intervals from the standard deviation and Student's t is negligible. The usefulness of confidence intervals from the mean absolute deviation for estimating the precision of measurements in Physics is discussed.},
  language = {English},
  timestamp = {2017-01-18T20:05:45Z},
  number = {309},
  journal = {Journal of the American Statistical Association},
  author = {Herrey, Erna M. J.},
  month = mar,
  year = {1965},
  pages = {pp. 257--269},
  groups = {refworks}
}

@article{hibbert2007,
  title = {Systematic Errors in Analytical Measurement Results},
  volume = {1158},
  timestamp = {2017-01-18T20:05:45Z},
  number = {1-2},
  journal = {Journal of Chromatography A},
  author = {Hibbert, D. Brynn},
  year = {7/27 2007},
  keywords = {Bias,measurement uncertainty,Metrological traceability,Recovery,Systematic errors},
  pages = {25--32},
  groups = {refworks}
}

@article{horn2003,
  title = {Constructing a Logic of Plausible Inference: A Guide to {{Cox}}'s Theorem},
  volume = {34},
  timestamp = {2017-01-18T20:05:45Z},
  number = {1},
  journal = {International Journal of Approximate Reasoning},
  author = {Horn, Kevin S. Van},
  month = sep,
  year = {2003},
  keywords = {Bayesian,Cox,probability},
  pages = {3--24},
  groups = {refworks}
}

@article{hund2001,
  title = {Operational Definitions of Uncertainty},
  volume = {20},
  timestamp = {2017-01-18T20:05:45Z},
  number = {8},
  journal = {TrAC Trends in Analytical Chemistry},
  author = {Hund, Edelgard and Massart, D. Luc and Smeyers-Verbeke, Johanna},
  month = aug,
  year = {2001},
  keywords = {Bias,Measurement error,measurement uncertainty,Traceability},
  pages = {394--406},
  groups = {refworks}
}

@book{jalkio2001,
  title = {Technical {{Report}} on the {{Lateral Resolution Test}} for Optical Non-Contact Probes Submitted to {{ASME}} Standards Committee {{B89}}.4.14},
  timestamp = {2017-01-18T20:05:45Z},
  author = {Jalkio, J. A.},
  year = {2001},
  groups = {refworks}
}

@techreport{jalkio1999b,
  type = {SME Technical Paper},
  title = {The {{Use}} of {{Optical Probes}} on {{Coordinate Measuring Machines}} \textendash{} {{Strengths}} and {{Weaknesses}}},
  timestamp = {2017-01-18T20:05:45Z},
  number = {IQ99-206},
  institution = {Society of Manufacturing Engineers},
  author = {Jalkio, J. A.},
  year = {1999},
  groups = {refworks}
}

@techreport{jalkio1987,
  type = {SME Technical Paper},
  title = {A {{Survey}} of {{Non Contact Range Measurement Techniques}}},
  timestamp = {2017-01-18T20:05:45Z},
  number = {IQ87-858},
  institution = {Society of Manufacturing Engineers},
  author = {Jalkio, J. A.},
  year = {1987},
  groups = {refworks}
}

@article{jalkio1985,
  title = {Three {{Dimensional Inspection}} Using {{Multi}}-Stripe {{Structured Light}}},
  volume = {24},
  timestamp = {2017-01-18T20:05:46Z},
  number = {6},
  journal = {Optical Engineering},
  author = {Jalkio, J. A. and Kim, R. C. and Case, S. K.},
  year = {1985},
  pages = {966--974},
  groups = {refworks,self}
}

@book{jaynes2003,
  address = {New York},
  title = {Probability {{Theory}}: {{The Logic}} of {{Science}}},
  isbn = {978-0-521-59271-0},
  timestamp = {2017-01-18T20:05:46Z},
  publisher = {{Cambridge University Press}},
  author = {Jaynes, E. T.},
  year = {2003},
  note = {edited by Bretthorst, G. Larry},
  groups = {refworks}
}

@article{kacker2003,
  title = {On Use of {{Bayesian}} Statistics to Make the {{Guide}} to the {{Expression}} of {{Uncertainty}} in {{Measurement}} Consistent},
  volume = {40},
  abstract = {The International Organization for Standardization (ISO) Guide to the Expression of Uncertainty in Measurement is being increasingly recognized as a de facto international standard. The ISO Guide recommends a standardized way of expressing uncertainty in all kinds of measurements and provides a comprehensive approach for combining information to evaluate that uncertainty. The ISO Guide supports uncertainties evaluated from statistical methods, Type A, and uncertainties determined by other means, Type B. The ISO Guide recommends classical (frequentist) statistics for evaluating the Type A components of uncertainty; but it interprets the combined uncertainty from a Bayesian viewpoint. This is inconsistent. In order to overcome this inconsistency, we suggest that all Type A uncertainties should be evaluated through a Bayesian approach. It turns out that the estimates from a classical statistical analysis are either equal or approximately equal to the corresponding estimates from a Bayesian analysis with non-informative prior probability distributions. So the classical (frequentist) estimates may be used provided they are interpreted from the Bayesian viewpoint. The procedure of the ISO Guide for evaluating the combined uncertainty is to propagate the uncertainties associated with the input quantities. This procedure does not yield a complete specification of the distribution represented by the result of measurement and its associated combined standard uncertainty. So the correct coverage factor for a desired coverage probability of an expanded uncertainty interval cannot always be determined. Nonetheless, the ISO Guide suggests that the coverage factor may be computed by assuming that the distribution represented by the result of measurement and its associated standard uncertainty is a normal distribution or a scaled-and-shifted t -distribution with degrees of freedom determined from the Welch\textendash{}Satterthwaite formula. This assumption may be unjustified and the coverage factor so determined may be incorrect. A popular convention is to set the coverage factor as 2. When the distribution represented by the result of measurement and its associated standard uncertainty is not completely determined, the 2-standard-uncertainty interval may be interpreted in terms of its minimum coverage probability for an applicable class of probability distributions.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {5},
  journal = {Metrologia},
  author = {Kacker, R. and Jones, A.},
  year = {2003},
  pages = {235},
  groups = {refworks}
}

@article{kacker2007,
  title = {Evolution of Modern Approaches to Express Uncertainty in Measurement},
  volume = {44},
  abstract = {An object of this paper is to discuss the logical development of the concept of uncertainty in measurement and the methods for its quantification from the classical error analysis to the modern approaches based on the Guide to the Expression of Uncertainty in Measurement (GUM) . We review authoritative literature on error analysis and then discuss its limitations which motivated the experts from the International Committee for Weights and Measures (CIPM), the International Bureau of Weights and Measures (BIPM) and various national metrology institutes to develop specific recommendations which form the basis of the GUM. We discuss the new concepts introduced by the GUM and their merits and limitations. The limitations of the GUM led the BIPM Joint Committee on Guides in Metrology to develop an alternative approach?the draft Supplement 1 to the GUM (draft GUM-S1). We discuss the draft GUM-S1 and its merits and limitations. We hope this discussion will lead to a more effective use of the GUM and the draft GUM-S1 and stimulate investigations leading to further improvements in the methods to quantify uncertainty in measurement.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {6},
  journal = {Metrologia},
  author = {Kacker, R. and Sommer, K. and Kessel, R.},
  year = {2007},
  pages = {513},
  groups = {refworks}
}

@article{kacker2006,
  title = {Bayesian Alternative to the {{ISO}}-{{GUM}}'s Use of the {{Welch}}\textendash{}{{Satterthwaite}} Formula},
  volume = {43},
  abstract = {In certain disciplines, uncertainty is traditionally expressed as an interval about an estimate for the value of the measurand. Development of such uncertainty intervals with a stated coverage probability based on the International Organization for Standardization (ISO) Guide to the Expression of Uncertainty in Measurement (GUM) requires a description of the probability distribution for the value of the measurand. The ISO-GUM propagates the estimates and their associated standard uncertainties for various input quantities through a linear approximation of the measurement equation to determine an estimate and its associated standard uncertainty for the value of the measurand. This procedure does not yield a probability distribution for the value of the measurand. The ISO-GUM suggests that under certain conditions motivated by the central limit theorem the distribution for the value of the measurand may be approximated by a scaled-and-shifted t -distribution with effective degrees of freedom obtained from the Welch\textendash{}Satterthwaite (W\textendash{}S) formula. The approximate t -distribution may then be used to develop an uncertainty interval with a stated coverage probability for the value of the measurand. We propose an approximate normal distribution based on a Bayesian uncertainty as an alternative to the t -distribution based on the W\textendash{}S formula. A benefit of the approximate normal distribution based on a Bayesian uncertainty is that it greatly simplifies the expression of uncertainty by eliminating altogether the need for calculating effective degrees of freedom from the W\textendash{}S formula. In the special case where the measurand is the difference between two means, each evaluated from statistical analyses of independent normally distributed measurements with unknown and possibly unequal variances, the probability distribution for the value of the measurand is known to be a Behrens\textendash{}Fisher distribution. We compare the performance of the approximate normal distribution based on a Bayesian uncertainty and the approximate t -distribution based on the W\textendash{}S formula with respect to the Behrens\textendash{}Fisher distribution. The approximate normal distribution is simpler and better in this case. A thorough investigation of the relative performance of the two approximate distributions would require comparison for a range of measurement equations by numerical methods.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {1},
  journal = {Metrologia},
  author = {Kacker, Raghu N.},
  year = {2006},
  pages = {1},
  groups = {refworks}
}

@article{kalman1960,
  title = {A {{New Approach}} to {{Linear Filtering}} and {{Prediction Problems}}},
  volume = {82},
  timestamp = {2017-01-18T20:05:46Z},
  number = {Series D},
  journal = {Transactions of the ASME\textendash{}Journal of Basic Engineering},
  author = {Kalman, Rudolph Emil},
  year = {1960},
  pages = {35--45},
  groups = {refworks}
}

@article{kane1997,
  title = {Analytical {{Bias}}: The {{Neglected Component}} of {{Measurement Uncertainty}}},
  volume = {122},
  abstract = {All analysts are accustomed to reporting measurement results accompanied by either the standard deviation of individual results from the mean or the standard error of the mean. These statements indicate repeatability of measurement under unchanged conditions, or reproducibility, where the time period over which measurements are taken is the source of changed conditions. Repeatability generally produces a smaller deviation between replicates than does reproducibility. However, most of the variability of measurements made by different laboratories, or using different methods in a single laboratory, is not accounted for by either repeatability or reproducibility attributable to time period. This is evident in all interlaboratory data sets; these frequently contain between-laboratory and between-method discrepancies that are very large in comparison with the uncertainty of measurements typically reported by individual laboratories. Recently issued ISO guidelines and related documents address these discrepancies as a legitimate component of measurement uncertainty, and recommend expanding the concept to include the deviation of a measurement from the true value of the measurand, so long as this deviation is small relative to fitness for purpose requirements. When the bias of measurement renders the result unsuitable for purpose, however, that bias is a significant effect and must be removed by use of a correction factor. To correct for significant bias, or to include smaller bias as an uncertainty component, laboratories must evaluate and quantify the bias in their measurements to the fullest extent possible. This paper presents uncertainty statements developed in accordance with the ISO guidelines for several reference sample measurements. Some are very complex, drawn from published work of metrology laboratories. Others, drawn from the authors data for the Japanese Sedimentary rock reference materials, are more suitable for routine laboratory use.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {11},
  journal = {Analyst},
  author = {Kane, Jean S.},
  year = {1997},
  pages = {1283--1288},
  groups = {refworks}
}

@article{kerridge1961,
  title = {Inaccuracy and {{Inference}}},
  volume = {23},
  abstract = {The idea of uncertainty, as developed in communication theory, is generalized to "inaccuracy". The applications of this inaccuracy to the problems of statistical inference are discussed.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {1},
  journal = {Journal of the Royal Statistical Society.Series B (Methodological)},
  author = {Kerridge, D. F.},
  year = {1961},
  pages = {184--194},
  groups = {refworks}
}

@book{kessel2006,
  title = {Coefficient of Contribution to the Combined Standard Uncertainty},
  volume = {43},
  isbn = {0026-1394},
  abstract = {The International Organization for Standardization (ISO) Guide to the Expression of Uncertainty in Measurement (GUM) describes a generic procedure for determining an estimate for the value of the measurand and its associated combined standard uncertainty from the estimates and their associated standard uncertainties for various input quantities. A user of the ISO-GUM who is interested in understanding, managing or improving the measurement procedure needs the details, usually expressed as an uncertainty budget, on how the estimate for the value of the measurand and its associated combined standard uncertainty were calculated. In particular, a user may be interested in quantifying the degrees of contribution to the combined standard uncertainty from its components. When the measurement equation is a linear function of uncorrelated input variables, the contribution from a component is usually quantified by the product of the component of uncertainty and its sensitivity coefficient. This paper introduces a coefficient of contribution that is suitable for both uncorrelated and correlated input variables. The proposed coefficient of contribution is useful for a variety of measurement equations. Correlations between input variables can significantly alter the relative importance of the contributions to the combined standard uncertainty from its components.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {4},
  author = {Kessel, R{\"u}diger and Kacker, Raghu and Berglund, Michael},
  year = {2006},
  groups = {refworks}
}

@inproceedings{kirkup2004,
  address = {Melbourne},
  title = {Reforming the Teaching of Uncertainty to Undergraduate Science and Engineering Students},
  timestamp = {2017-01-18T20:05:46Z},
  publisher = {{Metrology Society of Australia}},
  author = {Kirkup, L.},
  month = mar,
  year = {2004},
  pages = {21--25},
  groups = {refworks}
}

@article{kirkup2002,
  title = {A Guide to {{GUM}}},
  volume = {23},
  abstract = {It is almost ten years since the International Organisation for Standardisation (ISO) published guidelines for calculating and expressing uncertainty in values obtained through measurement. The guidelines are commonly referred to as 'GUM'. Despite the emphasis placed on errors and uncertainties in laboratory work in introductory physics courses, awareness of these guidelines is not widespread. Much like the SI system of units, which has taken some time to become established world wide, the ISO guidelines are likely to take many years to become widely known and consistently applied outside the 'metrology community'. Here an introduction is presented to calculating and expressing uncertainty, as recommended in the ISO document, with explanations of some of the more unfamiliar terms it employs, such as standard uncertainty, coverage factor and expanded uncertainty.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {5},
  journal = {European Journal of Physics},
  author = {Kirkup, L.},
  year = {2002},
  pages = {483--487},
  groups = {refworks}
}

@book{kirkup2006,
  address = {New york, N.Y.},
  title = {An {{Introduction}} to {{Uncertainty}} in {{Measurement}}},
  isbn = {0-521-84428-2},
  timestamp = {2017-01-18T20:05:46Z},
  publisher = {{Cambridge University Press}},
  author = {Kirkup, L. and Frenkel, R. B.},
  year = {2006},
  groups = {refworks}
}

@book{kirkup1998,
  title = {Designing a New Physics Laboratory Programme for First-Year Engineering Students},
  volume = {33},
  isbn = {0031-9120},
  abstract = {Physics laboratories can play a key role in the education of engineering students. Laboratory programmes should be stimulating, revealing and rewarding, though many fall short of these ambitious aims. Much has been written on improving physics laboratories for physics and science students, but far less on physics laboratory work for engineering students. Perhaps the assumption is made that what is `good for first-year physics majors' is also good for first-year engineering majors. But is this true? Here we explore the issue of physics laboratory work for engineering students and discuss the design, implementation and evaluation of a new laboratory programme developed for first-year engineering students. Both qualitative and quantitative evaluation data suggest that the new programme better serves the needs and expectations of engineering academics and students than the `old' programme.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {4},
  author = {Kirkup, L. and Johnson, S. and Hazel, E. and Cherry, R. W. and Green, D. C. and Swift, P. and Holliday, W.},
  year = {1998},
  groups = {refworks}
}

@article{kociszewski1985,
  title = {On the Calculations of Maximum Entropy Distributions Having Prescribed the Moments},
  volume = {18},
  abstract = {It is shown that maximum entropy distributions having prescribed the finite number of moments can be determined from a suitable system of differential equations.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {7},
  journal = {Journal of Physics A: Mathematical and General},
  author = {Kociszewski, A.},
  year = {1985},
  pages = {337},
  groups = {refworks}
}

@article{krouwer2003,
  title = {Critique of the {{Guide}} to the {{Expression}} of {{Uncertainty}} in {{Measurement Method}} of {{Estimating}} and {{Reporting Uncertainty}} in {{Diagnostic Assays}}},
  volume = {49},
  abstract = {Background: The Guide to the Expression of Uncertainty in Measurement (GUM) provides instructions for constructing uncertainty intervals for a measurement. This method is usually reserved for reference materials, but GUM has been recently proposed as a way to express uncertainty for commercial diagnostic assays. Methods: Using the official GUM standard and published applications of GUM to commercial diagnostic assays, I undertook an analysis to evaluate whether applying GUM to commercial diagnostic assays is warranted. Results: Certain important assays, such as troponin I, would not be candidates for GUM because troponin I is not a well-defined physical quantity. Unlike definitive methods, in which efforts are taken to detect and eliminate all systematic error sources, commercial assays often trade off features such as ease of use and cost with accuracy and allow systematic errors to be present as long as the overall accuracy meets the medical need goal. Laboratories are hindered in preparing GUM models because the knowledge required to specify some systematic errors is often available only to manufacturers. Some non-GUM methods to estimate uncertainty rely on observed data, which include both known and unknown sources of error. The occurrence of large, unknown errors for assays in routine use (e.g., outliers) is not unusual because diagnostic assays must be chemically specific in the presence of thousands of potentially interfering substances. There is no provision in GUM to deal with unexplained outliers, which may lead to uncertainty intervals that are not wide enough. Some clinicians assume that diagnostic assay results have little uncertainty. This situation may be made worse by including an uncertainty interval, which implies certification. Conclusions: Evaluations for accuracy (total analytical error) based on describing the distribution of result differences between commercial assays and reference methods indicate that some assays have a few results with large differences (e.g., outliers). This leads to a wide accuracy interval (total analytical error limits). It is unlikely that GUM would be able to predict these wide intervals, especially because there is little or no provision for outlier treatment in GUM. Presenting too narrow GUM uncertainty intervals to clinicians would be misleading. The modeling used by practitioners of the GUM method is potentially useful in improving quality, but commercial diagnostic assays are not ready for GUM uncertainty statements.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {11},
  journal = {Clinical chemistry},
  author = {Krouwer, Jan S.},
  month = nov,
  year = {2003},
  pages = {1818--1821},
  groups = {refworks}
}

@article{krystek2015,
  title = {The Term `Dimension' in the International System of Units},
  volume = {52},
  abstract = {The term `dimension' as used in the International System of Units (SI) is not well defined. Moreover, the term `quantity of dimension one' (formerly called `dimensionless quantity', a term which is unfortunately still sometimes used) as defined in the International Vocabulary of Metrology (VIM) is mathematically unsound. In order to overcome these difficulties, a new quantity dimension, called `dimension number', will be introduced in this paper. It will be shown that such a dimension necessarily belongs to any system of dimensions , but is not a base dimension.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {2},
  url = {http://stacks.iop.org/0026-1394/52/i=2/a=297},
  journal = {Metrologia},
  author = {Krystek, M. P.},
  year = {2015},
  pages = {297},
  groups = {refworks}
}

@article{kung2005,
  title = {Teaching the Concepts of Measurement: {{An}} Example of a Concept-Based Laboratory Course},
  volume = {73},
  timestamp = {2017-01-18T20:05:46Z},
  number = {8},
  journal = {American Journal of Physics},
  author = {Kung, Rebecca Lippmann},
  month = aug,
  year = {2005},
  keywords = {educational courses,measurement theory,measurement uncertainty,physics education,student experiments},
  pages = {771--777},
  groups = {refworks}
}

@article{kyriazis2008,
  title = {Comparison of {{GUM Supplement}} 1 and {{Bayesian}} Analysis Using a Simple Linear Calibration Model},
  volume = {45},
  abstract = {Recent work referred to two approaches for doing a Bayesian analysis for simple linear calibration and pointed out that there could be a difference between the results of applying the procedure of GUM Supplement 1 and one of those approaches. It will be shown that the difference between the two Bayesian approaches reflects the use of two different priors. It will be shown that the results obtained by GUM Supplement 1 are those of a Bayesian analysis with commonly used priors and a measurement equation that satisfies the principles of the Guide to the Expression of Uncertainty in Measurement (GUM).},
  timestamp = {2017-01-18T20:05:46Z},
  number = {2},
  journal = {Metrologia},
  author = {Kyriazis, G. A.},
  year = {2008},
  pages = {9--11},
  groups = {refworks}
}

@article{lampasi2008,
  title = {An Alternative Approach to Measurement Based on Quantile Functions},
  volume = {41},
  timestamp = {2017-01-18T20:05:46Z},
  number = {9},
  journal = {Measurement},
  author = {Lampasi, D. A.},
  month = nov,
  year = {2008},
  keywords = {Coverage intervals,Digital signal processing,measurement uncertainty,Monte Carlo method,probability,Quantile},
  pages = {994--1013},
  groups = {refworks}
}

@article{laviolette1995,
  title = {A {{Probabilistic}} and {{Statistical View}} of {{Fuzzy Methods}}},
  volume = {37},
  abstract = {Fuzzy set theory has primarily been associated with control theory and with the representation of uncertainty in applications in artificial intelligence. More recently, fuzzy methods have been proposed as alternatives to traditional statistical methods in statistical quality control, linear regression, and forecasting, among other areas. We review some basic concepts of fuzzy methods, point out some philosophical and practical problems, and offer simpler alternatives based on traditional probability and statistical theory. Applications in control theory and statistical quality control serve as our primary examples.},
  language = {English},
  timestamp = {2017-01-18T20:05:46Z},
  number = {3},
  journal = {Technometrics},
  author = {Laviolette, Michael and Jr, John W. Seaman and Barrett, J. Douglas and Woodall, William H.},
  month = aug,
  year = {1995},
  pages = {pp. 249--261},
  groups = {refworks}
}

@article{lira2010,
  title = {Reply to '{{Probability}}, Belief and Success Rate: Comments on {{On}} the Meaning of Coverage Probabilities'},
  volume = {47},
  abstract = {In 2009 Metrologia [/0026-1394/46/6/002] 46 616\textendash{}8 it is claimed that a coverage probability associated with an interval derived from a state-of-knowledge probability distribution should not be interpreted as a relative frequency of successful intervals in a large series of imagined or simulated intervals. In his comments, Willink affirms that this claim is untenable. This reply is limited to a summary restatement of the two examples used to support the above assertion.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {3},
  journal = {Metrologia},
  author = {Lira, I.},
  year = {2010},
  pages = {347},
  groups = {refworks}
}

@article{lira2009a,
  title = {Comment on '{{A}} Probabilistic Approach to the Analysis of Measurement Processes'},
  volume = {46},
  abstract = {The formula for the consumer's risk that appeared in a recent paper (Cox et al 2008 Metrologia [/0026-1394/45/5/001] 45 493\dbend{}502 ) is compared with that in the author's previously published research (Lira 1999 Metrologia [/0026-1394/36/5/1] 36 397\dbend{}402 ), which is herein summarized.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {1},
  journal = {Metrologia},
  author = {Lira, I.},
  year = {2009},
  pages = {L8},
  groups = {refworks}
}

@article{lira2009,
  title = {On the Meaning of Coverage Probabilities},
  volume = {46},
  timestamp = {2017-01-18T20:05:46Z},
  number = {6},
  journal = {Metrologia},
  author = {Lira, I.},
  year = {2009},
  pages = {616},
  groups = {refworks}
}

@article{lira2008,
  title = {On the Long-Run Success Rate of Coverage Intervals},
  volume = {45},
  abstract = {When constructing a coverage interval from the probability density function that describes the state of knowledge about a measurand, it seems reasonable to expect that the long-run success rate of that interval will be about equal to the stipulated coverage probability. Through a specific example, the validity of this criterion is examined.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {4},
  journal = {Metrologia},
  author = {Lira, I.},
  year = {2008},
  pages = {L21},
  groups = {refworks}
}

@book{lira2002,
  address = {Philadelphia, PA},
  title = {Evaluating the {{Measurement Uncertainty}}: {{Fundamentals}} and {{Practical Guidance}}},
  isbn = {978-0-7503-0840-3},
  timestamp = {2017-01-18T20:05:46Z},
  publisher = {{Taylor \& Francis}},
  author = {Lira, I.},
  year = {2002},
  groups = {refworks}
}

@book{lira1999,
  title = {A {{Bayesian}} Approach to the Consumer's and Producer's Risks in Measurement},
  volume = {36},
  isbn = {0026-1394},
  abstract = {In industrial practice, the decision to accept or reject an inspected item is usually made on the basis of measurement information. Since this information is rarely complete, it is not possible in general to be absolutely certain about the value of the measurand. As a consequence, incorrect decisions may be made. In this paper, formulae for the probabilities of improperly accepting or rejecting an item are derived. Bayesian statistics provides the theoretical framework, and use is made of the Principle of Maximum Entropy. Applications to the inspection of workpieces and to the verification of measuring instruments are considered and examples are given. The conventional, frequency-based approach is also discussed.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {5},
  author = {Lira, I.},
  year = {1999},
  groups = {refworks}
}

@book{lira2010a,
  title = {Bayesian Assessment of Uncertainty in Metrology: A Tutorial},
  volume = {47},
  isbn = {0026-1394},
  abstract = {The publication of the Guide to the Expression of Uncertainty in Measurement (GUM), and later of its Supplement 1, can be considered to be landmarks in the field of metrology. The second of these documents recommends a general Monte Carlo method for numerically constructing the probability distribution of a measurand given the probability distributions of its input quantities. The output probability distribution can be used to estimate the fixed value of the measurand and to calculate the limits of an interval wherein that value is expected to be found with a given probability. The approach in Supplement 1 is not restricted to linear or linearized models (as is the GUM) but it is limited to a single measurand. In this paper the theory underlying Supplement 1 is re-examined with a view to covering explicit or implicit measurement models that may include any number of output quantities. It is shown that the main elements of the theory are Bayes' theorem, the principles of probability calculus and the rules for constructing prior probability distributions. The focus is on developing an analytical expression for the joint probability distribution of all quantities involved. In practice, most times this expression will have to be integrated numerically to obtain the distribution of the output quantities, but not necessarily by using the Monte Carlo method. It is stressed that all quantities are assumed to have unique values, so their probability distributions are to be interpreted as encoding states of knowledge that are (i) logically consistent with all available information and (ii) conditional on the correctness of the measurement model and on the validity of the statistical assumptions that are used to process the measurement data. A rigorous notation emphasizes this interpretation.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {3},
  author = {Lira, I. and Grientschnig, D.},
  year = {2010},
  groups = {refworks}
}

@article{lira2006,
  title = {Comparison between the Conventional and {{Bayesian}} Approaches to Evaluate Measurement Data},
  volume = {43},
  timestamp = {2017-01-18T20:05:46Z},
  number = {4},
  journal = {Metrologia},
  author = {Lira, I. and W{\"o}ger, W.},
  year = {2006},
  pages = {S249--S259},
  groups = {refworks}
}

@article{little2006,
  title = {Calibrated {{Bayes}}: {{A Bayes}}/{{Frequentist Roadmap}}},
  volume = {60},
  abstract = {No abstract is available for this item.},
  timestamp = {2017-01-18T20:05:46Z},
  journal = {The American Statistician},
  author = {Little, Roderick J.},
  month = aug,
  year = {2006},
  pages = {213--223},
  groups = {refworks}
}

@article{liu2004,
  title = {Some Simple Corrected Confidence Intervals Following a Sequential Test},
  volume = {126},
  timestamp = {2017-01-18T20:05:46Z},
  number = {1},
  journal = {Journal of Statistical Planning and Inference},
  author = {Liu, Wei},
  year = {11/1 2004},
  keywords = {Approximate pivot,Asymptotic moment expansions,Confidence level,Coverage probability,Sequential test},
  pages = {189--205},
  groups = {refworks}
}

@article{mari2005,
  title = {The Problem of Foundations of Measurement},
  volume = {38},
  abstract = {Given the common assumption that measurement plays an important role in the foundation of science, the paper analyzes the possibility that Measurement Science, and therefore measurement itself, can be properly founded. The realist and the representational positions are analyzed at this regards: the conclusion, that such positions unavoidably lead to paradoxical situations, opens the discussion for a new epistemology of measurement, whose characteristics and interpretation are sketched here but are still largely matter of investigation.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {4},
  journal = {Measurement},
  author = {Mari, Luca},
  month = dec,
  year = {2005},
  keywords = {Foundations of measurement,Measurement science,Uncertainty in measurement},
  pages = {259--266},
  groups = {refworks}
}

@article{martens2002,
  title = {Evaluation of Uncertainty in Measurements\textemdash{}problems and Tools},
  volume = {38},
  timestamp = {2017-01-18T20:05:46Z},
  number = {3-4},
  journal = {Optics and Lasers in Engineering},
  author = {von Martens, Hans-J{\"u}rgen},
  month = oct,
  year = {2002},
  keywords = {Accuracy,Calibration,Error analysis,Laser interferometer,measurement,Measuring instrument,Transducer,Uncertainty evaluation,Vibration},
  pages = {185--206},
  groups = {refworks}
}

@article{mauris2001,
  title = {A Fuzzy Approach for the Expression of Uncertainty in Measurement},
  volume = {29},
  abstract = {This paper deals with a fuzzy expression of uncertainty in measurement. The fuzzy approach proposed consists of representing measurements by a family of intervals of confidence stacked atop one another, that in fact define the upper bound of the probability distributions consistent with these intervals of confidence. This approach is compatible with the ISO Guide for the expression of uncertainty in measurement, and is particularly interesting because it allows both the handling of specificity and uncertainty of measurement. Moreover, fuzzy uncertainty propagation is available thanks to fuzzy arithmetic, which is a generalization of interval analysis, yielding both worst case results and best estimates at the same time. In order to simplify the propagation, a parameterized possibility distribution approximating the optimal one is proposed and compared with the probabilistic approaches.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {3},
  journal = {Measurement},
  author = {Mauris, Gilles and Lasserre, Virginie and Foulloy, Laurent},
  month = apr,
  year = {2001},
  keywords = {Fuzzy subset theory,measurement uncertainty,possibility theory,probability theory},
  pages = {165--177},
  groups = {refworks}
}

@inproceedings{mencattini2008,
  title = {A Comparison between Different Methods for Processing the Random Part of Random-Fuzzy Variables Representing Measurement Results},
  abstract = {In the recent years, fuzzy variables and random-fuzzy variables have been proposed to represent the measurement results with their associated uncertainty. However, up to now, the different authors do not yet agree in the mathematical way fuzzy variables should be composed together, so that different approaches have been proposed. This paper compares these approaches, in order to find their advantages and disadvantages and shows a new proposal, that is supposed to overcome, hopefully, the disadvantages of the original ones.},
  timestamp = {2017-01-18T20:05:46Z},
  booktitle = {Advanced {{Methods}} for {{Uncertainty Estimation}} in {{Measurement}}, 2008. {{AMUEM}} 2008. {{IEEE International Workshop}} On},
  author = {Mencattini, A. and Salicone, S.},
  month = jul,
  year = {2008},
  keywords = {fuzzy logic,measurement uncertainty,random contributions,random-fuzzy variables,random processes,t-norms,uncertainty estimation},
  pages = {72},
  groups = {refworks}
}

@article{michell2005,
  title = {The Logic of Measurement: {{A}} Realist Overview},
  volume = {38},
  abstract = {According to the realist interpretation, measurement commits us not just to the logically independent existence of things in space and time, but also to the existence of quantitatively structured properties and relations, and to the existence of real numbers, understood as relations of ratio between specific levels of such attributes. Measurement is defined as the estimation of numerical relations (or ratios) between magnitudes of a quantitative attribute and a unit. The history of scientific measurement, from antiquity to the present may be interpreted as revealing a progressive deepening in the understanding of this position. First, the concept of ratio was broadened to include ratios between incommensurable magnitudes; second, the concept of a quantitative attribute was broadened to include non-extensive quantities; third, quantitative structure and its relations to ratios and real numbers were elaborated; and finally, the issue of empirically distinguishing between quantitative and non-quantitative structures was addressed. This interpretation of measurement understands it in a way that is continuous with scientific investigation in general, i.e., as an attempt to discover independently existing facts.},
  timestamp = {2017-01-18T20:05:46Z},
  number = {4},
  journal = {Measurement},
  author = {Michell, Joel},
  month = dec,
  year = {2005},
  keywords = {History of measurement,Philosophy of measurement,Realism},
  pages = {285--294},
  groups = {refworks}
}

@book{moore1966,
  address = {Englewood Cliff, New Jersey},
  title = {Interval {{Analysis}}},
  isbn = {0-13-476853-1},
  timestamp = {2017-01-18T20:05:47Z},
  publisher = {{Prentice Hall}},
  author = {Moore, R. E.},
  year = {1966},
  groups = {refworks}
}

@article{muller2008,
  title = {{{MUSE}}: Computational Aspects of a {{GUM}} Supplement 1 Implementation},
  volume = {45},
  abstract = {The new guideline GUM Supplement 1\textendash{}Propagation of Distributions Using a Monte Carlo Method (GS1) is currently published by JCGM/WG1. It describes an approximate method to calculate the measurement uncertainty in nearly all areas of metrology. In this way it overcomes the various limitations and drawbacks of the uncertainty propagation detailed in GUM. However, GS1 demands a software implementation in contrast to the uncertainty propagation. Therefore we have developed a software tool called MUSE (Measurement Uncertainty Simulation and Evaluation), which is a comprehensive implementation of GS1. In this paper we present the major computational aspects of the software which are the sampling from probability density functions (PDFs), an efficient way to propagate the PDFs with the help of a block design through the equation of the measurand and the calculation of the summarizing parameters based on these blocks. Also the different quality measures which are in place during the life cycle of the tool are elaborated.},
  timestamp = {2017-01-18T20:05:47Z},
  number = {5},
  journal = {Metrologia},
  author = {Muller, Martin and Wolf, Marco and Rosslein, Matthias},
  year = {2008},
  pages = {586--594},
  groups = {refworks}
}

@article{mysore1995a,
  title = {Evaluation of {{High Precision Triangulation Sensors}} for {{Coordinate}} Measurement},
  timestamp = {2017-01-18T20:05:47Z},
  journal = {Proceedings of the 1995 ASPE annual meeting},
  author = {Mysore, A. and Jalkio, J. A. and Konicek, L.},
  year = {1995},
  groups = {refworks}
}

@article{nyquist1928,
  title = {Thermal {{Agitation}} of {{Electric Charge}} in {{Conductors}}},
  volume = {32},
  timestamp = {2017-01-18T20:05:47Z},
  number = {1},
  journal = {Phys.Rev.},
  author = {Nyquist, H.},
  month = jul,
  year = {1928},
  pages = {110--113},
  groups = {refworks}
}

@article{nyquist1924,
  title = {Certain {{Factors Affecting Telegraph Speed}}},
  volume = {3},
  timestamp = {2017-01-18T20:05:47Z},
  number = {2},
  journal = {Bell System Technical Journal},
  author = {Nyquist, H.},
  month = apr,
  year = {1924},
  pages = {324},
  groups = {refworks}
}

@article{pavese2009,
  title = {About the Treatment of Systematic Effects in Metrology},
  volume = {42},
  abstract = {A comparison of the text of VIM recent III Edition with that of the GUM and of its contemporary VIM II Edition alights significant differences in the definition of basic measurement terms in the two documents, and with respect to the basic written standards in the field of testing, ISO 5725 and ISO 3534. The paper intends to introduce author's interpretation of these \textendash{} and companion \textendash{} texts, concerning specifically the terminology and the statistical treatment of the influence quantities and of the effects of their variability (in time and standard-to-standard), either related to replicated measurements performed on a single standard (standard `reproducibility') or to the comparisons of different standards, thus involving the concept of `accuracy' and its estimate, and consequently directly relevant to traceability. Another question that arose a few years ago was whether different types of measurand could be the consequence of the different intrinsic nature of different types of standards. It prompted an analysis that resulted in the proposal of considering two distinct `classes' of standards. These classes require different answers to the issue of the treatment of systematic effects. The distinction is relevant, in particular, to the statistical treatment of comparison data, which form the basis of the traceability assessment. The paper is presenting a discussion on the implications of the above distinction, concentrating on cases where systematic effects are dominating the experimental results, a common case in several metrology fields, and on ways to tackle the problem of the correction required by the GUM for standards of class 2 (standards whose values are accurate measures of a common measurand) \textendash{} a class often not recognised in the general literature.},
  timestamp = {2017-01-18T20:05:47Z},
  number = {10},
  journal = {Measurement},
  author = {Pavese, Franco},
  month = dec,
  year = {2009},
  keywords = {metrology,Systematic effects,Systematic error,Uncertainty},
  pages = {1459--1462},
  groups = {refworks}
}

@article{pedersen1978,
  title = {Fiducial {{Inference}}},
  volume = {46},
  timestamp = {2017-01-18T20:05:47Z},
  number = {2},
  journal = {International Statistical Review / Revue Internationale de Statistique},
  author = {Pedersen, J. G.},
  month = aug,
  year = {1978},
  pages = {147--170},
  groups = {refworks}
}

@book{pennecchi2010,
  title = {Uncertainty Evaluation for the Estimate of a Complex-Valued Quantity Modulus},
  volume = {47},
  isbn = {0026-1394},
  abstract = {This paper analyses two possible estimators for the modulus of a complex-valued quantity. Their expectations are here recalled from a previous work and their variances and mean square errors (MSEs) are analytically derived in the case of uncorrelated, homoskedastic and normally distributed measurements of the real and imaginary parts of the considered quantity. Numerical simulations are performed in the case of (uncorrelated) heteroskedastic measurements of the real and imaginary parts. The two estimators are compared by studying the behaviour of their variances and MSEs, and some guidance is given on the best choice between the two, depending on the specific experimental context. First and second-order approximations for their variance, hence for the uncertainty associated with the estimates they produce, are also provided in a more general case, that is, for correlated and heteroskedastic measurements of the real and imaginary parts. Examples from the metrological and the physical fields are given in order to compare the estimators on actual data.},
  timestamp = {2017-01-18T20:05:47Z},
  number = {3},
  author = {Pennecchi, F. and Oberto, L.},
  year = {2010},
  groups = {refworks}
}

@article{pertile2008,
  title = {Uncertainty Evaluation for Complex Propagation Models by Means of the Theory of Evidence},
  volume = {19},
  abstract = {The present paper describes operating procedures for uncertainty expression and propagation using different approaches. Well-known methods, such as the propagation formula of the GUM (Guide to the Expression of Uncertainty in Measurement) and Monte Carlo method, are briefly described and summarized as operating procedures, while a more detailed description of the new approach based on the theory of evidence and random-fuzzy variables (RFVs) is presented. This new method based on RFV allows us to take into explicit account and to properly manage systematic effects and complete ignorance contributions to uncertainty. For all three methods, concise and schematic procedures are presented in order to give a clear comparison among them and to ease implementation. Particular attention is focused on how uncertainty can be expressed and propagated in an indirect measurement through a mathematical model. Furthermore, this paper proposes a generalized method to express and propagate uncertainty by means of RFV. This proposed method is characterized by its applicability to any type of mathematical model, even if it comprises complex numerical functions or algorithms.},
  timestamp = {2017-01-18T20:05:47Z},
  number = {5},
  journal = {Measurement Science and Technology},
  author = {Pertile, Marco and Cecco, Mariolino De},
  year = {2008},
  pages = {055103 (10pp)},
  groups = {refworks}
}

@article{pertile2010,
  title = {Comparison between Two Modern Uncertainty Expression and Propagation Approaches},
  volume = {238},
  timestamp = {2017-01-18T20:05:47Z},
  journal = {Journal of Physics: Conference Series},
  author = {Pertile, M. and Debei, S.},
  year = {2010},
  pages = {012033},
  groups = {refworks}
}

@article{pillay2008,
  title = {Effectiveness of a {{GUM}}-Compliant Course for Teaching Measurement in the Introductory Physics Laboratory},
  volume = {29},
  abstract = {An evaluation of a course aimed at developing university students' understanding of the nature of scientific measurement and uncertainty is described. The course materials follow the framework for metrology as recommended in the Guide to the Expression of Uncertainty in Measurement (GUM). The evaluation of the course is based on responses to written questionnaires administered to a cohort of 76 first year physics students both pre- and post-instruction, which were interpreted in terms of 'point' or 'set' reasoning. These findings are compared with responses from a control group of 70 students who completed a similar laboratory course apart from the use of traditional approaches to measurement and data analysis. The results suggest that the GUM framework, together with the specific teaching strategies described, provides opportunities for more effective learning of measurement and uncertainty in the introductory laboratory.},
  timestamp = {2017-01-18T20:05:47Z},
  number = {3},
  journal = {European Journal of Physics},
  author = {Pillay, Seshini and Buffler, Andy and Lubben, Fred and Allie, Saalih},
  year = {2008},
  pages = {647--659},
  groups = {refworks}
}

@article{possolo2007,
  title = {Assessment of Measurement Uncertainty via Observation Equations},
  volume = {44},
  timestamp = {2017-01-18T20:05:47Z},
  number = {6},
  journal = {Metrologia},
  author = {Possolo, Antonio and Toman, Blaza},
  year = {2007},
  pages = {464--475},
  groups = {refworks}
}

@article{possolo2009,
  title = {Contribution to a Conversation about the {{Supplement}} 1 to the {{GUM}}},
  volume = {46},
  timestamp = {2017-01-18T20:05:47Z},
  number = {1},
  journal = {Metrologia},
  author = {Possolo, Antonio and Toman, Blaza and Estler, Tyler},
  year = {2009},
  pages = {L1--L7},
  groups = {refworks}
}

@book{press2007,
  address = {Hong Kong},
  title = {Numerical {{Recipes}}: {{The Art}} of {{Scientific Computing}}},
  isbn = {978-0-521-88068-8},
  timestamp = {2017-01-18T20:05:47Z},
  publisher = {{Cambridge University Press}},
  author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
  year = {2007},
  groups = {refworks}
}

@article{priel2009,
  title = {From {{GUM}} to Alternative Methods for Measurement Uncertainty Evaluation},
  volume = {14},
  abstract = {Since the advent of the Guide to the expression of Uncertainty in Measurement (GUM) in 1995 laying the principles of uncertainty evaluation numerous projects have been carried out to develop alternative practical methods that are easier to implement namely when it is impossible to model the measurement process for technical or economical aspects. In this paper, the author presents the recent evolution of measurement uncertainty evaluation methods. The evaluation of measurement uncertainty can be presented according to two axes based on intralaboratory and interlaboratory approaches. The intralaboratory approach includes the modelling approach (application of the procedure described in section 8 of the GUM, known as GUM uncertainty framework) and the single laboratory validation approach . The interlaboratory approaches are based on collaborative studies and they are respectively named interlaboratory validation approach and proficiency testing approach .},
  timestamp = {2017-01-18T20:05:47Z},
  number = {5},
  journal = {Accreditation and Quality Assurance: Journal for Quality, Comparability and Reliability in Chemical Measurement},
  author = {Priel, Marc},
  year = {2009},
  keywords = {Engineering},
  pages = {235--241},
  groups = {refworks}
}

@book{rabinovich2005b,
  address = {New York, New York},
  title = {Measurement {{Errors}} and {{Uncertainties}}},
  isbn = {0-387-25358-0},
  timestamp = {2017-01-18T20:05:47Z},
  publisher = {{Springer}},
  author = {Rabinovich, Semyon G.},
  year = {2005},
  groups = {refworks}
}

@article{rossi2006,
  title = {A Probabilistic Theory of Measurement},
  volume = {39},
  timestamp = {2017-01-18T20:05:47Z},
  number = {1},
  journal = {Measurement},
  author = {Rossi, Giovanni Battista},
  month = jan,
  year = {2006},
  keywords = {Measurement model,measurement theory,probability,Representational theory of measurement},
  pages = {34--50},
  groups = {refworks}
}

@article{rossi2003,
  title = {A Probabilistic Model for Measurement Processes},
  volume = {34},
  timestamp = {2017-01-18T20:05:47Z},
  number = {2},
  journal = {Measurement},
  author = {Rossi, Giovanni Battista},
  month = sep,
  year = {2003},
  keywords = {measurement theory,Probabilistic models,Uncertainty in measurement},
  pages = {85--99},
  groups = {refworks}
}

@article{rossi2006a,
  title = {A Probabilistic Approach to Measurement-Based Decisions},
  volume = {39},
  abstract = {Measurements are often an objective support for decision-making, in all fields, scientific, industrial or human-related. In this last case particular care has to be taken as a wrong decision can affect health, life or environment.},
  timestamp = {2017-01-18T20:05:47Z},
  number = {2},
  journal = {Measurement},
  author = {Rossi, Giovanni Battista and Crenna, Francesco},
  month = feb,
  year = {2006},
  keywords = {Conformity assessment,Measurement-based decisions,Probabilistic models for measurement,Risk evaluation,Uncertainty of measurement},
  pages = {101--119},
  groups = {refworks}
}

@article{rubin1984,
  title = {Bayesianly {{Justifiable}} and {{Relevant Frequency Calculations}} for the {{Applies Statistician}}},
  volume = {12},
  abstract = {A common reaction among applied statisticians is that the Bayesian statistician's energies in an applied problem must be directed at the a priori elicitation of one model specification from which an optimal design and all inferences follow automatically by applying Bayes's theorem to calculate conditional distributions of unknowns given knowns. I feel, however, that the applied Bayesian statistician's tool-kit should be more extensive and include tools that may be usefully labeled frequency calculations. Three types of Bayesianly justifiable and relevant frequency calculations are presented using examples to convey their use for the applied statistician.},
  timestamp = {2017-01-18T20:05:47Z},
  number = {4},
  journal = {The Annals of Statistics},
  author = {Rubin, Donald B.},
  month = dec,
  year = {1984},
  pages = {1151--1172},
  groups = {refworks}
}

@book{russell1948,
  address = {London},
  title = {Human {{Knowledge}}: {{Its Scope}} and {{Limits}}},
  timestamp = {2017-01-18T20:05:47Z},
  publisher = {{Routledge}},
  author = {Russell, Bertrand},
  year = {1948},
  groups = {refworks}
}

@book{salicone2007,
  address = {New York},
  title = {Measurement {{Uncertainty}}, {{An Approach}} via the {{Mathematical Theory}} of {{Evidence}}},
  isbn = {0-387-3065502},
  timestamp = {2017-01-18T20:05:47Z},
  publisher = {{Springer}},
  author = {Salicone, Simona},
  year = {2007},
  groups = {refworks}
}

@article{satterthwaite1946,
  title = {An {{Approximate Distribution}} of {{Estimates}} of {{Variance Components}}},
  volume = {2},
  abstract = {In moments folder Approach used in GUM to approximate number of degrees of freedom},
  timestamp = {2017-01-18T20:05:47Z},
  number = {6},
  journal = {Biometrics Bulletin},
  author = {Satterthwaite, F. E.},
  year = {1946},
  keywords = {asymptotics,statistical_theory},
  pages = {110--114},
  groups = {refworks}
}

@article{shafer2004,
  title = {Comments on {{Constructing}} a Logic of Plausible Inference: A Guide to {{Cox}}'s {{Theorem}}, by {{Kevin S}}. {{Van Horn}}},
  volume = {35},
  abstract = {Professor Van Horn [International Journal of Approximate Reasoning 34 (2003) 3] has reviewed much of the work that followed Richard Cox's publication of his axioms for probability in 1946. My comments will emphasize work that came earlier and also merits attention. In particular, I will discuss work by Sergei Bernstein that is closely related to Cox's but has been neglected by Cox and his commentators. Before reviewing Bernstein's work, I will discuss the context of Cox's work and explain why it did not dissuade me from studying alternative representations of uncertainty. After discussing Bernstein, I will make just one comment on Van Horn's presentation."},
  timestamp = {2017-01-18T20:05:47Z},
  number = {1},
  journal = {International Journal of Approximate Reasoning},
  author = {Shafer, Glenn},
  year = {2004},
  pages = {97},
  groups = {refworks}
}

@article{shannon1948,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  volume = {27},
  timestamp = {2017-01-18T20:05:47Z},
  number = {3,4},
  journal = {Bell System Technical Journal},
  author = {Shannon, C. E.},
  year = {1948},
  pages = {379--423;623--656},
  groups = {refworks}
}

@book{sim2008,
  title = {Evaluating Expanded Uncertainty in Measurement with a Fitted Distribution},
  volume = {45},
  isbn = {0026-1394},
  abstract = {It is recommended in the internationally adopted standard GUM that the measurement result of a measurand Y , determined from input quantities through a known functional relationship, should be reported as an uncertainty interval with a specific level of confidence. The uncertainty intervals discussed in GUM are evaluated by assuming that the measurements of Y follow a normal or t distribution. However, in practice, the resulting measurements of Y tend to follow an asymmetric distribution. This communication reviews some generalized univariate distributions that can be fitted to the measurements of Y based on its shape in terms of its coefficients of skewness and kurtosis. The uncertainty interval of the measurand is then evaluated from the fitted distribution. Examples are given to illustrate the use of these distributions in evaluating the expanded uncertainty in measurement.},
  timestamp = {2017-01-18T20:05:47Z},
  number = {2},
  author = {Sim, C. H. and Lim, M.H.},
  year = {2008},
  groups = {refworks}
}

@article{styer1996,
  title = {Common Misconceptions Regarding Quantum Mechanics},
  volume = {64},
  timestamp = {2017-01-18T20:05:47Z},
  number = {1},
  journal = {American Journal of Physics},
  author = {Styer, Daniel F.},
  month = jan,
  year = {1996},
  keywords = {EDUCATION},
  pages = {31--34},
  groups = {refworks}
}

@incollection{swartz1998,
  address = {New York},
  edition = {1},
  series = {Teaching Introductory Physics : A Sourcebook},
  title = {Error ({{Uncertainty}}) {{Analysis}}},
  isbn = {1-56396-320-5},
  timestamp = {2017-01-18T20:05:47Z},
  publisher = {{Springer-Verlag}},
  author = {Swartz, Clifford E. and Miner, Thomas},
  year = {1998},
  pages = {53--70},
  groups = {refworks}
}

@article{sydenham2003,
  title = {Relationship between Measurement, Knowledge and Advancement},
  volume = {34},
  abstract = {Many have observed that measurement, knowledge generation and advancement are companion activities. The evidence for this comes from numerous sources over the long history of mankind. Better understanding of these connections appears able to yield major advances in knowledge generation and use of measurement. Current formal understanding of the meaningful nature of measurements lags behind other aspects of the practice of measurement science. It is a major area needing attention through application of the differing kinds of thinking found in the hard and soft sciences. The state of understanding and trends of these relationships are reviewed from a measurement practitioner's viewpoint.},
  timestamp = {2017-01-18T20:05:47Z},
  number = {1},
  journal = {Measurement},
  author = {Sydenham, Peter H.},
  month = jul,
  year = {2003},
  keywords = {Advancement,Human progress,Information management,Knowledge,Philosophy of measurement},
  pages = {3--16},
  groups = {refworks}
}

@article{synek2007,
  title = {Effect of Insignificant Bias and Its Uncertainty on the Coverage Probability of Uncertainty Intervals: {{Part}} 2. {{Evaluation}} for a Found Insignificant Experimental Bias},
  volume = {71},
  timestamp = {2017-01-18T20:05:47Z},
  number = {3},
  journal = {Talanta},
  author = {Synek, V{\'a}clav},
  year = {2/28 2007},
  keywords = {Bias,Correction for systematic error,Coverage probability,Level of significance,t-Test,Uncertainty},
  pages = {1304--1311},
  groups = {refworks}
}

@article{synek2006,
  title = {Effect of Insignificant Bias and Its Uncertainty on the Coverage Probability of Uncertainty Intervals: {{Part}} 1. {{Evaluation}} for a given Value of the True Bias},
  volume = {70},
  timestamp = {2017-01-18T20:05:47Z},
  number = {5},
  journal = {Talanta},
  author = {Synek, V{\'a}clav},
  year = {12/15 2006},
  keywords = {Bias,Correction for systematic error,Coverage probability,t-Test,Uncertainty,-Error},
  pages = {1024--1034},
  groups = {refworks}
}

@article{synek2005,
  title = {Attempts to Include Uncorrected Bias in the Measurement Uncertainty},
  volume = {65},
  timestamp = {2017-01-18T20:05:47Z},
  number = {4},
  journal = {Talanta},
  author = {Synek, V{\'a}clav},
  year = {2/28 2005},
  keywords = {Bias,Coverage probability,Level of confidence,Systematic error,Uncertainty},
  pages = {829--837},
  groups = {refworks}
}

@book{taylor1996,
  address = {New York, N.Y.},
  title = {Introduction to {{Error Analysis}}: {{The Study}} of {{Uncertainties}} in {{Physical Measurements}}},
  isbn = {0-935702-75-X},
  timestamp = {2017-01-18T20:05:47Z},
  publisher = {{University Science Books}},
  author = {Taylor, J. R.},
  year = {1996},
  groups = {refworks}
}

@techreport{taylor1994,
  type = {NIST Technical Note},
  title = {Guidelines for {{Evaluating}} and {{Expressing}} the {{Uncertainty}} of {{NIST Measurement Results}}},
  timestamp = {2017-01-18T20:05:47Z},
  number = {1297},
  author = {Taylor, N. and Kuyatt, C. E.},
  year = {1994},
  groups = {refworks}
}

@incollection{tukey1960,
  address = {Stanford},
  series = {Contributions to Probability and Statistics},
  title = {A Survey of Sampling from Contaminated Distributions},
  timestamp = {2017-01-18T20:05:47Z},
  publisher = {{Stanford University Press}},
  author = {Tukey, J. W.},
  editor = {Olkin, I. and Ghurye, S. and Hoeffding, W. and Madow, W. and Mann, H.},
  year = {1960},
  keywords = {robust statistics},
  pages = {448--485},
  groups = {refworks}
}

@article{volkwyn2008,
  title = {Impact of a Conventional Introductory Laboratory Course on the Understanding of Measurement},
  volume = {4},
  timestamp = {2017-01-18T20:05:47Z},
  number = {1},
  journal = {Phys.Rev.ST Phys.Educ.Res.},
  author = {Volkwyn, Trevor S. and Allie, Saalih and Buffler, Andy and Lubben, Fred},
  month = may,
  year = {2008},
  pages = {010108},
  groups = {refworks}
}

@article{wagner1979,
  title = {On the Quantitative Characterization of the Uncertainty of Experimental Results in Metrology},
  volume = {89},
  timestamp = {2017-01-18T20:05:47Z},
  journal = {PTB Mitt.},
  author = {Wagner, S. R.},
  year = {1979},
  pages = {83--89},
  groups = {refworks}
}

@book{wang2009,
  title = {Fiducial Intervals for the Magnitude of a Complex-Valued Quantity},
  volume = {46},
  isbn = {0026-1394},
  abstract = {This paper discusses a fiducial approach for constructing uncertainty intervals for the distance between k normal means and the origin. When k = 2 this distance is equivalent to the magnitude of a complex-valued quantity. A simulation study was conducted to assess the frequentist performance of the proposed fiducial intervals and to compare their performance with the methods from the Guide to the Expression of Uncertainty in Measurement and from Supplement 1 to the 'Guide to the Expression of Uncertainty in Measurement'{\^a}\texteuro''Propagation of Distributions using a Monte Carlo Method . Our results indicate that the fiducial intervals generally outperform the GUM and GUM Supplement 1 methods with respect to frequentist coverage probabilities. Computer programs for calculating the fiducial intervals, written using open-source software, are listed.},
  timestamp = {2017-01-18T20:05:47Z},
  number = {1},
  author = {Wang, C. M. and Hari, K.Iyer},
  year = {2009},
  groups = {refworks}
}

@book{wang2006,
  title = {Uncertainty Analysis for Vector Measurands Using Fiducial Inference},
  volume = {43},
  isbn = {0026-1394},
  abstract = {This paper presents a method for constructing uncertainty regions for a vector measurand in the presence of both type-A and type-B errors. The method is based on the principle of fiducial inference and generally requires a Monte Carlo approach for computing uncertainty regions. A small simulation study is carried out to evaluate the performance of the proposed method. Computer programs written using public-domain software for computing uncertainty regions are listed. An example, involving complex S -parameter measurements, is used to illustrate the proposed method.},
  timestamp = {2017-01-18T20:05:47Z},
  number = {6},
  author = {Wang, C. M. and Hari, K.Iyer},
  year = {2006},
  groups = {refworks}
}

@article{weibull1951,
  title = {A {{Statistical Distribution Function}} of {{Wide Applicability}}},
  volume = {18},
  timestamp = {2017-01-18T20:05:47Z},
  number = {3},
  journal = {Journal of Applied Mechanics},
  author = {Weibull, Waloddi},
  month = sep,
  year = {1951},
  pages = {293--297},
  groups = {refworks}
}

@article{weise1993,
  title = {A {{Bayesian}} Theory of Measurement Uncertainty},
  volume = {4},
  abstract = {The theory of measurement uncertainty outlined forms the mathematical foundation of the results of international discussions of many years. In contrast to the usual approaches in the literature, it is based entirely on Bayesian statistics and on the principle of maximum entropy. Individual and mutual uncertainties of measurement results are expressed and treated by a covariance matrix in conformity with international recommendations. The theory is universally applicable to most measurement data evaluation tasks including complex nonlinear adjustments and, in particular, in cases where the well-established least-squares or maximum-likelihood techniques fail, as in strongly underdetermined few-channel particle energy spectrum unfolding problems.},
  timestamp = {2017-01-18T20:05:47Z},
  number = {1},
  journal = {Measurement Science and Technology},
  author = {Weise, K. and W{\"o}ger, W.},
  year = {1993},
  pages = {1},
  groups = {refworks}
}

@book{willink2010,
  title = {Probability, Belief and Success Rate: Comments on '{{On}} the Meaning of Coverage Probabilities'},
  volume = {47},
  isbn = {0026-1394},
  abstract = {The method of uncertainty evaluation discussed in Supplement 1 to the Guide to the Expression of Uncertainty in Measurement generates a coverage interval in which the measurand is said to have a certain probability (the coverage probability) of lying. This communication contains a response to the recent claim that 'when a coverage interval summarizes the resulting state of knowledge, the coverage probability should not be interpreted as a relative frequency of successful intervals in a large series of imagined or simulated intervals' (Lira 2009 Metrologia [/0026-1394/46/6/002] 46 616{\^a}\texteuro``8 ). First, Bernoulli's law of large numbers is used to prove that the long-run success rate of a methodology used to calculate 95\% coverage intervals must be 95\%. Second, the usual definition of subjective probability or 'degree of belief' is stated, and the weak law of large numbers is then used to show that this definition{\^a}\texteuro''and the corresponding definition of 'state of knowledge'{\^a}\texteuro''relies on the concept of long-run behaviour. This provides an alternative proof of the same result.},
  timestamp = {2017-01-18T20:05:47Z},
  number = {3},
  author = {Willink, R.},
  year = {2010},
  groups = {refworks}
}

@book{willink2010b,
  title = {Difficulties Arising from the Representation of the Measurand by a Probability Distribution},
  volume = {21},
  isbn = {0957-0233},
  abstract = {This paper identifies difficulties associated with the concept of representing fixed unknown quantities by probability distributions. This concept, which we refer to as the distributed-measurand concept, is at the heart of the approach to the evaluation of measurement uncertainty described in Supplement 1 to the Guide to the Expression of Uncertainty in Measurement . The paper notes (i) the resulting lack of invariance of measurement results to nonlinear reparametrizations of the measurement problem, (ii) the potential undetected divergence of measurement estimates obtained by Monte Carlo evaluation, (iii) the potential failure of the methodology to give uncertainty intervals enclosing the values of the measurands with an acceptable frequency and (iv) the potential loss of measurement precision. The distributed-measurand concept is gaining popularity partly because of its association with analysis using the Monte Carlo principle. However, the Monte Carlo principle is also applicable without adopting the distributed-measurand concept. Accordingly, an alternative approach to the evaluation of measurement uncertainty is briefly described.},
  timestamp = {2017-01-18T20:05:47Z},
  number = {1},
  author = {Willink, R.},
  year = {2010},
  groups = {refworks}
}

@book{willink2010a,
  title = {On the Validity of Methods of Uncertainty Evaluation},
  volume = {47},
  isbn = {0026-1394},
  abstract = {This paper continues a discussion generated by a recent paper of Hall ( 2008 Metrologia [/0026-1394/45/2/n01] 45 L5{\^a}\texteuro``8 ) regarding the performance of methods of uncertainty evaluation. The 'validity' of a method of generating intervals of measurement uncertainty is identified principally with the frequency with which these intervals contain the measurand. Two approaches to the evaluation of such intervals are described, and their performances are compared for the simple measurement function appearing in Hall's paper. The first is a Bayesian approach, which is consistent with the numerical method described in Supplement 1 to the Guide to the Expression of Uncertainty in Measurement . The second is an approach based on frequentist principles. Simulations with fixed values of the unknown parameters are conducted to find the rate at which the methods generate intervals containing the value of the measurand and to find the mean widths of the intervals produced. The results show that the standard Bayesian procedure and its modifications can perform poorly. In contrast, the frequentist procedure achieves the required rate of 0.95 while generating intervals of similar width.},
  timestamp = {2017-01-18T20:05:47Z},
  number = {1},
  author = {Willink, R.},
  year = {2010},
  groups = {refworks}
}

@book{willink2009,
  title = {Representing {{Monte Carlo}} Output Distributions for Transferability in Uncertainty Analysis: Modelling with Quantile Functions},
  volume = {46},
  isbn = {0026-1394},
  abstract = {Ideally, the output of one calculation of measurement uncertainty should be usable as an input to another calculation. This paper describes (i) how the output distribution of a Monte Carlo (MC) evaluation of uncertainty can be summarized for use in another analysis and (ii) how a general probability distribution can be summarized for efficient use as an MC input distribution. The principal technique discussed involves fitting an asymmetric form of 'lambda distribution' to the summarizing data. This distribution is defined by the inverse of its distribution function, so the generation of random samples from this distribution is straightforward. The inverse of the distribution function is known as the 'quantile function'. The principle advocated is that of working with distributions with convenient quantile functions instead of distributions with convenient probability density functions. This principle is applicable whether the distributions represent sampling distributions, as in frequentist statistics, or patterns of belief, as in Bayesian statistics.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {3},
  author = {Willink, R.},
  year = {2009},
  groups = {refworks}
}

@book{willink2009a,
  title = {A Formulation of the Law of Propagation of Uncertainty to Facilitate the Treatment of Shared Influences},
  volume = {46},
  isbn = {0026-1394},
  abstract = {The law of propagation of uncertainty requires the proper treatment of dependence between the estimates of quantities influencing the measurement. Sometimes these estimates are themselves obtained in previous separate measurements, and the correlation between these estimates is not recorded. In such a case, the corresponding correlation coefficient or covariance will wrongly be deemed to be zero. In other situations, the number of input or output quantities might be large and information about dependence might not be recorded because of the apparent need for the construction of a large matrix of covariances. This paper presents (i) a method whereby information about dependence is recorded in a table with, typically, fewer columns than the matrix and (ii) a corresponding re-expression of the law of propagation of uncertainty. Examples of the application of the method are given.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {3},
  author = {Willink, R.},
  year = {2009},
  groups = {refworks}
}

@book{willink2008b,
  title = {What Is Robustness in Data Analysis?},
  volume = {45},
  isbn = {0026-1394},
  abstract = {The idea of using a 'robust' method of data analysis seems popular. Many scientists will regard such a method as being superior to one for which no such property is claimed. But what actually is a 'robust' method? This paper discusses what is meant by robustness in statistical analysis and, by implication, in general data analysis. It is motivated by recent debate surrounding the estimation of the Newtonian constant G from a set of measurements.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {4},
  author = {Willink, R.},
  year = {2008},
  groups = {refworks}
}

@book{willink2008a,
  title = {An Inconsistency in Uncertainty Analysis Relating to Effective Degrees of Freedom},
  volume = {45},
  isbn = {0026-1394},
  abstract = {An internally consistent procedure of uncertainty analysis will give the same results no matter how the calculations are legitimately rearranged. This paper identifies an apparent internal inconsistency arising with the full procedure described in the Guide to the Expression of Uncertainty in Measurement . The phenomenon occurs when the measurand is expressible as a function of quantities with common sources, and is related to the calculation of an effective number of degrees of freedom. One common situation in which this inconsistency arises is where a point on a line is measured using estimates of the intercept and slope derived from the same dataset; the numbers of degrees of freedom obtained differ according to whether this point is measured directly from the dataset or from the estimates of the intercept and slope. An example is given where the widths of the corresponding expanded uncertainty intervals differ by a factor of approximately 2. Statistical principles explaining this issue are discussed, and a principle for the calculation of an effective number of degrees of freedom is proposed.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {1},
  author = {Willink, R.},
  year = {2008},
  groups = {refworks}
}

@book{willink2008,
  title = {Estimation and Uncertainty in Fitting Straight Lines to Data: Different Techniques},
  volume = {45},
  isbn = {0026-1394},
  abstract = {This paper describes three different techniques for fitting straight lines to experimental data and discusses the corresponding evaluation of uncertainty. The techniques are (i) traditional fitting by least-squares, (ii) a Bayesian linear-regression analysis and (iii) an analysis according to the propagation of probability density functions attributed to the points measured. The material is presented to clarify assumptions underlying the techniques, to highlight differences between the techniques and to point to difficulties associated with applying the techniques under current views of 'uncertainty analysis'. Considerable attention is given to the estimation of values of the function and not just to the estimation of parameters of the function. The paper gives a summary of many results of least-squares fitting, including some unfamiliar results for the simultaneous estimation of the unknown function at all points. On many occasions the unknown function will only be approximately linear, in which case we must define a unique unknown gradient to give proper meaning to our 'estimate' of slope. This can be achieved by defining an interval of interest and then applying a least-squares-type result.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {3},
  author = {Willink, R.},
  year = {2008},
  groups = {refworks}
}

@book{willink2007d,
  title = {On the Uncertainty of the Mean of Digitized Measurements},
  volume = {44},
  isbn = {0026-1394},
  abstract = {Two equations relating to the effect of finite resolution on the uncertainty of a sample mean give the impression of being contradictory. One suggests that knowing the data to be digitized increases uncertainty while the other might be thought to imply the opposite. This issue is clarified and relevant concepts are discussed. This discussion leads to the development of a new approximation relating to the uncertainty of an estimate obtained by taking the mean of a set of digital measurement results. This result is consistent with others published recently in implying that resolution error can usually be ignored in such a situation. The solution to this problem obtained in a Bayesian analysis with the usual non-informative prior distribution is shown to give an inappropriate result when all the observations are equal and to exhibit anomalous behaviour in certain other circumstances. One of the appendices describes an estimator that has smaller mean-square error than the sample mean when averaging over the set of measurement problems of this type.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {1},
  author = {Willink, R.},
  year = {2007},
  groups = {refworks}
}

@book{willink2007b,
  title = {A Generalization of the {{Welch}}-{{Satterthwaite}} Formula for Use with Correlated Uncertainty Components},
  volume = {44},
  isbn = {0026-1394},
  abstract = {The Welch-Satterthwaite (W-S) formula described in the Guide to the Expression of Uncertainty in Measurement enables an effective number of degrees of freedom to be associated with the standard uncertainty of a measurement estimate. This facilitates the calculation of an expanded uncertainty interval for the value of the measurand. However, the W?S formula is only applicable when the components of measurement error with finite degrees of freedom are uncorrelated. This paper considers the generalization of the formula to accommodate correlated components with finite degrees of freedom. We show that the number of degrees of freedom to be associated with any estimate derived from repeated observation of several quantities is n$>$ 1, where n is the number of sets of observations, and we then give two equations by which the number of effective degrees may be calculated.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {5},
  author = {Willink, R.},
  year = {2007},
  groups = {refworks}
}

@book{willink2007c,
  title = {Uncertainty and Data-Fitting Procedures},
  volume = {44},
  isbn = {0026-1394},
  abstract = {We discuss an apparent paradox in which the uncertainty of an estimate obtained in a data-based minimization problem with a unique solution seems to depend on the method of minimization. The paradox is explained by showing that the sensitivity coefficients appearing in the law of propagation of uncertainty cannot be calculated by explicitly following the iterative process. A simple experimental evaluation of the sensitivity coefficients seems preferable.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {3},
  author = {Willink, R.},
  year = {2007},
  groups = {refworks}
}

@book{willink2007a,
  title = {Uncertainty of Functionals of Calibration Curves},
  volume = {44},
  isbn = {0026-1394},
  abstract = {Typically an estimate \#\#IMG\#\# [http://ej.iop.org/images/0026-1394/44/3/004/metro247551in001.gif] $\backslash$hatf(x) of an unknown response curve f ( x ) is obtained with an associated function \#\#IMG\#\# [http://ej.iop.org/images/0026-1394/44/3/004/metro247551in002.gif] u[$\backslash$hatf(x)] describing the standard uncertainty of the estimate at each value of x . Often the quantity of interest will be a functional of f ( x ), such as a derivative or integral. In such a case the standard uncertainty cannot be calculated without knowledge of the correlation between \#\#IMG\#\# [http://ej.iop.org/images/0026-1394/44/3/004/metro247551in003.gif] $\backslash$smashu[$\backslash$hatf(x\_i)] and \#\#IMG\#\# [http://ej.iop.org/images/0026-1394/44/3/004/metro247551in004.gif] $\backslash$smashu[$\backslash$hatf(x\_j)] for all relevant pairs of points ( x i , x j ). This information might be stored as a two-dimensional function \#\#IMG\#\# [http://ej.iop.org/images/0026-1394/44/3/004/metro247551in005.gif] $\backslash$smashu[$\backslash$hatf(x\_i),$\backslash$smash$\backslash$hatf(x\_j)] in the continuous case or as a matrix ( u ij ) in the discrete case, but this will often be impractical. The difficulty can often be avoided by instead storing the 'random' and 'systematic' components of uncertainty, which is a concept that is familiar but out of favour. This step enables the calculation of standard uncertainty for many functionals of f ( x ) from numerical data and from a graphical representation. Three examples are given illustrating these concepts. The paper also discusses the issue of expressing the uncertainty associated with \#\#IMG\#\# [http://ej.iop.org/images/0026-1394/44/3/004/metro247551in001.gif] $\backslash$hatf(x) as a whole; that is, the simultaneous estimation of \#\#IMG\#\# [http://ej.iop.org/images/0026-1394/44/3/004/metro247551in001.gif] $\backslash$hatf(x) at every value of x .},
  timestamp = {2017-01-18T20:05:48Z},
  number = {3},
  author = {Willink, R.},
  year = {2007},
  groups = {refworks}
}

@book{willink2007,
  title = {Comments on '{{Bayesian}} Estimate of the {{Newtonian}} Constant of Gravitation' with an Alternative Analysis},
  volume = {18},
  isbn = {0957-0233},
  abstract = {This paper provides critical comments on the methodology employed in 'Bayesian estimate of the Newtonian constant of gravitation' (Dose 2007 Meas. Sci. Technol. [http://www.iop.org/EJ/abstract/0957-0233/18/1/022] 18 176?82 ), where many independent estimates of G and their quoted uncertainties are combined to form a single estimate and standard uncertainty. It is argued that the use of the non-Gaussian likelihood functions is unwarranted and that some of the prior distributions used are inappropriate. Another criticism is that the uncertainty of the final estimate is interpreted differently from the uncertainties of the contributing estimates. A non-Bayesian means of analysis is presented for problems of this type. The corresponding estimate is G = 6.674 08(12) {\~A}\textemdash{} 10 ?11 m 3 kg ?1 s ?2 , with the figures in parentheses indicating standard uncertainty.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {7},
  author = {Willink, R.},
  year = {2007},
  groups = {refworks}
}

@book{willink2007e,
  title = {On the {{Lp}} Estimation of a Quantity from a Set of Observations},
  volume = {44},
  isbn = {0026-1394},
  abstract = {This paper considers the estimation of a quantity from a set of repeated measurements x i by the value of t minimizing ? i | x i ? t | p , as in Pennecchi and Callegaro ( 2006 Metrologia [/0026-1394/43/3/004] 43 213?19 ). The estimate with p = 2 is the familiar sample mean. Assumptions that are relevant when preferring an estimate with p $<$ 2 are identified and examined. Theoretical considerations and results of simulations weaken the case for choosing an estimate with p $<$ 2. The simulation results support the regular use of the sample mean or an estimate that involves the value p = 1.5 when the kurtosis of the dataset is sufficiently large.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {2},
  author = {Willink, R.},
  year = {2007},
  groups = {refworks}
}

@book{willink2006d,
  title = {On Using the {{Monte Carlo}} Method to Calculate Uncertainty Intervals},
  volume = {43},
  isbn = {0026-1394},
  abstract = {This communication reviews one method of calculating uncertainty intervals by Monte Carlo simulation, suggests that this method produces intervals that are slightly wider than is necessary and shows that a simpler, more intuitive, method generates narrower intervals. The analysis raises questions about the concept of describing measurands by probability distributions.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {6},
  author = {Willink, R.},
  year = {2006},
  groups = {refworks}
}

@book{willink2006c,
  title = {Forming a Comparison Reference Value from Different Distributions of Belief},
  volume = {43},
  isbn = {0026-1394},
  abstract = {If measurement uncertainty is to be expressed by forming a distribution of belief for the measurand then this should be reflected in the theory underlying the analysis of interlaboratory comparison data. This paper presents a corresponding method for the calculation of a reference value in a comparison where each laboratory independently assigns a probability density function, f i ( x ), to the value X of a stable artefact. Straightforward argument shows that a consensus density function for X can be taken as \#\#IMG\#\# [http://ej.iop.org/images/0026-1394/43/1/002/metro207092in001.gif] f(x)=$\backslash$prod\_i=1\^n f\_i(x)/{\i}nt $\backslash$prod\_i=1\^n f\_i(x)$\backslash$,$\backslash$rmd x , assuming that the densities of the n laboratories are reliable and mutually consistent. A method is also presented for examining this consistency. The result f ( x ) is a special case of a consensus in the statistical literature called the logarithmic opinion pool. The key comparison reference value might be taken as the mean, median or mode of f ( x ). The method developed does not explicitly involve laboratory biases (offsets), which are important features of most methods for comparison analysis published recently and which are relevant to the calculation of degrees of equivalence. The belief approach does not seem well-suited when it is assumed that such offsets exist.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {1},
  author = {Willink, R.},
  year = {2006},
  groups = {refworks}
}

@book{willink2006,
  title = {Principles of Probability and Statistics for Metrology},
  volume = {43},
  isbn = {0026-1394},
  abstract = {The sciences of probability and statistics involve the practitioner in a kind of thinking that promotes good decision-making. This article describes principles of probability and statistics that underlie such thinking in metrology, particularly in the analysis of uncertainties and interlaboratory comparisons. These principles are applicable whatever reasonable understanding of probability is held.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {4},
  author = {Willink, R.},
  year = {2006},
  groups = {refworks}
}

@book{willink2006b,
  title = {Meaning and Models in Key Comparisons, with Measures of Operability and Interoperability},
  volume = {43},
  isbn = {0026-1394},
  abstract = {This paper reviews the purposes of CIPM key comparisons and presents a model-based analysis that is in keeping with these purposes. Measures of 'operability' and 'interoperability' are derived to answer the reasonable questions 'how accurate will the result be when the laboratory in question makes some measurement in the future?' and 'how similar would the results be if two laboratories were to make a common measurement in the future?' These are explicit questions that relate to the concepts of measurement ability and the equivalence of standards. The basic concepts emphasized are that (i) a measurement result cannot legitimately contribute to its own point of reference and that (ii) the effects causing error, or uncertainty, that are relevant in forming the measures of operability and interoperability are those that vary after the key comparison. Therefore, points of references for each measurement result are formed by excluding the result in question, and the measures of operability and interoperability are defined using the uncertainty components with effects varying on the short time-scale. These measures have clear and communicable meanings.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {4},
  author = {Willink, R.},
  year = {2006},
  groups = {refworks}
}

@book{willink2006a,
  title = {Uncertainty Analysis by Moments for Asymmetric Variables},
  volume = {43},
  isbn = {0026-1394},
  abstract = {This paper gives equations that enable a method of uncertainty analysis recently described (Willink R 2005 Metrologia 42 329?43) to accommodate influence variables with asymmetric probability distributions. The equations permit the calculation of 90\%, 95\%, 98\% and 99\% coverage intervals [ a , b ] for the measurand Y = F ( X 1 ,..., X m ) from the first four moments of an approximating distribution. The equations for 90\% and 98\% are presented so that the one-sided probability statements Pr( Y ? a ) = P and Pr( Y ? b ) = P can be supplied for the familiar probability levels of P = 95\% and P = 99\%. Additional equations are provided so that the exact moments can be calculated in certain non-linear problems. An improved method is given for dealing with inputs that have shifted and scaled t -distributions with 4 or fewer degrees of freedom, for which the first four moments do not all exist.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {6},
  author = {Willink, R.},
  year = {2006},
  groups = {refworks}
}

@article{willink2005a,
  title = {A Procedure for the Evaluation of Measurement Uncertainty Based on Moments},
  volume = {42},
  abstract = {A practical, transparent and accurate procedure is proposed for the evaluation of measurement uncertainty. The procedure is based on established statistical theory, allows the treatment of correlated input quantities estimated from a finite number of observations and permits the assignment of asymmetric distributions to input quantities. The procedure is applicable wherever the basic method of clause 8 of the Guide to the Expression of Uncertainty in Measurement (the Guide ) is applicable. As in the method of the Guide , distributions are associated with input quantities for use with a linearized formulation of the measurement function. These distributions are combined using a property of their statistical cumulants, which are simple functions of their moments. The distribution ultimately obtained for the measurand is a member of the Pearson family, which has the set of scaled t -distributions as a subset. So the procedure allows greater flexibility than the method of the Guide in the representation of the distribution associated with the measurand. The procedure has the qualities of internal consistency and transferability as outlined in clause 0.4 of the Guide and is consistent with principles of the 'propagation of distributions' that underlie a proposed supplement to the Guide . In many cases it will permit the quotation of uncertainty intervals that are narrower than those of the Guide yet with the same justifiable probability of containing the measurand, so providing a more informative statement to the user of the interval. The procedure is presented for the probabilities of 0.95 and 0.99 but can be extended to accommodate other probabilities.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {5},
  journal = {Metrologia},
  author = {Willink, R.},
  year = {2005},
  pages = {329},
  groups = {refworks}
}

@article{willink2004a,
  title = {Coverage Intervals and Statistical Coverage Intervals},
  volume = {41},
  abstract = {There is potential for confusion between the term 'coverage interval', which is developing a common usage in metrology, and the term 'statistical coverage interval', as defined in the Guide to the Expression of Uncertainty in Measurement . The meaning of a 'statistical coverage interval' is explained further, and possible courses of action are suggested.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {3},
  url = {http://stacks.iop.org/0026-1394/41/i=3/a=L01},
  journal = {Metrologia},
  author = {Willink, R.},
  year = {2004},
  pages = {5},
  groups = {refworks}
}

@inproceedings{willink2004,
  address = {New Jersey},
  title = {Fast Computational Alternative to {{Monte Carlo}} Simulation of an Output Distribution},
  timestamp = {2017-01-18T20:05:48Z},
  booktitle = {Advanced {{Mathematical}} \& {{Computational Tools}} in {{Metrology VI}}},
  publisher = {{World Scientific}},
  author = {Willink, R. and Hall, B. D.},
  editor = {Ciarlini, P.},
  year = {2004},
  groups = {refworks}
}

@article{willink2005,
  title = {A United Interpretation of Different Uncertainty Intervals},
  volume = {38},
  timestamp = {2017-01-18T20:05:48Z},
  number = {1},
  journal = {Measurement},
  author = {Willink, R. and Lira, I.},
  month = jul,
  year = {2005},
  keywords = {Bayesian statistics,Confidence interval,Credible interval,probability,Uncertainty},
  pages = {61--66},
  groups = {refworks}
}

@article{wubbeler2010,
  title = {A Two-Stage Procedure for Determining the Number of Trials in the Application of a {{Monte Carlo}} Method for Uncertainty Evaluation},
  volume = {47},
  abstract = {Adaptive Monte Carlo schemes can be used to determine the number of Monte Carlo trials (the number of evaluations of the measurement model) necessary for the evaluation of uncertainty according to Supplement 1 to the GUM (GUM S1). The goal is to reach a prescribed numerical accuracy of the Monte Carlo results (the estimate, associated standard uncertainty and coverage interval endpoints) for a chosen confidence level. It is shown that simple sequential adaptive Monte Carlo schemes may not perform well in this regard and an alternative method based on a two-stage procedure due to Stein is proposed. The implementation of this two-stage scheme for GUM S1 is described, and its performance and robustness are demonstrated in terms of simulation results.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {3},
  journal = {Metrologia},
  author = {W{\"u}bbeler, Gerd and Harris, Peter M. and Cox, Maurice G. and Elster, Clemens},
  year = {2010},
  pages = {317},
  groups = {refworks}
}

@article{wubbeler2008,
  title = {Evaluation of Measurement Uncertainty and Its Numerical Calculation by a {{Monte Carlo}} Method},
  volume = {19},
  abstract = {The Guide to the Expression of Uncertainty in Measurement (GUM) is the de facto standard for the evaluation of measurement uncertainty in metrology. Recently, evaluation of measurement uncertainty has been proposed on the basis of probability density functions (PDFs) using a Monte Carlo method. The relation between this PDF approach and the standard method described in the GUM is outlined. The Monte Carlo method required for the numerical calculation of the PDF approach is described and illustrated by its application to two examples. The results obtained by the Monte Carlo method for the two examples are compared to the corresponding results when applying the GUM.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {8},
  journal = {Measurement Science and Technology},
  author = {W{\"u}bbeler, Gerd and Krystek, Michael and Elster, Clemens},
  year = {2008},
  pages = {084009 (4pp)},
  groups = {refworks}
}

@article{xia2000,
  title = {Estimation of Non-Statistical Uncertainty Using Fuzzy-Set Theory},
  volume = {11},
  abstract = {A novel method using a fuzzy practicable interval to characterize non-statistical uncertainty in dynamic measurement is proposed. The method permits the uncertainty being estimated under the conditions that the number of measurements is very small and the probability distribution unknown. The feasibility of the method is validated by computer-simulation experiments.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {4},
  journal = {Measurement Science and Technology},
  author = {Xia, Xintao and Wang, Zhongyu and Gao, Yongsheng},
  year = {2000},
  pages = {430},
  groups = {refworks}
}

@article{zacks1984,
  title = {Estimating the {{Shift}} to {{Wear}}-{{Out}} of {{Systems Having Exponential}}-{{Weibull Life Distributions}}},
  volume = {32},
  abstract = {We develop a new family of life distributions, called the exponential-Weibull wear-out distributions, for systems whose failure rate function is a constant up to a change-point (wear-out point) and strictly increasing afterward. We derive properties of these wear-out distributions and develop a Bayes adaptive procedure for estimating the change point. Recursive formulas are given for determining the posterior probability that the change has occurred and its Bayes estimator. Results of numerical simulations are given to illustrate the properties of the adaptive procedure.},
  timestamp = {2017-01-18T20:05:48Z},
  number = {3},
  journal = {Operations research},
  author = {Zacks, S.},
  year = {1984},
  pages = {741--749},
  groups = {refworks}
}

@article{zadeh1995,
  title = {Discussion: {{Probability Theory}} and {{Fuzzy Logic Are Complementary Rather Than Competitive}}},
  volume = {37},
  abstract = {The relationship between probability theory and fuzzy logic has long been an object of discussion and some controversy. The position articulated in this article is that probability theory by itself is not sufficient for dealing with uncertainty and imprecision in real-world settings. To enhance its effectiveness, probability theory needs an infusion of concepts and techniques drawn from fuzzy logic-especially the concept of a linguistic variable and the calculus of fuzzy if-then rules. In the final analysis, probability theory and fuzzy logic are complementary rather than competitive.},
  language = {English},
  timestamp = {2017-01-18T20:05:48Z},
  number = {3},
  journal = {Technometrics},
  author = {Zadeh, Lotfi A.},
  month = aug,
  year = {1995},
  pages = {pp. 271--276},
  groups = {refworks}
}

@article{zadeh1986,
  title = {A {{Simple View}} of the {{Dempster}}-{{Shafer Theory}} of {{Evidence}} and Its {{Implication}} for the {{Rule}} of {{Combination}}},
  volume = {7},
  timestamp = {2017-01-18T20:05:48Z},
  journal = {AI Magazine},
  author = {Zadeh, Lotfi A.},
  year = {1986},
  pages = {85--90},
  groups = {refworks}
}

@article{zadeh1978,
  title = {Fuzzy Sets as a Basis for a Theory of Possibility},
  volume = {1},
  timestamp = {2017-01-18T20:05:48Z},
  number = {1},
  journal = {Fuzzy Sets and Systems},
  author = {Zadeh, Lotfi A.},
  year = {1978},
  keywords = {fuzzy logic,fuzzy set theory,Fuzzy subset theory,fuzzy theory},
  pages = {3},
  groups = {refworks}
}

@article{zadeh1965,
  title = {Fuzzy Sets},
  volume = {8},
  abstract = {A fuzzy set is a class of objects with a continuum of grades of membership. Such a set is characterized by a membership (characteristic) function which assigns to each object a grade of membership ranging between zero and one. The notions of inclusion, union, intersection, complement, relation, convexity, etc., are extended to such sets, and various properties of these notions in the context of fuzzy sets are established. In particular, a separation theorem for convex fuzzy sets is proved without requiring that the fuzzy sets be disjoint."},
  timestamp = {2017-01-18T20:05:48Z},
  number = {3},
  journal = {Information and Control},
  author = {Zadeh, L. A.},
  year = {1965},
  pages = {338},
  groups = {refworks}
}

@techreport{jalkio1999,
  type = {SME Technical Paper},
  title = {The {{Use}} of {{Optical Probes}} on {{Coordinate Measuring Machines}} \textendash{} {{Strengths}} and {{Weaknesses}}},
  timestamp = {2017-01-18T20:09:25Z},
  number = {IQ99-206},
  institution = {Society of Manufacturing Engineers},
  author = {Jalkio, J. A.},
  year = {1999}
}

@article{leonard2016,
  title = {Comment on `{{On}} the Units Radian and Cycle for the Quantity Plane Angle'},
  volume = {53},
  issn = {0026-1394},
  doi = {10.1088/0026-1394/53/6/1281},
  abstract = {In the referenced paper, Ian Mills addresses the confusion caused by the treatment of plane angle in the International System of Units (SI). As he points out, what the SI calls an `angle' is not a dimensional physical quantity but, rather, the dimensionless numerical value of the angle when expressed in radians, thus creating widespread confusion regarding terminology and notation. For example, Mills shows that for the harmonic oscillator, if the conventional argument of the sinusoid represents an angle, it must be divided by a dimensional constant equal to one radian in order to correctly render it dimensionless, thereby greatly clarifying the notation. However, there is a problem with the author's interpretation of frequency. Although, for uniform rotation, Mills correctly defines the revolution frequency as the number of complete revolutions, N , divided by the time interval, he takes the unit for N to be `cycle' (which he defines as one revolution) rather than the correct unit: the number one. The unit for `frequency' then appears to be `cycle per second' (i.e. revolution per second), whereas it should be one per second, correctly called hertz. Thus Mills concludes that `frequency' is the same physical quantity as angular velocity and calls for the `hertz' to be redefined as 2 $\pi$ rad s -1 , a non-coherent derived unit for angular velocity. This misinterpretation of frequency corrupts the remainder of the author's discussion of the examples considered. In my comment, I explain and correct these and related errors.},
  language = {en},
  timestamp = {2017-01-18T20:29:20Z},
  number = {6},
  urldate = {2017-01-18},
  url = {http://stacks.iop.org/0026-1394/53/i=6/a=1281},
  journal = {Metrologia},
  author = {Leonard, B. P.},
  year = {2016},
  pages = {1281},
  file = {IOP Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\CQAX7PRT\\Leonard - 2016 - Comment on On the units radian and cycle for the .pdf:application/pdf}
}

@article{mills2016,
  title = {Reply to {{Comment}} on `{{On}} the Units Radian and Cycle for the Quantity Plane Angle'},
  volume = {53},
  issn = {0026-1394},
  doi = {10.1088/0026-1394/53/6/1286},
  abstract = {This paper is a reply to the preceding paper (Leonard 2016 Metrologia 53 1281\textendash{}5) which was a comment on my paper (Mills 2016 Metrologia 53 991\textendash{}7) `On the units radian and cycle for the quantity plane angle'.},
  language = {en},
  timestamp = {2017-01-18T20:30:16Z},
  number = {6},
  urldate = {2017-01-18},
  url = {http://stacks.iop.org/0026-1394/53/i=6/a=1286},
  journal = {Metrologia},
  author = {Mills, I. M.},
  year = {2016},
  pages = {1286},
  file = {IOP Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\XQ53A4NK\\Mills - 2016 - Reply to Comment on On the units radian and cycle.pdf:application/pdf}
}

@misc{zotero-null-248,
  title = {Retorquere/Zotero-Better-Bibtex},
  abstract = {zotero-better-bibtex - Make Zotero effective for us LaTeX holdouts},
  timestamp = {2017-02-07T22:13:52Z},
  urldate = {2017-02-07},
  url = {https://github.com/retorquere/zotero-better-bibtex},
  journal = {GitHub},
  file = {Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\NWIQ7PEN\\zotero-better-bibtex.html:text/html},
  groups = {pandoc}
}

@book{lehman2006,
  title = {The Biblatex Package},
  timestamp = {2017-02-07T22:19:11Z},
  urldate = {2017-02-07},
  url = {http://mirror.neu6.edu.cn/CTAN/macros/latex/contrib/biblatex/doc/biblatex.pdf},
  author = {Lehman, Philipp and Kime, Philip},
  year = {2006},
  file = {biblatex.pdf:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\S99E5JWZ\\biblatex.pdf:application/pdf},
  groups = {pandoc}
}

@article{patashnik1988,
  title = {{{BibTeXing}}},
  timestamp = {2017-02-07T22:20:32Z},
  author = {Patashnik, Oren},
  year = {1988},
  file = {btxdoc.pdf:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\UZANPZ88\\btxdoc.pdf:application/pdf},
  groups = {pandoc}
}

@article{markey2005,
  title = {Tame the Beast},
  volume = {1},
  timestamp = {2017-02-07T22:20:41Z},
  urldate = {2017-02-07},
  url = {ftp://ftp.belnet.be/mirrors/pub/ftp.tex.ac.uk/ctan/tex-archive/documentation/bibtex/tamethebeast/ttb_en.pdf},
  journal = {The B to X of BibTeX. Version},
  author = {Markey, Nicolas},
  year = {2005},
  file = {ttb_en.pdf:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\9UPGZVXF\\ttb_en.pdf:application/pdf},
  groups = {pandoc}
}

@article{luthi2008,
  title = {High-Resolution Carbon Dioxide Concentration Record 650,000\textendash{}800,000\,Years before Present},
  volume = {453},
  copyright = {\textcopyright{} 2008 Nature Publishing Group},
  issn = {0028-0836},
  doi = {10.1038/nature06949},
  abstract = {Changes in past atmospheric carbon dioxide concentrations can be determined by measuring the composition of air trapped in ice cores from Antarctica. So far, the Antarctic Vostok and EPICA Dome C ice cores have provided a composite record of atmospheric carbon dioxide levels over the past 650,000 years. Here we present results of the lowest 200 m of the Dome C ice core, extending the record of atmospheric carbon dioxide concentration by two complete glacial cycles to 800,000 yr before present. From previously published data and the present work, we find that atmospheric carbon dioxide is strongly correlated with Antarctic temperature throughout eight glacial cycles but with significantly lower concentrations between 650,000 and 750,000 yr before present. Carbon dioxide levels are below 180 parts per million by volume (p.p.m.v.) for a period of 3,000 yr during Marine Isotope Stage 16, possibly reflecting more pronounced oceanic carbon storage. We report the lowest carbon dioxide concentration measured in an ice core, which extends the pre-industrial range of carbon dioxide concentrations during the late Quaternary by about 10 p.p.m.v. to 172\textendash{}300 p.p.m.v.},
  language = {en},
  timestamp = {2017-02-15T22:02:40Z},
  number = {7193},
  urldate = {2017-02-15},
  url = {http://www.nature.com/nature/journal/v453/n7193/full/nature06949.html\#f1},
  journal = {Nature},
  author = {L{\"u}thi, Dieter and Le Floch, Martine and Bereiter, Bernhard and Blunier, Thomas and Barnola, Jean-Marc and Siegenthaler, Urs and Raynaud, Dominique and Jouzel, Jean and Fischer, Hubertus and Kawamura, Kenji and Stocker, Thomas F.},
  month = may,
  year = {2008},
  pages = {379--382},
  file = {Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\6X94ACVE\\Lthi et al. - 2008 - High-resolution carbon dioxide concentration recor.pdf:application/pdf;Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\82CKKK4S\\nature06949.html:text/html},
  groups = {climate}
}

@article{moran1971,
  title = {Estimating Structural and Functional Relationships},
  volume = {1},
  issn = {0047-259X},
  doi = {10.1016/0047-259X(71)90013-3},
  abstract = {This paper surveys the problem of estimating a linear relationship between variables which are observed with error. These are either fixed variables (functional relationship) or random variables (structural relationship). After considering various conditions for identifiability, estimation methods are surveyed in various cases when additional information is available, including Wald's method, the use of instrumental variates, and the case of more than two variables. The paper concludes with a list of unsolved problems.},
  timestamp = {2017-02-16T16:49:09Z},
  number = {2},
  urldate = {2017-02-16},
  url = {http://www.sciencedirect.com/science/article/pii/0047259X71900133},
  journal = {Journal of Multivariate Analysis},
  author = {Moran, P. A. P.},
  month = jun,
  year = {1971},
  keywords = {Errors-in-variable model,functional relationships,identification problem,incidental parameters,instrumental variates,multivariate analysis,structural relationships,Wald's method},
  pages = {232--255},
  file = {ScienceDirect Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\H6M66RJW\\0047259X71900133.html:text/html},
  groups = {curve fitting}
}

@misc{zotero-null-264,
  title = {Ontology of {{Units}} of {{Measure}} and {{Related Concepts}} | Www.Semantic-Web-Journal.Net},
  timestamp = {2017-02-20T22:30:31Z},
  urldate = {2017-02-20},
  url = {http://www.semantic-web-journal.net/content/ontology-units-measure-and-related-concepts},
  file = {Ontology of Units of Measure and Related Concepts | www.semantic-web-journal.net:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\UWHASDN6\\ontology-units-measure-and-related-concepts.html:text/html},
  groups = {angles}
}

@article{Banks2001,
  title = {Modelling and Estimating Uncertainty in Parameter Estimation},
  volume = {17},
  abstract = {In this paper we discuss questions related to reliability or variability of estimated parameters in deterministic least-squares problems. By viewing the parameters for the inverse problem as realizations for a random variable we are able to use standard results from probability theory to formulate a tractable probabilistic framework to treat this uncertainty. We discuss method stability and approximate problems and are able to show convergence of solutions of the approximate problems to those of the original problem. The efficacy of our approach is demonstrated in numerical examples involving estimation of constant parameters in differential equations.},
  timestamp = {2017-02-21T17:10:53Z},
  number = {1},
  url = {http://stacks.iop.org/0266-5611/17/i=1/a=308},
  journal = {Inverse Problems},
  author = {Banks, H. T. and Bihari, Kathleen L.},
  year = {2001},
  pages = {95},
  groups = {curve fitting,curve fitting 2/21/2017, 11:10:52 AM},
  __markedentry = {[jajalkio:6]},
  owner = {jajalkio}
}

@article{1981,
  title = {Evaluating {{Structural Equation Models}} with {{Unobservable Variables}} and {{Measurement Error}}},
  volume = {18},
  copyright = {Copyright \textcopyright{} 1981 American Marketing Association},
  issn = {00222437},
  abstract = {The statistical tests used in the analysis of structural equation models with unobservable variables and measurement error are examined. A drawback of the commonly applied chi square test, in addition to the known problems related to sample size and power, is that it may indicate an increasing correspondence between the hypothesized model and the observed data as both the measurement properties and the relationship between constructs decline. Further, and contrary to common assertion, the risk of making a Type II error can be substantial even when the sample size is large. Moreover, the present testing methods are unable to assess a model's explanatory power. To overcome these problems, the authors develop and apply a testing system based on measures of shared variance within the structural model, measurement model, and overall model.},
  timestamp = {2017-02-21T17:10:54Z},
  number = {1},
  url = {http://www.jstor.org/stable/3151312},
  journal = {Journal of Marketing Research},
  author = {{Fornell, Claes} and {Larcker, David F.}},
  year = {1981},
  note = {Contributor:},
  pages = {39--50},
  groups = {curve fitting,curve fitting 2/21/2017, 11:10:52 AM},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Feb., 1981}
}

@article{Jefferys1980,
  title = {On the {{Method}} of {{Least}}-{{Squares}}},
  volume = {85},
  timestamp = {2017-02-21T17:13:42Z},
  number = {2},
  url = {http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1980AJ.....85..177J\&amp;data_type=PDF_HIGH\&amp;whole_paper=YES\&amp;type=PRINTER\&amp;filetype=.pdf},
  journal = {Astronomical Journal},
  author = {Jefferys, W. H.},
  year = {1980},
  pages = {177},
  groups = {curve fitting,curve fitting 2/21/2017, 11:10:52 AM},
  owner = {jajalkio}
}

@article{1988CeMec..41...39J,
  title = {{{GaussFit}} - a {{System}} for {{Least Squares}} and {{Robust Estimation}}},
  volume = {41},
  timestamp = {2017-02-21T17:10:54Z},
  journal = {Celestial Mechanics},
  author = {Jefferys, W.~H. and Fitzpatrick, M.~J. and McArthur, B.~E.},
  year = {1988},
  pages = {39},
  groups = {curve fitting,curve fitting 2/21/2017, 11:10:52 AM},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl = {http://adsabs.harvard.edu/abs/1988CeMec..41...39J}
}

@article{Malzahn2005,
  title = {A Statistical Physics Approach for the Analysis of Machine Learning Algorithms on Real Data},
  volume = {2005},
  abstract = {We combine the replica approach of statistical physics with a variational technique to make it applicable for the analysis of machine learning algorithms on real data. The method is applied to Gaussian process models and their relative, the support vector machine. We discuss the quality of our theoretical results in comparison to experiments. As a key result, we apply our theory on real world benchmark data and show its potential for practical applications by deriving approximate expressions for data averaged performance measures which hold for general data distributions and allow us to optimize the performance of the learning algorithm.},
  timestamp = {2017-02-21T17:10:54Z},
  number = {11},
  url = {http://stacks.iop.org/1742-5468/2005/i=11/a=P11001},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  author = {Malzahn, D{\~A}\textparagraph{}rthe and Opper, Manfred},
  year = {2005},
  pages = {P11001},
  groups = {curve fitting,curve fitting 2/21/2017, 11:10:52 AM},
  __markedentry = {[jajalkio:]},
  owner = {jajalkio}
}

@article{Reed2010,
  title = {A Spreadsheet for Linear Least-Squares Fitting with Errors in Both Coordinates},
  volume = {45},
  abstract = {A freely available, easy-to-use Excel spreadsheet for performing linear least-squares fits for ( x , y ) data with errors in both coordinates is described.},
  timestamp = {2017-02-21T17:10:54Z},
  number = {1},
  url = {http://stacks.iop.org/0031-9120/45/i=1/a=011},
  journal = {Physics Education},
  author = {Reed, B. Cameron},
  year = {2010},
  pages = {93},
  groups = {curve fitting,curve fitting 2/21/2017, 11:10:52 AM},
  owner = {jajalkio}
}

@inproceedings{Shlens05atutorial,
  title = {A Tutorial on {{Principal Component Analysis}}},
  timestamp = {2017-02-21T17:10:54Z},
  booktitle = {Systems {{Neurobiology Laboratory}}, {{Salk Institute}} for {{Biological Studies}}},
  author = {Shlens, Jonathon},
  year = {2005},
  groups = {curve fitting,curve fitting 2/21/2017, 11:10:52 AM}
}

@article{RefWorks:364,
  title = {Fitting Data to Nonlinear Functions with Uncertainties in All Measurement Variables},
  volume = {19},
  abstract = {An analytical approach is made to the nonlinear least squares problem having uncertainties in all measurements coordinates. It is shown how the unknown independent coordinates may be eliminated from the sum of the squares, thus reducing it to an ordinary minimisation problem. An algorithm is derived using analytical derivatives, but a simple procedure also allows the use of standard numerical derivatives.},
  timestamp = {2017-02-21T17:10:54Z},
  number = {1},
  journal = {The Computer Journal},
  author = {Southwell, W. H.},
  month = jan,
  year = {1976},
  pages = {69--73},
  groups = {curve fitting,curve fitting 2/21/2017, 11:10:52 AM}
}

@article{RefWorks:363,
  title = {Least-Squares Analysis of Data with Uncertainty in x and y: {{A Monte Carlo}} Methods Comparison},
  volume = {103},
  abstract = {The least-squares analysis of data with error in x and y is generally thought to yield best results when the quantity minimized is the sum of the properly weighted squared residuals in x and in y. As an alternative to this {\^a}\texteuro{\oe}total variance{\^a}\texteuro\dbend{} (TV) method, {\^a}\texteuro{\oe}effective variance{\^a}\texteuro\dbend{} (EV) methods convert the uncertainty in x into an effective contribution to that in y, and though easier to use are considered to be less reliable. There are at least two EV methods, differing in how the weights are treated in the optimization. One of these is identical to the TV method for fits to a straight line. The formal differences among these methods are clarified, and Monte Carlo simulations are used to examine the statistical properties of each on the widely used straight-line model of York, a quadratic variation on this, Orear's hyperbolic model, a nonlinear binding (Langmuir) model, and Wentworth's kinetics model. The simulations confirm that the EV and TV methods are statistically equivalent in the limit of small data error, where they yield unbiased, normally distributed parameter estimates, with standard errors correctly predicted by the a priori covariance matrix. With increasing data error, these properties fail to hold; and the TV method is not always statistically best. Nonetheless, the method differences should seldom be of practical significance, since they are likely to be small compared with uncertainties from incomplete information about the data error in x and y.},
  timestamp = {2017-02-21T17:10:54Z},
  number = {2},
  journal = {Chemometrics and Intelligent Laboratory Systems},
  author = {Tellinghuisen, Joel},
  year = {10/15 2010},
  keywords = {data analysis,Effective variance,Least-squares,Monte Carlo,Total variance,Uncertainty in x and y},
  pages = {160--169},
  groups = {curve fitting,curve fitting 2/21/2017, 11:10:52 AM}
}

@article{RefWorks:362,
  title = {Using {{R2}} to Compare Least-Squares Fit Models: {{When}} It Must Fail},
  volume = {105},
  abstract = {R2 can be used correctly to select from among competing least-squares fit models when the data are fitted in common form and with common weighting. However, when models are compared by fitting data that have been mathematically transformed in different ways, R2 is a flawed statistic, even when the data are properly weighted in accord with the transformations. The reason is that in its most commonly used form, R2 can be expressed in terms of the excess variance (s2) and the total variance in y (sy2) {\^a}\texteuro'' the first of which is either invariant or approximately so with proper weighting, but the second of which can vary substantially in data transformations. When given data are analyzed {\^a}\texteuro{\oe}as is{\^a}\texteuro\dbend{} with different models and fixed weights, sy2 remains constant and R2 is a valid statistic. However, then s2, and {\"I}\textdaggerdbl{}2 in weighted fitting, are arguably better metrics for such comparisons.},
  timestamp = {2017-02-21T17:10:54Z},
  number = {2},
  journal = {Chemometrics and Intelligent Laboratory Systems},
  author = {Tellinghuisen, Joel and Bolster, Carl H.},
  year = {2/15 2011},
  keywords = {Data transformation,Langmuir,MichaelisMenten,Model comparison,R,Weighted least squares},
  pages = {220--222},
  groups = {curve fitting,curve fitting 2/21/2017, 11:10:52 AM}
}

@article{myhre1998,
  title = {New Estimates of Radiative Forcing Due to Well Mixed Greenhouse Gases},
  volume = {25},
  issn = {1944-8007},
  doi = {10.1029/98GL01908},
  abstract = {We have performed new calculations of the radiative forcing due to changes in the concentrations of the most important well mixed greenhouse gases (WMGG) since pre-industrial time. Three radiative transfer models are used. The radiative forcing due to CO2, including shortwave absorption, is 15\% lower than the previous IPCC estimate. The radiative forcing due to all the WMGG is calculated to 2.25 Wm-2, which we estimate to be accurate to within about 5\%. The importance of the CFCs is increased by about 20\% relative to the total effect of all WMGG compared to previous estimates. We present updates to simple forcing-concentration relationships previously used by IPCC.},
  language = {en},
  timestamp = {2017-02-23T21:17:42Z},
  number = {14},
  urldate = {2017-02-23},
  url = {http://onlinelibrary.wiley.com/doi/10.1029/98GL01908/abstract},
  journal = {Geophysical Research Letters},
  author = {Myhre, Gunnar and Highwood, Eleanor J. and Shine, Keith P. and Stordal, Frode},
  month = jul,
  year = {1998},
  keywords = {0399 Atmospheric Composition and Structure: General or miscellaneous,1600 Global Change,3399 Meteorology and Atmospheric Dynamics: General or miscellaneous},
  pages = {2715--2718},
  file = {Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\WV99JSXM\\Myhre et al. - 1998 - New estimates of radiative forcing due to well mix.pdf:application/pdf;Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\5M4MRTPF\\abstract.html:text/html},
  groups = {climate}
}

@article{ballesteros2017,
  title = {Unifying {{Inflation}} with the {{Axion}}, {{Dark Matter}}, {{Baryogenesis}}, and the {{Seesaw Mechanism}}},
  volume = {118},
  doi = {10.1103/PhysRevLett.118.071802},
  abstract = {A minimal extension of the standard model (SM) with a single new mass scale and providing a complete and consistent picture of particle physics and cosmology up to the Planck scale is presented. We add to the SM three right-handed SM-singlet neutrinos, a new vectorlike color triplet fermion, and a complex SM-singlet scalar $\sigma$ that stabilizes the Higgs potential and whose vacuum expectation value at $\sim$1011 GeV breaks lepton number and a Peccei-Quinn symmetry simultaneously. Primordial inflation is produced by a combination of $\sigma$ (nonminimally coupled to the scalar curvature) and the SM Higgs boson. Baryogenesis proceeds via thermal leptogenesis. At low energies, the model reduces to the SM, augmented by seesaw-generated neutrino masses, plus the axion, which solves the strong CP problem and accounts for the dark matter in the Universe. The model predicts a minimum value of the tensor-to-scalar ratio r$\simeq$0.004, running of the scalar spectral index $\alpha\simeq$-7\texttimes{}10-4, the axion mass mA$\sim$100 $\mu$eV, and cosmic axion background radiation corresponding to an increase of the effective number of relativistic neutrinos of $\sim$0.03. It can be probed decisively by the next generation of cosmic microwave background and axion dark matter experiments.},
  timestamp = {2017-02-24T21:46:42Z},
  number = {7},
  urldate = {2017-02-24},
  url = {http://link.aps.org/doi/10.1103/PhysRevLett.118.071802},
  journal = {Physical Review Letters},
  author = {Ballesteros, Guillermo and Redondo, Javier and Ringwald, Andreas and Tamarit, Carlos},
  month = feb,
  year = {2017},
  pages = {071802},
  file = {Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\QVSV54C5\\Ballesteros et al. - 2017 - Unifying Inflation with the Axion, Dark Matter, Ba.pdf:application/pdf;APS Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\XNJGH9NK\\PhysRevLett.118.html:text/html},
  groups = {quantum}
}

@misc{zotero-null-283,
  title = {Songrotek/{{Deep}}-{{Learning}}-{{Papers}}-{{Reading}}-{{Roadmap}}},
  abstract = {Deep-Learning-Papers-Reading-Roadmap - Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!},
  timestamp = {2017-03-03T21:22:29Z},
  urldate = {2017-03-03},
  url = {https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap},
  journal = {GitHub},
  file = {Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\V2UFKRWV\\Deep-Learning-Papers-Reading-Roadmap.html:text/html},
  groups = {deep learning}
}

@article{tononi2003,
  title = {Measuring Information Integration},
  volume = {4},
  issn = {1471-2202},
  doi = {10.1186/1471-2202-4-31},
  abstract = {To understand the functioning of distributed networks such as the brain, it is important to characterize their ability to integrate information. The paper considers a measure based on effective information, a quantity capturing all causal interactions that can occur between two parts of a system.},
  timestamp = {2017-07-18T20:22:40Z},
  url = {http://dx.doi.org/10.1186/1471-2202-4-31},
  journal = {BMC Neuroscience},
  author = {Tononi, Giulio and Sporns, Olaf},
  month = dec,
  year = {2003},
  pages = {31},
  file = {Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\MCC48ZPQ\\Tononi and Sporns - 2003 - Measuring information integration.pdf:application/pdf;Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\7HH9CACJ\\1471-2202-4-31.html:text/html},
  groups = {deep learning}
}

@article{granger1969,
  title = {Investigating {{Causal Relations}} by {{Econometric Models}} and {{Cross}}-Spectral {{Methods}}},
  volume = {37},
  issn = {0012-9682},
  doi = {10.2307/1912791},
  abstract = {There occurs on some occasions a difficulty in deciding the direction of causality between two related variables and also whether or not feedback is occurring. Testable definitions of causality and feedback are proposed and illustrated by use of simple two-variable models. The important problem of apparent instantaneous causality is discussed and it is suggested that the problem often arises due to slowness in recording information or because a sufficiently wide class of possible causal variables has not been used. It can be shown that the cross spectrum between two variables can be decomposed into two parts, each relating to a single causal arm of a feedback situation. Measures of causal lag and causal strength can then be constructed. A generalisation of this result with the partial cross spectrum is suggested.},
  timestamp = {2017-07-18T20:24:22Z},
  number = {3},
  url = {http://www.jstor.org/stable/1912791},
  journal = {Econometrica},
  author = {Granger, C. W. J.},
  year = {1969},
  pages = {424--438},
  groups = {deep learning}
}

@article{schreiber2000,
  title = {Measuring {{Information Transfer}}},
  volume = {85},
  doi = {10.1103/PhysRevLett.85.461},
  abstract = {An information theoretic measure is derived that quantifies the statistical coherence between systems evolving in time. The standard time delayed mutual information fails to distinguish information that is actually exchanged from shared information due to common history and input signals. In our new approach, these influences are excluded by appropriate conditioning of transition probabilities. The resulting transfer entropy is able to distinguish effectively driving and responding elements and to detect asymmetry in the interaction of subsystems.},
  timestamp = {2017-07-18T20:29:36Z},
  number = {2},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.85.461},
  journal = {Physical Review Letters},
  author = {Schreiber, Thomas},
  month = jul,
  year = {2000},
  pages = {461--464},
  file = {APS Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\74W2G3TJ\\PhysRevLett.85.html:text/html},
  groups = {deep learning}
}

@article{janzing2013,
  title = {Quantifying Causal Influences},
  volume = {41},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/13-AOS1145},
  abstract = {Many methods for causal inference generate directed acyclic graphs (DAGs) that formalize causal relations between nnn variables. Given the joint distribution on all these variables, the DAG contains all information about how intervening on one variable changes the distribution of the other n-1n-1n-1 variables. However, quantifying the causal influence of one variable on another one remains a nontrivial question. Here we propose a set of natural, intuitive postulates that a measure of causal strength should satisfy. We then introduce a communication scenario, where edges in a DAG play the role of channels that can be locally corrupted by interventions. Causal strength is then the relative entropy distance between the old and the new distribution. Many other measures of causal strength have been proposed, including average causal effect, transfer entropy, directed information, and information flow. We explain how they fail to satisfy the postulates on simple DAGs of $\leq$3$\leq$3$\backslash$leq3 nodes. Finally, we investigate the behavior of our measure on time-series, supporting our claims with experiments on simulated data.},
  language = {EN},
  timestamp = {2017-07-18T20:30:59Z},
  number = {5},
  urldate = {2017-07-18},
  url = {http://projecteuclid.org/euclid.aos/1383661266},
  journal = {The Annals of Statistics},
  author = {Janzing, Dominik and Balduzzi, David and Grosse-Wentrup, Moritz and Sch{\"o}lkopf, Bernhard},
  month = oct,
  year = {2013},
  keywords = {Bayesian networks,Causality,information flow,transfer entropy},
  pages = {2324--2358},
  file = {Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\BZTHATBM\\1383661266.html:text/html},
  groups = {deep learning},
  mrnumber = {MR3127868},
  zmnumber = {1281.62030}
}

@article{quincey2017,
  title = {A Clearer Approach for Defining Unit Systems},
  volume = {54},
  issn = {0026-1394},
  doi = {10.1088/1681-7575/aa7160},
  abstract = {We present the SI and other unit systems, including cgs-em and cgs-es, in a framework whereby a system of fully independent and dimensionally orthogonal base units is modified by conventions designed to simplify the equations that are used within each system. We propose that the radian can be seen as an independent unit whose dimensional status is modified in the SI and other unit systems for this purpose. This framework clarifies how different unit systems are interrelated, and identifies the key pieces of information that are needed to define both a unit system and the equations that are to be used with it. Specifically, these are the size of the base units in the unsimplified system, together with sufficient equations to identify all the conventions adopted by the particular unit system. The appropriate extra information for the revised SI is presented. We do not propose that the treatment of angles as dimensionless within the SI is changed. It is also proposed that the Gaussian unit system is best seen as identical to cgs-es, but with the B and H symbols in equations used to represent `relativistic' versions of B and H , which should properly be treated as different quantities to the non-relativistic versions. The relativistic versions of B and H can similarly be used within the SI, with many of the advantages of the Heaviside\textendash{}Lorentz system.},
  language = {en},
  timestamp = {2017-07-20T14:41:21Z},
  number = {4},
  url = {http://stacks.iop.org/0026-1394/54/i=4/a=454},
  journal = {Metrologia},
  author = {Quincey, Paul and Brown, Richard J. C.},
  year = {2017},
  pages = {454},
  file = {IOP Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\8VPKH7CX\\Quincey and Brown - 2017 - A clearer approach for defining unit systems.pdf:application/pdf},
  groups = {angles}
}

@article{quincey2016,
  title = {Natural Units in Physics, and the Curious Case of the Radian},
  volume = {51},
  issn = {0031-9120},
  doi = {10.1088/0031-9120/51/6/065012},
  abstract = {Sets of natural units, like `atomic units', are sometimes used to simplify the equations of physics. This choice of units can be seen as a way of showing the relationships between quantities in their simplest form, in specialised situations, while still being correct. The unit system used for teaching, the SI, is instead designed primarily to be a set of units that can be applied consistently across all areas of science, commerce and industry. The SI also gives priority to emphasising the distinction between different quantities, by (most of the time) giving different units to different quantities, rather than producing the simplest possible equations. It is not widely appreciated that the SI treats the radian as the natural unit for angle, the only unit to be treated this way, with consequent issues for clear distinctions between quantities involving angles. The system that the SI would become if this anomaly were removed is presented. Rather than advocating this major change to the SI itself, it is proposed to highlight the existence of the underlying system to clarify how angles are handled physics, to make it easier to include angles in software calculations, and as an example of how the choice of unit system affects the equations we use.},
  language = {en},
  timestamp = {2017-07-20T14:43:03Z},
  number = {6},
  url = {http://stacks.iop.org/0031-9120/51/i=6/a=065012},
  journal = {Physics Education},
  author = {Quincey, Paul},
  year = {2016},
  pages = {065012},
  file = {IOP Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\774CI2V7\\Quincey - 2016 - Natural units in physics, and the curious case of .pdf:application/pdf},
  groups = {angles}
}

@book{williams2014,
  title = {Defining and {{Measuring Nature}}},
  isbn = {978-1-62705-279-5},
  timestamp = {2017-07-20T14:44:21Z},
  url = {http://iopscience.iop.org/book/978-1-627-05279-5},
  publisher = {{Morgan \& Claypool Publishers}},
  author = {Williams, Jeffrey},
  year = {2014},
  groups = {angles},
  doi = {10.1088/978-1-627-05279-5}
}

@article{dybkaer2004,
  title = {Units for Quantities of Dimension One},
  volume = {41},
  issn = {0026-1394},
  doi = {10.1088/0026-1394/41/1/010},
  abstract = {All quantities of dimension one are said to have the SI coherent derived unit "one" with the symbol '1'. (Single quotation marks are used here sometimes to indicate a quote, name, term or symbol; double quotation marks flag a concept when necessary.) Conventionally, the term and symbol may not be combined with the SI prefixes (except for the special terms and symbols for one and 1: radian, rad, and steradian, sr). This restriction is understandable, but leads to correct yet impractical alternatives and ISO deprecated symbols such as ppm or in some cases redundant combinations of units, such as mg/kg. "Number of entities" is dimensionally independent of the current base quantities and should take its rightful place among them. The corresponding base unit is "one". A working definition is given. Other quantities of dimension one are derived as fraction, ratio, efficiency, relative quantity, relative increment or characteristic number and may also use the unit "one", whether considered to be base or derived. The special term 'uno' and symbol 'u' in either case are proposed, allowing combination with SI prefixes.},
  language = {en},
  timestamp = {2017-07-20T14:54:23Z},
  number = {1},
  url = {http://stacks.iop.org/0026-1394/41/i=1/a=010},
  journal = {Metrologia},
  author = {Dybkaer, Ren{\'e}},
  year = {2004},
  pages = {69},
  file = {IOP Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\7M2X3957\\Dybkaer - 2004 - Units for quantities of dimension one.pdf:application/pdf},
  groups = {angles}
}

@article{mills2016a,
  title = {On the Units Radian and Cycle for the Quantity Plane Angle},
  volume = {53},
  issn = {0026-1394},
  doi = {10.1088/0026-1394/53/3/991},
  abstract = {This paper is concerned with the names and symbols for quantities used to describe oscillatory motion such as for a harmonic oscillator, and the units to be used for the quantity plane angle and phase angle for an oscillator, and related quantities. I draw attention to the need to carefully distinguish the names and symbols for quantities from the names and symbols for their numerical values in any application, and the significance of including units such as radian and cycle for the quantity plane angle. The familiar equations for a harmonic oscillator such as $\omega$ = 2 $\pi\nu$ , and the relation ${\Elzxh}$ = h /2 $\pi$ for the Planck constant, are shown to hold only if the symbols are taken to represent the dimensionless numerical values of the quantities concerned in particular units, rather than the actual values which are not dimensionless as generally used in the equations of physics. Alternative ways of handling these quantities and units are discussed.},
  language = {en},
  timestamp = {2017-07-20T14:54:41Z},
  number = {3},
  url = {http://stacks.iop.org/0026-1394/53/i=3/a=991},
  journal = {Metrologia},
  author = {Mills, Ian},
  year = {2016},
  pages = {991},
  file = {IOP Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\67UI5JAC\\Mills - 2016 - On the units radian and cycle for the quantity pla.pdf:application/pdf},
  groups = {angles}
}

@article{peil2017,
  title = {Microwave-Clock Timescale with Instability on Order of 10 -17},
  volume = {54},
  issn = {0026-1394},
  doi = {10.1088/1681-7575/aa65f7},
  abstract = {The fundamental limits of atomic fountains as operational clocks are considered. Four rubidium fountains in operation at the US Naval Observatory for over 5.5 years have demonstrated unprecedented long-term stability for continuously running clocks (Peil et al 2014 Metrologia 51 263\textendash{}9, Peil et al 2016 J. Phys.: Conf. Ser . 723 012004). With only these rubidium fountains, a post-processed timescale can be created that demonstrates superior long-term performance to any individual clock by compensating for occasional frequency steps. By comparing to the world's primary standards we demonstrate instability of this rubidium fountain timescale reaching the mid 10 -17 's and zero drift at the level of 1.3 \texttimes{} 10 -19 d -1 . We discuss fundamental limits due to common mode behaviour or individual fountain performance that cannot be corrected.},
  language = {en},
  timestamp = {2017-07-20T14:57:21Z},
  number = {3},
  url = {http://stacks.iop.org/0026-1394/54/i=3/a=247},
  journal = {Metrologia},
  author = {Peil, Steven and Swanson, Thomas B. and Hanssen, James and Taylor, Jennifer},
  year = {2017},
  pages = {247},
  file = {IOP Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\SKFTJFF2\\Peil et al. - 2017 - Microwave-clock timescale with instability on orde.pdf:application/pdf},
  groups = {time and frequency metrology}
}

@article{kuramoto2017,
  title = {Volume Measurements of 28 {{Si}}-Enriched Spheres Using an Improved Optical Interferometer for the Determination of the {{Avogadro}} Constant},
  volume = {54},
  issn = {0026-1394},
  doi = {10.1088/1681-7575/aa5379},
  abstract = {For the determination of the Avogadro constant by the x-ray crystal density method, an accurate volume measurement of 1 kg Si spheres is of primary importance. For this purpose, an optical interferometer was improved and used to measure the volumes of two 1 kg silicon spheres which were manufactured from a silicon crystal highly enriched in 28 Si. The apparent volumes of the spheres, which do not take into account the influence of the surface layers on the volume measurement by interferometry, were determined with a relative standard uncertainty of as small as 2.0 \texttimes{} 10 -8 . The surface of the spheres was characterized by using an improved spectroscopic ellipsometer. By considering the influence of the surface layers, the core volumes of the spheres, which exclude the surface layers, were determined. These results were used for the determination of the Avogadro constant in 2015 as a framework organized by the International Avogadro Coordination project. This paper provides details on the measurements, the improvements made to the apparatus, the data analysis and the uncertainty evaluation.},
  language = {en},
  timestamp = {2017-07-20T15:00:39Z},
  number = {2},
  url = {http://stacks.iop.org/0026-1394/54/i=2/a=193},
  journal = {Metrologia},
  author = {Kuramoto, Naoki and Azuma, Yasushi and Inaba, Hajime and Fujii, Kenichi},
  year = {2017},
  pages = {193},
  file = {IOP Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\XG8N2MXC\\Kuramoto et al. - 2017 - Volume measurements of 28 Si-enriched spheres usin.pdf:application/pdf},
  groups = {mass metrology}
}

@article{parisi2016,
  title = {A New Approach to {{UTC}} Calculation by Means of the {{Kalman}} Filter},
  volume = {53},
  issn = {0026-1394},
  doi = {10.1088/0026-1394/53/5/1185},
  abstract = {In this paper a new approach to Coordinated Universal Time (UTC) calculation is presented by means of the Kalman filter. An ensemble of atomic clocks participating in UTC is selected for analyzing and testing the potentiality of this new method.},
  language = {en},
  timestamp = {2017-07-20T15:04:11Z},
  number = {5},
  url = {http://stacks.iop.org/0026-1394/53/i=5/a=1185},
  journal = {Metrologia},
  author = {Parisi, Federica and Panfilo, Gianna},
  year = {2016},
  pages = {1185},
  file = {IOP Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\BUXRBK37\\Parisi and Panfilo - 2016 - A new approach to UTC calculation by means of the .pdf:application/pdf},
  groups = {time and frequency metrology}
}

@misc{zotero-null-316,
  title = {Agent {{Above}}, {{Atom Below}}: {{How}} Agents Causally Emerge from Their Underlying Microphysics by {{Erik P Hoel}}},
  shorttitle = {Agent {{Above}}, {{Atom Below}}},
  abstract = {Essay AbstractSome physical entities, which we often refer to as agents, can be described as having intentions and engaging in goal-oriented behavior. Yet agents can also be described in terms of low-level dynamics that are mindless, intention-less, and without goals or purpose. How we can reconcile via @FQXi},
  timestamp = {2017-07-21T19:29:27Z},
  urldate = {2017-07-21},
  url = {http://fqxi.org/community/forum/topic/2873},
  file = {Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\3F8EWBCF\\2873.html:text/html},
  groups = {deep learning}
}

@article{wolpert2008,
  title = {Physical Limits of Inference},
  volume = {237},
  issn = {01672789},
  doi = {10.1016/j.physd.2008.03.040},
  abstract = {I show that physical devices that perform observation, prediction, or recollection share an underlying mathematical structure. I call devices with that structure "inference devices". I present a set of existence and impossibility results concerning inference devices. These results hold independent of the precise physical laws governing our universe. In a limited sense, the impossibility results establish that Laplace was wrong to claim that even in a classical, non-chaotic universe the future can be unerringly predicted, given sufficient knowledge of the present. Alternatively, these impossibility results can be viewed as a non-quantum mechanical "uncertainty principle". Next I explore the close connections between the mathematics of inference devices and of Turing Machines. In particular, the impossibility results for inference devices are similar to the Halting theorem for TM's. Furthermore, one can define an analog of Universal TM's (UTM's) for inference devices. I call those analogs "strong inference devices". I use strong inference devices to define the "inference complexity" of an inference task, which is the analog of the Kolmogorov complexity of computing a string. However no universe can contain more than one strong inference device. So whereas the Kolmogorov complexity of a string is arbitrary up to specification of the UTM, there is no such arbitrariness in the inference complexity of an inference task. I end by discussing the philosophical implications of these results, e.g., for whether the universe "is" a computer.},
  timestamp = {2017-08-16T21:30:00Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0708.1362},
  number = {9},
  url = {http://arxiv.org/abs/0708.1362},
  journal = {Physica D: Nonlinear Phenomena},
  author = {Wolpert, David H.},
  month = jul,
  year = {2008},
  keywords = {Computer Science - Computational Complexity,Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,General Relativity and Quantum Cosmology},
  pages = {1257--1281},
  file = {arXiv\:0708.1362 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\E4A8HD7M\\Wolpert - 2008 - Physical limits of inference.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\8TZDME4E\\0708.html:text/html},
  groups = {deep learning}
}

@book{tse2013,
  series = {The Neural Basis of Free Will: Criterial Causation},
  title = {The Neural Basis of Free Will: {{Criterial}} Causation},
  shorttitle = {The Neural Basis of Free Will},
  abstract = {The issues of mental causation, consciousness, and free will have vexed philosophers since Plato. In this book, Peter Tse examines these unresolved issues from a neuroscientific perspective. In contrast with philosophers who use logic rather than data to argue whether mental causation or consciousness can exist given unproven first assumptions, Tse proposes that we instead listen to what neurons have to say. Tse draws on exciting recent neuroscientific data concerning how informational causation is realized in physical causation at the level of NMDA receptors, synapses, dendrites, neurons, and neuronal circuits. He argues that a particular kind of strong free will and "downward" mental causation are realized in rapid synaptic plasticity. Such informational causation cannot change the physical basis of information realized in the present, but it can change the physical basis of information that may be realized in the immediate future. This gets around the standard argument against free will centered on the impossibility of self-causation. Tse explores the ways that mental causation and qualia might be realized in this kind of neuronal and associated information-processing architecture, and considers the psychological and philosophical implications of having such an architecture realized in our brains. \textcopyright{} 2013 Massachusetts Institute of Technology. All rights reserved.},
  timestamp = {2017-08-16T21:56:49Z},
  author = {Tse, P.U.},
  year = {2013},
  file = {SCOPUS Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\GMKXAEC7\\display.html:text/html},
  groups = {deep learning}
}

@article{brown2009,
  title = {Reentrant Emergence},
  volume = {46},
  timestamp = {2017-08-16T21:57:19Z},
  number = {3},
  journal = {American Philosophical Quarterly},
  author = {Brown, S.R.},
  year = {2009},
  pages = {225--238},
  file = {SCOPUS Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\JBI48UCD\\display.html:text/html},
  groups = {deep learning}
}

@article{mehta2014,
  title = {An Exact Mapping between the {{Variational Renormalization Group}} and {{Deep Learning}}},
  abstract = {Deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data. Recently, such techniques have yielded record-breaking results on a diverse set of difficult machine learning tasks in computer vision, speech recognition, and natural language processing. Despite the enormous success of deep learning, relatively little is understood theoretically about why these techniques are so successful at feature learning and compression. Here, we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics, the renormalization group (RG). RG is an iterative coarse-graining scheme that allows for the extraction of relevant features (i.e. operators) as a physical system is examined at different length scales. We construct an exact mapping from the variational renormalization group, first introduced by Kadanoff, and deep learning architectures based on Restricted Boltzmann Machines (RBMs). We illustrate these ideas using the nearest-neighbor Ising Model in one and two-dimensions. Our results suggests that deep learning algorithms may be employing a generalized RG-like scheme to learn relevant features from data.},
  timestamp = {2017-09-11T14:02:55Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1410.3831},
  primaryClass = {cond-mat, stat},
  url = {http://arxiv.org/abs/1410.3831},
  journal = {arXiv:1410.3831 [cond-mat, stat]},
  author = {Mehta, Pankaj and Schwab, David J.},
  month = oct,
  year = {2014},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Statistical Mechanics,Statistics - Machine Learning},
  file = {arXiv\:1410.3831 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\JNZZUP5H\\Mehta and Schwab - 2014 - An exact mapping between the Variational Renormali.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\AR4E3X2V\\1410.html:text/html},
  groups = {deep learning}
}

@misc{martin2015,
  title = {Why {{Deep Learning Works II}}: The {{Renormalization Group}}},
  shorttitle = {Why {{Deep Learning Works II}}},
  abstract = {Deep Learning is amazing. ~But why is Deep Learning so successful? ~Is Deep Learning just old-school Neural Networks on modern hardware? ~Is it just that we have so much data now the methods work b\ldots{}},
  timestamp = {2017-09-11T14:03:05Z},
  urldate = {2017-09-11},
  url = {https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/},
  journal = {CALCULATED CONTENT},
  author = {Martin, Charles H. and {PhD}},
  year = {2015-04-01T16:39:13+00:00},
  file = {Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\UGNDUMXB\\why-deep-learning-works-ii-the-renormalization-group.html:text/html},
  groups = {deep learning}
}

@article{sfondrini2012,
  title = {An Introduction to Universality and Renormalization Group Techniques},
  abstract = {These lecture notes have been written for a short introductory course on universality and renormalization group techniques given at the VIII Modave School in Mathematical Physics by the author, intended for PhD students and researchers new to these topics. First the basic ideas of dynamical systems (fixed points, stability, etc.) are recalled, and an example of universality is discussed in this context: this is Feigenbaum's universality of the period doubling cascade for iterated maps on the interval. It is shown how renormalization ideas can be applied to explain universality and compute Feigenbaum's constants. Then, universality is presented in the scenario of quantum field theories, and studied by means of functional renormalization group equations, which allow for a close comparison with the case of dynamical systems. In particular, Wetterich equation for a scalar field is derived and discussed, and then applied to the computation of the Wilson-Fisher fixed point and critical exponent for the Ising universality class. References to more advanced topics and applications are provided.},
  timestamp = {2017-09-11T14:03:10Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1210.2262},
  primaryClass = {cond-mat, physics:hep-th, physics:math-ph},
  url = {http://arxiv.org/abs/1210.2262},
  journal = {arXiv:1210.2262 [cond-mat, physics:hep-th, physics:math-ph]},
  author = {Sfondrini, Alessandro},
  month = oct,
  year = {2012},
  keywords = {37-06; 37E05; 37E20; 81-06; 81T17; 82B28,Condensed Matter - Statistical Mechanics,High Energy Physics - Theory,Mathematical Physics},
  file = {arXiv\:1210.2262 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\TS6PF45E\\Sfondrini - 2012 - An introduction to universality and renormalizatio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\49GVVD3I\\1210.html:text/html},
  groups = {deep learning}
}

@article{apenko2009,
  title = {Information Theory and Renormalization Group Flows},
  abstract = {We present a possible approach to the study of the renormalization group (RG) flow based entirely on the information theory. The average information loss under a single step of Wilsonian RG transformation is evaluated as a conditional entropy of the fast variables, which are integrated out, when the slow ones are held fixed. Its positivity results in the monotonic decrease of the informational entropy under renormalization. This, however, does not necessarily imply the irreversibility of the RG flow, because the entropy explicitly depends on the total number of degrees of freedom, which is reduced. Only some size-independent additive part of the entropy could possibly provide the required Lyapunov function. We also introduce a mutual information of fast and slow variables as probably a more adequate quantity to represent the changes in the system under renormalization and evaluate it for some simple systems. It is shown that for certain real space decimation transformations the positivity of the mutual information directly leads to the monotonic growth of the entropy per lattice site along the RG flow and hence to its irreversibility.},
  timestamp = {2017-09-12T18:25:36Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0910.2097},
  primaryClass = {cond-mat, physics:hep-th},
  url = {http://arxiv.org/abs/0910.2097},
  journal = {arXiv:0910.2097 [cond-mat, physics:hep-th]},
  author = {Apenko, S. M.},
  month = oct,
  year = {2009},
  keywords = {Condensed Matter - Statistical Mechanics,High Energy Physics - Theory},
  file = {arXiv\:0910.2097 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\R7WDM623\\Apenko - 2009 - Information theory and renormalization group flows.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\RMII9E5S\\0910.html:text/html},
  groups = {deep learning}
}

@book{kelsey1997,
  title = {Pre-{{Scheme}}: {{A Scheme Dialect}} for {{Systems Programming}}},
  shorttitle = {Pre-{{Scheme}}},
  abstract = {Pre-Scheme is a statically typed dialect of Scheme that gives the programmer the efficiency and lowlevel machine access of C while retaining many of the desirable features of Scheme. The PreScheme compiler makes use of type inference, partial evaluation and Scheme and Lisp compiler technology to compile the problematic features of Scheme, such as closures, into C code without significant run-time overhead. Use of such features in Pre-Scheme programs is restricted to those cases that can be compiled into efficient code. Type reconstruction is done using a modified Hindley/Milner algorithm that allows overloaded user-defined functions. All top-level forms in Pre-Scheme programs are evaluated at compile time, which gives the user additional control over the compiler's partial evaluation of a program. Pre-Scheme has been implemented and used to write a byte-code interpeter and associated support code for a complete Scheme implementation. 1 Introduction  High-level programming languages, su...},
  timestamp = {2017-10-18T21:45:02Z},
  author = {Kelsey, Richard A.},
  year = {1997},
  file = {Citeseer - Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\JMTPVEJR\\Kelsey - 1997 - Pre-Scheme A Scheme Dialect for Systems Programmi.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\A2J7BWPS\\summary.html:text/html},
  groups = {programming languages}
}

@article{oliva1995,
  title = {The {{VLISP Verified PreScheme Compiler}}},
  volume = {8},
  issn = {0892-4635},
  doi = {10.1007/BF01128408},
  timestamp = {2017-10-18T21:51:06Z},
  number = {1-2},
  url = {http://dx.doi.org/10.1007/BF01128408},
  journal = {Lisp Symb. Comput.},
  author = {Oliva, Dino P. and Ramsdell, John D. and Wand, Mitchell},
  month = mar,
  year = {1995},
  keywords = {compiler,verified},
  pages = {111--182},
  groups = {programming languages}
}

@article{sabour2017,
  title = {Dynamic {{Routing Between Capsules}}},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation paramters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  timestamp = {2017-11-01T16:22:57Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.09829},
  primaryClass = {cs},
  url = {http://arxiv.org/abs/1710.09829},
  journal = {arXiv:1710.09829 [cs]},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
  month = oct,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {arXiv\:1710.09829 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\GIGXDQEC\\Sabour et al. - 2017 - Dynamic Routing Between Capsules.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\E5VRJNMH\\1710.html:text/html},
  groups = {deep learning}
}

@article{anonymous2017,
  title = {Matrix Capsules with {{EM}} Routing},
  timestamp = {2017-11-01T16:24:04Z},
  urldate = {2017-11-01},
  url = {https://openreview.net/forum?id=HJWLfGWRb\&noteId=HJWLfGWRb},
  author = {Anonymous},
  month = oct,
  year = {2017},
  file = {Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\GT557JCD\\Anonymous - 2017 - Matrix capsules with EM routing.pdf:application/pdf;Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\DMN966CM\\forum.html:text/html},
  groups = {deep learning}
}

@misc{timmer2017,
  title = {Inspired by Brain's Visual Cortex, New {{AI}} Utterly Wrecks {{CAPTCHA}} Security},
  abstract = {Recognizing surfaces and edges key to picking out distorted characters.},
  timestamp = {2017-11-01T16:46:22Z},
  urldate = {2017-11-01},
  url = {https://arstechnica.com/science/2017/10/inspired-by-brains-visual-cortex-new-ai-utterly-wrecks-captcha-security/},
  journal = {Ars Technica},
  author = {Timmer, John},
  month = oct,
  year = {2017},
  file = {Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\MI8MGHAU\\inspired-by-brains-visual-cortex-new-ai-utterly-wrecks-captcha-security.html:text/html},
  groups = {deep learning}
}

@misc{lee2017,
  title = {Higgs Boson Uncovered by Quantum Algorithm on {{D}}-{{Wave}} Machine},
  abstract = {Particle physics data sorted by quantum machine learning but still needs work.},
  timestamp = {2017-11-01T17:03:27Z},
  urldate = {2017-11-01},
  url = {https://arstechnica.com/science/2017/10/quantum-algorithm-finds-higgs-needle-in-photon-haystack/},
  journal = {Ars Technica},
  author = {Lee, Chris},
  month = oct,
  year = {2017},
  file = {Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\T63AU9IT\\quantum-algorithm-finds-higgs-needle-in-photon-haystack.html:text/html},
  groups = {deep learning}
}

@article{salge2017,
  title = {Empowerment {{As Replacement}} for the {{Three Laws}} of {{Robotics}}},
  volume = {4},
  issn = {2296-9144},
  doi = {10.3389/frobt.2017.00025},
  abstract = {The greater ubiquity of robots creates a need for generic guidelines for robot behaviour. We focus less on how a robot can technically achieve a predefined goal, and more on what a robot should do in the first place. Particularly, we are interested in the question how a heuristic should look like which motivates the robot's behaviour in interaction with human agents. We make a concrete, operational proposal as to how the information-theoretic concept of empowerment can be used as a generic heuristic to quantify concepts such as self-preservation, protection of the human partner and responding to human actions. While elsewhere we studied involved single-agent scenarios in detail, here we present proof-of-principle scenarios demonstrating how empowerment interpreted in light of these perspectives allows one to specify core concepts with a similar aim as Asimov's Three Laws of Robotics in an operational way. Importantly, this route does not depend on having to establish an explicit verbalized understanding of human language and conventions in the robots. Also, it incorporates the ability to take into account a rich variety of different situations and types of robotic embodiment.},
  language = {English},
  timestamp = {2017-11-06T23:49:07Z},
  urldate = {2017-11-06},
  url = {https://www.frontiersin.org/articles/10.3389/frobt.2017.00025/full},
  journal = {Frontiers in Robotics and AI},
  author = {Salge, Christoph and Polani, Daniel},
  year = {2017},
  keywords = {empowerment,human robot interaction,information-theory,intrinsic motivation,robot,Three Laws of Robotics,Value function},
  groups = {deep learning}
}

@misc{2017,
  title = {Researchers {{Print Washable Electronics Directly Into Fabric}}},
  abstract = {Researchers from the University of Cambridge have successfully printed integrated electronic circuits into fabric, opening up new smart textile opportunities. The textile-based electronics are centered around...},
  timestamp = {2017-11-10T22:41:11Z},
  urldate = {2017-11-10},
  url = {https://www.ecnmag.com/blog/2017/11/researchers-print-washable-electronics-directly-fabric},
  journal = {Electronic Component News},
  year = {2017-11-10T12:17:24-05:00},
  file = {Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\JT3NUGCQ\\researchers-print-washable-electronics-directly-fabric.html:text/html}
}

@misc{2017a,
  title = {Researchers {{Print Washable Electronics Directly Into Fabric}}},
  abstract = {Researchers from the University of Cambridge have successfully printed integrated electronic circuits into fabric, opening up new smart textile opportunities. The textile-based electronics are centered around...},
  timestamp = {2017-11-10T22:42:14Z},
  urldate = {2017-11-10},
  url = {https://www.ecnmag.com/blog/2017/11/researchers-print-washable-electronics-directly-fabric},
  journal = {Electronic Component News},
  year = {2017-11-10T12:17:24-05:00},
  file = {Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\3PJ9ZU4Z\\researchers-print-washable-electronics-directly-fabric.html:text/html}
}

@misc{zotero-null-368,
  title = {Researchers {{Print Washable Electronics Directly Into Fabric}}},
  timestamp = {2017-11-10T22:42:26Z},
  urldate = {2017-11-10},
  url = {https://www.ecnmag.com/blog/2017/11/researchers-print-washable-electronics-directly-fabric?et_cid=6169853\&et_rid=45547034\&type=headline\&et_cid=6169853\&et_rid=45547034\&linkid=Featured+Headline},
  file = {Researchers Print Washable Electronics Directly Into Fabric:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\GFDIQSN3\\researchers-print-washable-electronics-directly-fabric.html:text/html}
}

@misc{zotero-null-372,
  title = {A {{Physicist}}'s {{Physicist Ponders}} the {{Nature}} of {{Reality}}},
  abstract = {Edward Witten reflects on the meaning of dualities in physics and math, emergent space-time, and the pursuit of a complete description of nature.},
  timestamp = {2017-12-26T23:23:21Z},
  urldate = {2017-12-26},
  url = {https://www.wired.com/story/a-physicists-physicist-ponders-the-nature-of-reality/},
  journal = {WIRED},
  file = {Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\JHMSUQWK\\a-physicists-physicist-ponders-the-nature-of-reality.html:text/html}
}

@misc{zotero-null-374,
  title = {A {{Physicist}}'s {{Physicist Ponders}} the {{Nature}} of {{Reality}}},
  abstract = {Edward Witten reflects on the meaning of dualities in physics and math, emergent space-time, and the pursuit of a complete description of nature.},
  timestamp = {2017-12-26T23:23:47Z},
  urldate = {2017-12-26},
  url = {https://www.wired.com/story/a-physicists-physicist-ponders-the-nature-of-reality/},
  journal = {WIRED},
  file = {Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\2MW34UDP\\a-physicists-physicist-ponders-the-nature-of-reality.html:text/html},
  groups = {deep learning}
}

@misc{zotero-null-376,
  title = {Nginx - {{Full Stack Python}}},
  timestamp = {2017-12-26T23:26:18Z},
  urldate = {2017-12-26},
  url = {https://www.fullstackpython.com/nginx.html},
  groups = {python}
}

@article{frieden2014,
  title = {Derivation of {{Principle}} of {{Extreme Physical Information}}},
  abstract = {The unknown amplitude law q(x) defining an observed effect may be found using the principle of Extreme Physical Information. EPI is derived as follows. The observations follow an information flow J --$>$ I, with J the information intrinsic to the source and I the Fisher information level in its data, obeying (i) I=4 Integral dx q' -squared. Here q'= dq/dx and p(x) = q(x)-squared is the probability. It was previously shown, using L. Hardy's 5 axioms defining physics, that I = max. Therefore, its variation (ii) delta I = 0. Note that I is generic, obeying (i) for all source effects, whereas J is specific to the particular effect. Hence, rather than having form (i), J obeys (iii) J = Integral dx j[q(x),s(x)] with j some function of its arguments and s(x) a known source, such as of mass, biological fitness, etc. Information I decreases under any irreversible operation such as measurement, so that I l.e. J or, equivalently, I = kJ where 0 l.e. k l.e. 1. Then the variation delta I = k delta J so that property (ii) gives (iv) delta J = 0 as well. Then combining (ii) and (iv), delta(I - J) = 0. Or, I - J = L = extremum. What kind of extremum? Eqs. (i) and (iii) give (v) L=4q'\^2 - j[q(x),s(x)]. Differentiating (v), (d\^2 L )/(dq'\^2) = +8. Then by the Legendre condition the extremum is a minimum. The unknown source effect obeys (vi) I - J= minimum, EPI.},
  timestamp = {2017-12-26T23:54:40Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.3615},
  primaryClass = {physics},
  url = {http://arxiv.org/abs/1406.3615},
  journal = {arXiv:1406.3615 [physics]},
  author = {Frieden, B. Roy and Gatenby, Robert A.},
  month = jun,
  year = {2014},
  keywords = {Physics - Computational Physics,Physics - Data Analysis; Statistics and Probability},
  file = {arXiv\:1406.3615 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\NUMCAQHZ\\Frieden and Gatenby - 2014 - Derivation of Principle of Extreme Physical Inform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\AHGZKTPF\\1406.html:text/html},
  groups = {deep learning}
}

@article{frieden2006,
  title = {Information-Theoretic Significance of the {{Wigner}} Distribution},
  volume = {74},
  issn = {1050-2947, 1094-1622},
  doi = {10.1103/PhysRevA.74.052108},
  abstract = {A coarse grained Wigner distribution p\_\{W\}(x,u) obeying positivity derives out of information-theoretic considerations. Let p(x,u) be the unknown joint PDF (probability density function) on position- and momentum fluctuations x,u for a pure state particle. Suppose that the phase part Psi(x,z) of its Fourier transform F.T.[p(x,u)]=|Z(x,z)|exp[iPsi(x,z)] is constructed as a hologram. (Such a hologram is often used in heterodyne interferometry.) Consider a particle randomly illuminating this phase hologram. Let its two position coordinates be measured. Require that the measurements contain an extreme amount of Fisher information about true position, through variation of the phase function Psi(x,z). The extremum solution gives an output PDF p(x,u) that is the convolution of the Wigner p\_\{W\}(x,u) with an instrument function defining uncertainty in either position x or momentum u. The convolution arises naturally out of the approach, and is one-dimensional, in comparison with the two-dimensional convolutions usually proposed for coarse graining purposes. The output obeys positivity, as required of a PDF, if the one-dimensional instrument function is sufficiently wide. The result holds for a large class of systems: those whose amplitudes a(x) are the same at their boundaries (Examples: states a(x) with positive parity; with periodic boundary conditions; free particle trapped in a box).},
  timestamp = {2017-12-27T00:06:07Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {quant-ph/0609157},
  number = {5},
  url = {http://arxiv.org/abs/quant-ph/0609157},
  journal = {Physical Review A},
  author = {Frieden, B. Roy and Soffer, Bernard H.},
  month = nov,
  year = {2006},
  keywords = {Quantum Physics},
  file = {arXiv\:quant-ph/0609157 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\TZTIH53V\\Frieden and Soffer - 2006 - Information-theoretic significance of the Wigner d.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\7T72JMTQ\\0609157.html:text/html},
  groups = {deep learning}
}

@article{janke2004,
  series = {Proceedings of the XVIII Max Born Symposium "Statistical Physics outside Physics"},
  title = {Information Geometry and Phase Transitions},
  volume = {336},
  issn = {0378-4371},
  doi = {10.1016/j.physa.2004.01.023},
  abstract = {The introduction of a metric onto the space of parameters in models in statistical mechanics and beyond gives an alternative perspective on their phase structure. In such a geometrisation, the scalar curvature, R, plays a central role. A non-interacting model has a flat geometry (R=0), while R diverges at the critical point of an interacting one. Here, the information geometry is studied for a number of solvable statistical\textendash{}mechanical models.},
  timestamp = {2018-01-02T17:55:38Z},
  number = {1},
  url = {http://www.sciencedirect.com/science/article/pii/S0378437104000469},
  journal = {Physica A: Statistical Mechanics and its Applications},
  author = {Janke, W. and Johnston, D. A. and Kenna, R.},
  month = may,
  year = {2004},
  keywords = {Information geometry,Phase transitions},
  pages = {181--186},
  file = {ScienceDirect Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\USF6UASX\\Janke et al. - 2004 - Information geometry and phase transitions.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\UMPVVXMC\\S0378437104000469.html:text/html},
  groups = {deep learning}
}

@article{turing1937,
  title = {Computability and $\lambda$-{{Definability}}},
  volume = {2},
  issn = {0022-4812},
  doi = {10.2307/2268280},
  timestamp = {2018-01-02T21:11:58Z},
  number = {4},
  url = {http://www.jstor.org/stable/2268280},
  journal = {The Journal of Symbolic Logic},
  author = {Turing, A. M.},
  year = {1937},
  pages = {153--163},
  groups = {deep learning}
}

@article{caticha2016,
  title = {Geometry from Information Geometry},
  volume = {1757},
  issn = {0094-243X},
  doi = {10.1063/1.4959050},
  timestamp = {2018-01-08T20:42:38Z},
  number = {1},
  url = {http://aip.scitation.org/doi/abs/10.1063/1.4959050},
  journal = {AIP Conference Proceedings},
  author = {Caticha, Ariel},
  month = jul,
  year = {2016},
  pages = {030001},
  file = {Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\XMRHK5R3\\Caticha - 2016 - Geometry from information geometry.pdf:application/pdf;Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\GZ88I236\\1.html:text/html},
  groups = {deep learning}
}

@article{amari2001,
  title = {Information Geometry on Hierarchy of Probability Distributions},
  volume = {47},
  issn = {0018-9448},
  doi = {10.1109/18.930911},
  abstract = {An exponential family or mixture family of probability distributions has a natural hierarchical structure. This paper gives an ``orthogonal'' decomposition of such a system based on information geometry. A typical example is the decomposition of stochastic dependency among a number of random variables. In general, they have a complex structure of dependencies. Pairwise dependency is easily represented by correlation, but it is more difficult to measure effects of pure triplewise or higher order interactions (dependencies) among these variables. Stochastic dependency is decomposed quantitatively into an ``orthogonal'' sum of pairwise, triplewise, and further higher order dependencies. This gives a new invariant decomposition of joint entropy. This problem is important for extracting intrinsic interactions in firing patterns of an ensemble of neurons and for estimating its functional connections. The orthogonal decomposition is given in a wide class of hierarchical structures including both exponential and mixture families. As an example, we decompose the dependency in a higher order Markov chain into a sum of those in various lower order Markov chains},
  timestamp = {2018-01-08T20:50:16Z},
  number = {5},
  journal = {IEEE Transactions on Information Theory},
  author = {Amari, S. I.},
  month = jul,
  year = {2001},
  keywords = {correlation,entropy,exponential family,firing patterns,functional connections estimation,Hierarchical systems,higher order interaction,higher order Markov chain,Information geometry,Information theory,invariant decomposition,joint entropy,lower order Markov chains,Markov processes,mixture family,neurons,neurophysiology,Neuroscience,orthogonal decomposition,pairwise dependency,Pairwise error probability,probability,Probability distribution,probability distributions hierarchy,random processes,random variables,stochastic dependency decomposition,Stochastic processes,triplewise interaction},
  pages = {1701--1711},
  file = {IEEE Xplore Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\4R87SCPA\\Amari - 2001 - Information geometry on hierarchy of probability d.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\KVR78TBA\\930911.html:text/html},
  groups = {deep learning}
}

@misc{zotero-null-398,
  title = {Information {{Geometry}}},
  timestamp = {2018-01-08T20:53:13Z},
  urldate = {2018-01-08},
  url = {http://bactra.org/notebooks/info-geo.html},
  groups = {deep learning}
}

@article{caticha2015,
  title = {The {{Basics}} of {{Information Geometry}}},
  doi = {10.1063/1.4905960},
  abstract = {To what extent can we distinguish one probability distribution from another? Are there quantitative measures of distinguishability? The goal of this tutorial is to approach such questions by introducing the notion of the "distance" between two probability distributions and exploring some basic ideas of such an "information geometry".},
  timestamp = {2018-01-08T20:54:12Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.5633},
  primaryClass = {physics},
  url = {http://arxiv.org/abs/1412.5633},
  journal = {arXiv:1412.5633 [physics]},
  author = {Caticha, Ariel},
  year = {2015},
  keywords = {Physics - Data Analysis; Statistics and Probability},
  pages = {15--26},
  file = {arXiv\:1412.5633 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\KDF33U5H\\Caticha - 2015 - The Basics of Information Geometry.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\4JSNASSA\\1412.html:text/html},
  groups = {deep learning}
}

@misc{zotero-null-405,
  title = {Information {{Geometry}}},
  timestamp = {2018-01-08T21:13:06Z},
  urldate = {2018-01-08},
  url = {http://math.ucr.edu/home/baez/information/},
  groups = {deep learning}
}

@article{ito2017,
  title = {Stochastic {{Thermodynamic Interpretation}} of {{Information Geometry}}},
  abstract = {In recent years, the unified theory of information and thermodynamics has been intensively discussed in the context of stochastic thermodynamics. The unified theory reveals that information theory would be useful to understand non-stationary dynamics of systems far from equilibrium. In this letter, we have found a new link between stochastic thermodynamics and information theory well known as information geometry. By applying this link, information geometric inequality can be interpreted as thermodynamic uncertainty relationship between speed and thermodynamic cost. We have numerically applied information geometric inequality to a thermodynamic model of biochemical enzyme reaction.},
  timestamp = {2018-01-10T23:19:35Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.04311},
  primaryClass = {cond-mat},
  url = {http://arxiv.org/abs/1712.04311},
  journal = {arXiv:1712.04311 [cond-mat]},
  author = {Ito, Sosuke},
  month = dec,
  year = {2017},
  keywords = {Condensed Matter - Statistical Mechanics},
  file = {arXiv\:1712.04311 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\B6RB78QS\\Ito - 2017 - Stochastic Thermodynamic Interpretation of Informa.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\GFQEKNG4\\1712.html:text/html},
  groups = {deep learning}
}

@article{cafaro2016a,
  title = {The {{Information Geometry}} of {{Chaos}}},
  abstract = {We present a new theoretical information-geometric framework (IGAC, Information Geometrodynamical Approach to Chaos) suitable to characterize chaotic dynamical behavior of arbitrary complex systems.},
  timestamp = {2018-01-10T23:20:42Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1601.07935},
  primaryClass = {math-ph},
  url = {http://arxiv.org/abs/1601.07935},
  journal = {arXiv:1601.07935 [math-ph]},
  author = {Cafaro, Carlo},
  month = jan,
  year = {2016},
  keywords = {Mathematical Physics},
  file = {arXiv\:1601.07935 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\9I55SHF6\\Cafaro - 2016 - The Information Geometry of Chaos.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\7VKQNNB5\\1601.html:text/html},
  groups = {deep learning}
}

@article{nielsen2013,
  title = {Cramer-{{Rao Lower Bound}} and {{Information Geometry}}},
  abstract = {This article focuses on an important piece of work of the world renowned Indian statistician, Calyampudi Radhakrishna Rao. In 1945, C. R. Rao (25 years old then) published a pathbreaking paper, which had a profound impact on subsequent statistical research.},
  timestamp = {2018-01-10T23:21:32Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.3578},
  primaryClass = {cs, math},
  url = {http://arxiv.org/abs/1301.3578},
  journal = {arXiv:1301.3578 [cs, math]},
  author = {Nielsen, Frank},
  month = jan,
  year = {2013},
  keywords = {Computer Science - Information Theory},
  file = {arXiv\:1301.3578 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\W7B46JPE\\Nielsen - 2013 - Cramer-Rao Lower Bound and Information Geometry.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\GHVKQ3PJ\\1301.html:text/html},
  groups = {deep learning}
}

@article{ito2017a,
  title = {Stochastic {{Thermodynamic Interpretation}} of {{Information Geometry}}},
  abstract = {In recent years, the unified theory of information and thermodynamics has been intensively discussed in the context of stochastic thermodynamics. The unified theory reveals that information theory would be useful to understand non-stationary dynamics of systems far from equilibrium. In this letter, we have found a new link between stochastic thermodynamics and information theory well known as information geometry. By applying this link, information geometric inequality can be interpreted as thermodynamic uncertainty relationship between speed and thermodynamic cost. We have numerically applied information geometric inequality to a thermodynamic model of biochemical enzyme reaction.},
  timestamp = {2018-01-15T18:20:26Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.04311},
  primaryClass = {cond-mat},
  url = {http://arxiv.org/abs/1712.04311},
  journal = {arXiv:1712.04311 [cond-mat]},
  author = {Ito, Sosuke},
  month = dec,
  year = {2017},
  keywords = {Condensed Matter - Statistical Mechanics},
  file = {arXiv\:1712.04311 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\RQBK5CV7\\Ito - 2017 - Stochastic Thermodynamic Interpretation of Informa.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\BHDX6U68\\1712.html:text/html},
  groups = {deep learning}
}

@article{jiao2013,
  title = {Universal {{Estimation}} of {{Directed Information}}},
  volume = {59},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/TIT.2013.2267934},
  abstract = {Four estimators of the directed information rate between a pair of jointly stationary ergodic finite-alphabet processes are proposed, based on universal probability assignments. The first one is a Shannon--McMillan--Breiman type estimator, similar to those used by Verd$\backslash$'u (2005) and Cai, Kulkarni, and Verd$\backslash$'u (2006) for estimation of other information measures. We show the almost sure and \$L\_1\$ convergence properties of the estimator for any underlying universal probability assignment. The other three estimators map universal probability assignments to different functionals, each exhibiting relative merits such as smoothness, nonnegativity, and boundedness. We establish the consistency of these estimators in almost sure and \$L\_1\$ senses, and derive near-optimal rates of convergence in the minimax sense under mild conditions. These estimators carry over directly to estimating other information measures of stationary ergodic finite-alphabet processes, such as entropy rate and mutual information rate, with near-optimal performance and provide alternatives to classical approaches in the existing literature. Guided by these theoretical results, the proposed estimators are implemented using the context-tree weighting algorithm as the universal probability assignment. Experiments on synthetic and real data are presented, demonstrating the potential of the proposed schemes in practice and the utility of directed information estimation in detecting and measuring causal influence and delay.},
  timestamp = {2018-01-22T22:57:44Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1201.2334},
  number = {10},
  url = {http://arxiv.org/abs/1201.2334},
  journal = {IEEE Transactions on Information Theory},
  author = {Jiao, Jiantao and Permuter, Haim H. and Zhao, Lei and Kim, Young-Han and Weissman, Tsachy},
  month = oct,
  year = {2013},
  keywords = {Computer Science - Information Theory},
  pages = {6220--6242},
  file = {arXiv\:1201.2334 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\KVIEPNBP\\Jiao et al. - 2013 - Universal Estimation of Directed Information.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\9WZFPJKQ\\1201.html:text/html},
  groups = {deep learning}
}

@article{modin2017,
  title = {Geometry of {{Matrix Decompositions Seen Through Optimal Transport}} and {{Information Geometry}}},
  volume = {9},
  issn = {1941-4889},
  doi = {10.3934/jgm.2017014},
  abstract = {The space of probability densities is an infinite-dimensional Riemannian manifold, with Riemannian metrics in two flavors: Wasserstein and Fisher--Rao. The former is pivotal in optimal mass transport (OMT), whereas the latter occurs in information geometry---the differential geometric approach to statistics. The Riemannian structures restrict to the submanifold of multivariate Gaussian distributions, where they induce Riemannian metrics on the space of covariance matrices. Here we give a systematic description of classical matrix decompositions (or factorizations) in terms of Riemannian geometry and compatible principal bundle structures. Both Wasserstein and Fisher--Rao geometries are discussed. The link to matrices is obtained by considering OMT and information geometry in the category of linear transformations and multivariate Gaussian distributions. This way, OMT is directly related to the polar decomposition of matrices, whereas information geometry is directly related to the \$QR\$, Cholesky, spectral, and singular value decompositions. We also give a coherent description of gradient flow equations for the various decompositions; most flows are illustrated in numerical examples. The paper is a combination of previously known and original results. As a survey it covers the Riemannian geometry of OMT and polar decompositions (smooth and linear category), entropy gradient flows, and the Fisher--Rao metric and its geodesics on the statistical manifold of multivariate Gaussian distributions. The original contributions include new gradient flows associated with various matrix decompositions, new geometric interpretations of previously studied isospectral flows, and a new proof of the polar decomposition of matrices based an entropy gradient flow.},
  timestamp = {2018-01-24T15:55:43Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1601.01875},
  number = {3},
  url = {http://arxiv.org/abs/1601.01875},
  journal = {Journal of Geometric Mechanics},
  author = {Modin, Klas},
  month = jun,
  year = {2017},
  keywords = {15A23; 53C21; 58B20; 15A18; 49M99; 65F15; 65F40,Mathematics - Differential Geometry,Mathematics - Numerical Analysis},
  pages = {335--390},
  file = {arXiv\:1601.01875 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\DBDZPWPC\\Modin - 2017 - Geometry of Matrix Decompositions Seen Through Opt.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\3R74K67H\\1601.html:text/html},
  groups = {deep learning}
}

@article{cafaro2010,
  title = {Reexamination of an {{Information Geometric Construction}} of {{Entropic Indicators}} of {{Complexity}}},
  abstract = {Information geometry and inductive inference methods can be used to model dynamical systems in terms of their probabilistic description on curved statistical manifolds. In this article, we present a formal conceptual reexamination of the information geometric construction of entropic indicators of complexity for statistical models. Specifically, we present conceptual advances in the interpretation of the information geometric entropy (IGE), a statistical indicator of temporal complexity (chaoticity) defined on curved statistical manifolds underlying the probabilistic dynamics of physical systems.},
  timestamp = {2018-01-31T19:43:55Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1011.5556},
  primaryClass = {math-ph},
  url = {http://arxiv.org/abs/1011.5556},
  journal = {arXiv:1011.5556 [math-ph]},
  author = {Cafaro, C. and Giffin, A. and Ali, S. A. and Kim, D.-H.},
  month = nov,
  year = {2010},
  keywords = {Mathematical Physics},
  file = {arXiv\:1011.5556 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\CCFG4IJP\\Cafaro et al. - 2010 - Reexamination of an Information Geometric Construc.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\PTA4QWVG\\1011.html:text/html},
  groups = {deep learning}
}

@article{cafaro2010a,
  title = {Reexamination of an Information Geometric Construction of Entropic Indicators of Complexity},
  volume = {217},
  issn = {0096-3003},
  doi = {10.1016/j.amc.2010.08.028},
  abstract = {Information geometry and inductive inference methods can be used to model dynamical systems in terms of their probabilistic description on curved statistical manifolds. In this article, we present a formal conceptual reexamination of the information geometric construction of entropic indicators of complexity for statistical models. Specifically, we present conceptual advances in the interpretation of the information geometric entropy (IGE), a statistical indicator of temporal complexity (chaoticity) defined on curved statistical manifolds underlying the probabilistic dynamics of physical systems.},
  timestamp = {2018-01-31T19:45:29Z},
  number = {7},
  url = {http://www.sciencedirect.com/science/article/pii/S0096300310008829},
  journal = {Applied Mathematics and Computation},
  author = {Cafaro, C. and Giffin, A. and Ali, S. A. and Kim, D. -H.},
  month = dec,
  year = {2010},
  keywords = {Chaos,Complexity,entropy,Probability theory,Riemannian geometry},
  pages = {2944--2951},
  file = {ScienceDirect Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\UTBS7NW3\\Cafaro et al. - 2010 - Reexamination of an information geometric construc.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\QGUD32HG\\S0096300310008829.html:text/html},
  groups = {deep learning}
}

@article{caiani1997,
  title = {Geometry of {{Dynamics}}, {{Lyapunov Exponents}}, and {{Phase Transitions}}},
  volume = {79},
  doi = {10.1103/PhysRevLett.79.4361},
  abstract = {The Hamiltonian dynamics of the classical planar Heisenberg model is numerically investigated in two and three dimensions. In three dimensions peculiar behaviors are found in the temperature dependence of the largest Lyapunov exponent and of other observables related to the geometrization of the dynamics. On the basis of a heuristic argument it is conjectured that the phase transition might correspond to a change in the topology of the manifolds whose geodesics are the motions of the system.},
  timestamp = {2018-02-08T16:20:23Z},
  number = {22},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.79.4361},
  journal = {Physical Review Letters},
  author = {Caiani, Lando and Casetti, Lapo and Clementi, Cecilia and Pettini, Marco},
  month = dec,
  year = {1997},
  pages = {4361--4364},
  file = {Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\UUB7EMSK\\Caiani et al. - 1997 - Geometry of Dynamics, Lyapunov Exponents, and Phas.pdf:application/pdf;APS Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\S42BEFT5\\PhysRevLett.79.html:text/html},
  groups = {deep learning}
}

@article{ruppeiner1995,
  title = {Riemannian Geometry in Thermodynamic Fluctuation Theory},
  volume = {67},
  doi = {10.1103/RevModPhys.67.605},
  abstract = {Although thermodynamic fluctuation theory originated from statistical mechanics, it may be put on a completely thermodynamic basis, in no essential need of any microscopic foundation. This review views the theory from the macroscopic perspective, emphasizing, in particular, notions of covariance and consistency, expressed naturally using the language of Riemannian geometry. Coupled with these concepts is an extension of the basic structure of thermodynamic fluctuation theory beyond the classical one of a subsystem in contact with an infinite uniform reservoir. Used here is a hierarchy of concentric subsystems, each of which samples only the thermodynamic state of the subsystem immediately larger than it. The result is a covariant thermodynamic fluctuation theory which is plausible beyond the standard second-order entropy expansion. It includes the conservation laws and is mathematically consistent when applied to fluctuations inside subsystems. Tests on known models show improvements. Perhaps most significantly, the covariant theory offers a qualitatively new tool for the study of fluctuation phenomena: the Riemannian thermodynamic curvature. The thermodynamic curvature gives, for any given thermodynamic state, a lower bound for the length scale where the classical thermodynamic fluctuation theory based on a uniform environment could conceivably hold. Straightforward computation near the critical point reveals that the curvature equals the correlation volume, a physically appealing finding. The combination of the interpretation of curvature with a well-known proportionality between the free energy and the inverse of the correlation volume yields a purely thermodynamic theory of the critical point. The scaled equation of state follows from the values of the critical exponents. The thermodynamic Riemannian metric may be put into the broader context of information theory.},
  timestamp = {2018-02-08T16:22:09Z},
  number = {3},
  url = {https://link.aps.org/doi/10.1103/RevModPhys.67.605},
  journal = {Reviews of Modern Physics},
  author = {Ruppeiner, George},
  month = jul,
  year = {1995},
  pages = {605--659},
  file = {Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\JUHND83P\\Ruppeiner - 1995 - Riemannian geometry in thermodynamic fluctuation t.pdf:application/pdf;APS Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\AW8WZEB9\\RevModPhys.67.html:text/html},
  groups = {deep learning}
}

@article{ruppeiner1979,
  title = {Thermodynamics: {{A Riemannian}} Geometric Model},
  volume = {20},
  shorttitle = {Thermodynamics},
  doi = {10.1103/PhysRevA.20.1608},
  abstract = {By including the theory of fluctuations in the axioms of thermodynamics it is shown that thermodynamic systems can be represented by Riemannian manifolds. Of special interest is the curvature of these manifolds which, for pure fluids, is associated with effective interparticle interaction strength by means of a general thermodynamic "interaction hypothesis." This interpretation of curvature appears to be consistent with hyperscaling and two-scale-factor universality. The Riemannian geometric model is a new attempt to extract information from the axioms of thermodynamics.},
  timestamp = {2018-03-29T16:11:28Z},
  number = {4},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.20.1608},
  journal = {Physical Review A},
  author = {Ruppeiner, George},
  month = oct,
  year = {1979},
  pages = {1608--1613},
  file = {Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\VJPZVQCN\\Ruppeiner - 1979 - Thermodynamics A Riemannian geometric model.pdf:application/pdf;APS Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\4J5A9WGK\\PhysRevA.20.html:text/html},
  groups = {deep learning,geometry in physics}
}

@article{felice2018,
  title = {Information Geometric Methods for Complexity},
  volume = {28},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.5018926},
  language = {en},
  timestamp = {2018-03-29T15:56:10Z},
  number = {3},
  urldate = {2018-03-29},
  url = {http://aip.scitation.org/doi/10.1063/1.5018926},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  author = {Felice, Domenico and Cafaro, Carlo and Mancini, Stefano},
  month = mar,
  year = {2018},
  keywords = {Condensed Matter - Statistical Mechanics},
  pages = {032101},
  file = {arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\P2RQMF6K\\1801.html:text/html},
  groups = {deep learning}
}

@book{frankel2012,
  address = {Cambridge ; New York},
  edition = {3rd ed},
  title = {The Geometry of Physics: An Introduction},
  isbn = {978-1-107-60260-1},
  lccn = {QC20.7.D52 F73 2012},
  shorttitle = {The Geometry of Physics},
  abstract = {"This book is intended to provide a working knowledge of those parts of exterior differential forms, differential geometry, algebraic and differential topology, Lie groups, vector bundles, and Chern forms that are essential for a deeper understanding of both classical and modern physics and engineering. Included are discussions of analytical and fluid dynamics, electromagnetism (in flat and curved space), thermodynamics, the deformation tensors of elasticity, soap films, special and general relativity, the Dirac operator and spinors, and gauge fields, including Yang-Mills, the Aharonov-Bohm effect, Berry phase, and instanton winding numbers, quarks, and the quark model for mesons. Before a discussion of abstract notions of differential geometry, geometric intuition is developed through a rather extensive introduction to the study of surfaces in ordinary space; consequently, the book should be of interest also to mathematics students. This book will be useful to graduate and advance undergraduate students of physics, engineering or mathematics. It can be used as a course text or for self-study. This third edition includes a new overview of Cartan's exterior differential forms. It previews many of the geometric concepts developed in the text and illustrates their applications to a single extended problem in engineering, namely the Cauchy stresses created by a small twist of an elastic cylindrical rod about its axis"--},
  timestamp = {2018-03-30T20:32:53Z},
  publisher = {{Cambridge University Press}},
  author = {Frankel, Theodore},
  year = {2012},
  keywords = {Geometry; Differential,Mathematical Physics,MATHEMATICS / Topology},
  groups = {geometry in physics}
}

@book{collier2014,
  title = {A Most Incomprehensible Thing: Notes towards a Very Gentle Introduction to the Mathematics of Relativity},
  isbn = {978-0-9573894-5-8},
  shorttitle = {A Most Incomprehensible Thing},
  language = {English},
  timestamp = {2018-04-02T14:47:30Z},
  author = {Collier, Peter},
  year = {2014},
  note = {OCLC: 947093044},
  groups = {geometry in physics}
}

@book{2017b,
  address = {New York, NY},
  title = {Physics from Symmetry},
  isbn = {978-3-319-66630-3},
  timestamp = {2018-04-02T15:57:17Z},
  publisher = {{Springer Berlin Heidelberg}},
  year = {2017},
  groups = {geometry in physics}
}

@book{weinberg1972,
  address = {New York},
  title = {Gravitation and Cosmology: Principles and Applications of the General Theory of Relativity},
  isbn = {978-0-471-92567-5},
  lccn = {QC6 .W47},
  shorttitle = {Gravitation and Cosmology},
  timestamp = {2018-04-02T14:53:09Z},
  publisher = {{Wiley}},
  author = {Weinberg, Steven},
  year = {1972},
  keywords = {Cosmology,General relativity (Physics),Gravitation},
  groups = {geometry in physics}
}

@book{dray2015,
  address = {Boca Raton},
  title = {Differential Forms and the Geometry of General Relativity},
  isbn = {978-1-4665-1000-5},
  lccn = {QC173.6 .D73 2015},
  timestamp = {2018-04-02T14:57:48Z},
  publisher = {{CRC Press, Taylor \& Francis Group}},
  author = {Dray, Tevian},
  year = {2015},
  keywords = {Black holes (Astronomy),Differential forms,General relativity (Physics),Geometry,Mathematics},
  groups = {geometry in physics}
}

@book{amari2000,
  address = {Providence, RI},
  series = {Translations of mathematical monographs},
  title = {Methods of Information Geometry},
  isbn = {978-0-8218-0531-2},
  lccn = {QA276 .A56313 2000},
  language = {eng},
  timestamp = {2018-04-02T15:25:34Z},
  number = {v. 191},
  publisher = {{American Mathematical Society}},
  author = {Amari, Shun'ichi and Nagaoka, Hiroshi},
  year = {2000},
  keywords = {Geometry; Differential,Mathematical statistics},
  groups = {geometry in physics}
}

@book{neuenschwander2015,
  address = {Baltimore},
  title = {Tensor Calculus for Physics: A Concise Guide},
  isbn = {978-1-4214-1564-2 978-1-4214-1565-9},
  lccn = {QC20.7.C28 N48 2015},
  shorttitle = {Tensor Calculus for Physics},
  abstract = {"Using a clear, step-by-step approach, the book strives to embed the logic of tensors in contexts that demonstrate why that logic is worth pursuing. It is an ideal companion for courses such as mathematical methods in physics, classical mechanics, electricity and magnetism, and relativity"--},
  timestamp = {2018-04-02T15:07:21Z},
  publisher = {{Johns Hopkins University Press}},
  author = {Neuenschwander, Dwight E.},
  year = {2015},
  note = {OCLC: ocn879584198},
  keywords = {Calculus of tensors,Physics},
  groups = {geometry in physics}
}

@article{possolo2018,
  title = {Approximate {{Bayesian}} Evaluations of Measurement Uncertainty},
  volume = {55},
  issn = {0026-1394},
  doi = {10.1088/1681-7575/aaa5be},
  timestamp = {2018-04-23T12:56:31Z},
  number = {2},
  urldate = {2018-04-23},
  url = {http://stacks.iop.org/0026-1394/55/i=2/a=147?key=crossref.e02894c42ac1e558a2d1a213e0526bf2},
  journal = {Metrologia},
  author = {Possolo, Antonio and Bodnar, Olha},
  month = apr,
  year = {2018},
  note = {Saved from BrowZine: https://thirdiron.com/download},
  pages = {147 -- 157}
}

@book{walpole1978,
  address = {New York},
  edition = {2d ed},
  title = {Probability and Statistics for Engineers and Scientists},
  isbn = {978-0-02-424110-8},
  lccn = {TA340 .W35 1978},
  timestamp = {2018-04-11T21:09:34Z},
  publisher = {{Macmillan}},
  author = {Walpole, Ronald E. and Myers, Raymond H.},
  year = {1978},
  keywords = {Engineering,Statistical methods},
  groups = {uncertainty}
}

@article{possolo2018a,
  title = {Approximate {{Bayesian}} Evaluations of Measurement Uncertainty},
  volume = {55},
  issn = {0026-1394},
  doi = {10.1088/1681-7575/aaa5be},
  abstract = {The Guide to the Expression of Uncertainty in Measurement (GUM) includes formulas that produce an estimate of a scalar output quantity that is a function of several input quantities, and an approximate evaluation of the associated standard uncertainty. This contribution presents approximate, Bayesian counterparts of those formulas for the case where the output quantity is a parameter of the joint probability distribution of the input quantities, also taking into account any information about the value of the output quantity available prior to measurement expressed in the form of a probability distribution on the set of possible values for the measurand. The approximate Bayesian estimates and uncertainty evaluations that we present have a long history and illustrious pedigree, and provide sufficiently accurate approximations in many applications, yet are very easy to implement in practice. Differently from exact Bayesian estimates, which involve either (analytical or numerical) integrations, or Markov Chain Monte Carlo sampling, the approximations that we describe involve only numerical optimization and simple algebra. Therefore, they make Bayesian methods widely accessible to metrologists. We illustrate the application of the proposed techniques in several instances of measurement: isotopic ratio of silver in a commercial silver nitrate; odds of cryptosporidiosis in AIDS patients; height of a manometer column; mass fraction of chromium in a reference material; and potential-difference in a Zener voltage standard.},
  language = {en},
  timestamp = {2018-04-24T17:31:36Z},
  number = {2},
  url = {http://stacks.iop.org/0026-1394/55/i=2/a=147},
  journal = {Metrologia},
  author = {Possolo, Antonio and Bodnar, Olha},
  year = {2018},
  pages = {147},
  file = {IOP Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\F6V24W56\\Possolo and Bodnar - 2018 - Approximate Bayesian evaluations of measurement un.pdf:application/pdf},
  groups = {uncertainty}
}

@article{pathak2017,
  title = {Using {{Machine Learning}} to {{Replicate Chaotic Attractors}} and {{Calculate Lyapunov Exponents}} from {{Data}}},
  volume = {27},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.5010300},
  abstract = {We use recent advances in the machine learning area known as 'reservoir computing' to formulate a method for model-free estimation from data of the Lyapunov exponents of a chaotic process. The technique uses a limited time series of measurements as input to a high-dimensional dynamical system called a 'reservoir'. After the reservoir's response to the data is recorded, linear regression is used to learn a large set of parameters, called the 'output weights'. The learned output weights are then used to form a modified autonomous reservoir designed to be capable of producing arbitrarily long time series whose ergodic properties approximate those of the input signal. When successful, we say that the autonomous reservoir reproduces the attractor's 'climate'. Since the reservoir equations and output weights are known, we can compute derivatives needed to determine the Lyapunov exponents of the autonomous reservoir, which we then use as estimates of the Lyapunov exponents for the original input generating system. We illustrate the effectiveness of our technique with two examples, the Lorenz system, and the Kuramoto-Sivashinsky (KS) equation. In particular, we use the Lorenz system to show that achieving climate reproduction may require tuning of the reservoir parameters. For the case of the KS equation, we note that as the system's spatial size is increased, the number of Lyapunov exponents increases, thus yielding a challenging test of our method, which we find the method successfully passes.},
  timestamp = {2018-04-24T17:42:51Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.07313},
  number = {12},
  url = {http://arxiv.org/abs/1710.07313},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  author = {Pathak, Jaideep and Lu, Zhixin and Hunt, Brian R. and Girvan, Michelle and Ott, Edward},
  month = dec,
  year = {2017},
  keywords = {Nonlinear Sciences - Chaotic Dynamics},
  pages = {121102},
  file = {arXiv\:1710.07313 PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\Q36WQP72\\Pathak et al. - 2017 - Using Machine Learning to Replicate Chaotic Attrac.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\7Z36DNIQ\\1710.html:text/html},
  groups = {chaos}
}

@article{cafaro2016,
  title = {The {{Information Geometry}} of {{Chaos}}},
  abstract = {We present a new theoretical information-geometric framework (IGAC, Information Geometrodynamical Approach to Chaos) suitable to characterize chaotic dynamical behavior of arbitrary complex systems.},
  timestamp = {2018-05-10T22:13:33Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1601.07935},
  primaryClass = {math-ph},
  urldate = {2018-05-10},
  url = {http://arxiv.org/abs/1601.07935},
  journal = {arXiv:1601.07935 [math-ph]},
  author = {Cafaro, Carlo},
  month = jan,
  year = {2016},
  keywords = {Mathematical Physics},
  groups = {geometry in physics}
}

@article{lolli2012,
  title = {A Comparison among General Orthogonal Regression Methods Applied to Earthquake Magnitude Conversions},
  volume = {190},
  issn = {0956-540X},
  doi = {10.1111/j.1365-246X.2012.05530.x},
  abstract = {Summary.  Until a decade ago, regression analyses for conversions between different types of magnitude were using only the ordinary least squares method, which},
  language = {en},
  timestamp = {2018-06-18T13:50:12Z},
  number = {2},
  urldate = {2018-06-18},
  url = {https://academic.oup.com/gji/article/190/2/1135/644394},
  journal = {Geophysical Journal International},
  author = {Lolli, Barbara and Gasperini, Paolo},
  month = aug,
  year = {2012},
  pages = {1135--1151},
  file = {Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\6UU7ZT9M\\Lolli and Gasperini - 2012 - A comparison among general orthogonal regression m.pdf:application/pdf;Snapshot:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\P4GDMVU9\\644394.html:text/html},
  groups = {curve fitting}
}

@article{nielsen1998,
  title = {Least-Squares Estimation Using {{Lagrange}} Multipliers},
  volume = {35},
  issn = {0026-1394},
  doi = {10.1088/0026-1394/35/2/6},
  abstract = {This paper presents a general formulation of least-squares estimation using Lagrange multipliers. It is general in the sense that it takes into account the covariance matrix of all measured quantities and allows any known constraint to be imposed on the quantities being estimated. The technique may be used to estimate parameters of a calibration curve for a transducer taking into account the uncertainty and correlation coefficients of the measured stimuli and responses. It may also be used to adjust measured fundamental constants to a coherent set obeying the appropriate physical laws.},
  language = {en},
  timestamp = {2018-07-03T17:07:08Z},
  number = {2},
  url = {http://stacks.iop.org/0026-1394/35/i=2/a=6},
  journal = {Metrologia},
  author = {Nielsen, L.},
  year = {1998},
  pages = {115},
  file = {IOP Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\S847S432\\Nielsen - 1998 - Least-squares estimation using Lagrange multiplier.pdf:application/pdf},
  groups = {curve fitting}
}

@article{lira2000,
  title = {Curve Adjustment by the Least-Squares Method},
  volume = {37},
  issn = {0026-1394},
  doi = {10.1088/0026-1394/37/6/5},
  abstract = {The least-squares procedure for obtaining the parameters of a curve adjusted to a set of measured values is discussed. Two algorithms are presented in matrix notation, which can handle the general case where all input quantities are uncertain. Attention is paid to the consistency of the adjustment. Two examples show that the algorithms exhibit comparable performances.},
  language = {en},
  timestamp = {2018-07-03T17:08:59Z},
  number = {6},
  url = {http://stacks.iop.org/0026-1394/37/i=6/a=5},
  journal = {Metrologia},
  author = {Lira, I.},
  year = {2000},
  pages = {677},
  file = {IOP Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\48JABRDT\\Lira - 2000 - Curve adjustment by the least-squares method.pdf:application/pdf},
  groups = {curve fitting}
}

@article{milton2006,
  title = {Implementation of a Generalized Least-Squares Method for Determining Calibration Curves from Data with General Uncertainty Structures},
  volume = {43},
  issn = {0026-1394},
  doi = {10.1088/0026-1394/43/4/S17},
  abstract = {The determination of a best-fit calibration curve that describes the response of a measuring system to the value of a standard is one of the most widely used procedures in metrology. The mathematical basis for a generalized least-squares solution to this problem is reviewed. Examples of the application of a software implementation of the method are presented to illustrate the treatment of calibration problems with different uncertainty structures for the calibration data, including correlated data. The examples concern the calibration of analysers to measure the composition of natural gas and the calibration of a gas flow dilutor.},
  language = {en},
  timestamp = {2018-07-03T17:11:45Z},
  number = {4},
  url = {http://stacks.iop.org/0026-1394/43/i=4/a=S17},
  journal = {Metrologia},
  author = {Milton, M. J. T. and Harris, P. M. and Smith, I. M. and Brown, A. S. and Goody, B. A.},
  year = {2006},
  pages = {S291},
  file = {IOP Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\5TAUZJKD\\Milton et al. - 2006 - Implementation of a generalized least-squares meth.pdf:application/pdf},
  groups = {curve fitting}
}

@article{elster2011,
  title = {Bayesian Uncertainty Analysis for a Regression Model versus Application of {{GUM Supplement}} 1 to the Least-Squares Estimate},
  volume = {48},
  issn = {0026-1394},
  doi = {10.1088/0026-1394/48/5/001},
  abstract = {Application of least-squares as, for instance, in curve fitting is an important tool of data analysis in metrology. It is tempting to employ the supplement 1 to the GUM (GUM-S1) to evaluate the uncertainty associated with the resulting parameter estimates, although doing so is beyond the specified scope of GUM-S1. We compare the result of such a procedure with a Bayesian uncertainty analysis of the corresponding regression model. It is shown that under certain assumptions both analyses yield the same results but this is not true in general. Some simple examples are given which illustrate the similarities and differences between the two approaches.},
  language = {en},
  timestamp = {2018-07-03T17:15:25Z},
  number = {5},
  url = {http://stacks.iop.org/0026-1394/48/i=5/a=001},
  journal = {Metrologia},
  author = {Elster, Clemens and Toman, Blaza},
  year = {2011},
  pages = {233},
  file = {IOP Full Text PDF:C\:\\Users\\jajalkio\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\5ku06r0f.default\\zotero\\storage\\X39CK4NS\\Elster and Toman - 2011 - Bayesian uncertainty analysis for a regression mod.pdf:application/pdf},
  groups = {curve fitting}
}

@comment{jabref-meta: databaseType:bibtex;}
@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:angles\;0\;;
1 ExplicitGroup:chaos\;0\;;
1 ExplicitGroup:climate\;0\;;
1 ExplicitGroup:computer architecture\;0\;;
1 ExplicitGroup:cosmology\;0\;;
1 ExplicitGroup:curve fitting\;0\;;
1 ExplicitGroup:curve fitting 2/21/2017, 11:10:52 AM\;0\;;
1 ExplicitGroup:deep learning\;0\;;
1 ExplicitGroup:geometry in physics\;0\;;
1 ExplicitGroup:machine learning\;0\;;
1 ExplicitGroup:mass metrology\;0\;;
1 ExplicitGroup:pandoc\;0\;;
1 ExplicitGroup:programming languages\;0\;;
1 ExplicitGroup:python\;0\;;
1 ExplicitGroup:quantum\;0\;;
1 ExplicitGroup:refworks\;0\;;
1 ExplicitGroup:self\;0\;;
1 ExplicitGroup:time and frequency metrology\;0\;;
1 ExplicitGroup:uncertainty\;0\;;
}

